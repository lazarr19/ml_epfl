{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_cost` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    e = np.abs(y - np.ravel(np.dot(tx, w)))\n",
    "    return np.sum(e)/e.shape[0]\n",
    "    \n",
    "    # ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from costs import *\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "        \n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    # ***************************************************\n",
    "    \n",
    "    for w0_index in range(losses.shape[0]):\n",
    "        for w1_index in range(losses.shape[1]):\n",
    "            losses[w0_index, w1_index] = compute_loss(y, tx, [[grid_w0[w0_index]],[grid_w1[w1_index]]])\n",
    "    \n",
    "    # ***************************************************\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=42.42448314678248, w0*=66.66666666666669, w1*=16.666666666666686, execution time=0.005 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=10)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "      l=loss_star, w0=w0_star, w1=w1_star, t=execution_time))\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0,6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    \n",
    "    e = y - np.ravel(np.dot(tx, w));   \n",
    "    return - np.dot(np.transpose(tx), e)/e.shape[0]\n",
    "    \n",
    "    # ***************************************************\n",
    "\n",
    "#y1, tx1 = build_model_data([3,2],[1,3])\n",
    "#w1 = [100, 20]\n",
    "#print(compute_gradient(y1, tx1, w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        w = w - gamma * compute_gradient(y, tx, w)\n",
    "        # ***************************************************\n",
    "        # ***************************************************\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        # ***************************************************\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2264.6350560300057, w0=7.329392200210475, w1=1.3479712434989204\n",
      "GD iter. 1/49: loss=1837.2777140793814, w0=13.92584518039996, w1=2.5611453626479217\n",
      "GD iter. 2/49: loss=1491.1182670993776, w0=19.862652862570464, w1=3.653002069882027\n",
      "GD iter. 3/49: loss=1210.7291150455742, w0=25.205779776523926, w1=4.635673106392729\n",
      "GD iter. 4/49: loss=983.6139018819919, w0=30.014593999082066, w1=5.520077039252341\n",
      "GD iter. 5/49: loss=799.6505792194912, w0=34.34252679938437, w1=6.316040578826014\n",
      "GD iter. 6/49: loss=650.6402878628655, w0=38.237666319656455, w1=7.032407764442313\n",
      "GD iter. 7/49: loss=529.9419518639996, w0=41.7432918879013, w1=7.677138231496978\n",
      "GD iter. 8/49: loss=432.1762997049176, w0=44.89835489932168, w1=8.257395651846178\n",
      "GD iter. 9/49: loss=352.98612145606035, w0=47.73791160960005, w1=8.779627330160464\n",
      "GD iter. 10/49: loss=288.8420770744868, w0=50.29351264885055, w1=9.249635840643316\n",
      "GD iter. 11/49: loss=236.8854011254118, w0=52.59355358417602, w1=9.67264350007789\n",
      "GD iter. 12/49: loss=194.80049360666106, w0=54.66359042596894, w1=10.053350393569005\n",
      "GD iter. 13/49: loss=160.711718516473, w0=56.52662358358257, w1=10.395986597711003\n",
      "GD iter. 14/49: loss=133.0998106934208, w0=58.20335342543483, w1=10.704359181438804\n",
      "GD iter. 15/49: loss=110.73416535674839, w0=59.71241028310187, w1=10.981894506793827\n",
      "GD iter. 16/49: loss=92.6179926340438, w0=61.0705614550022, w1=11.231676299613346\n",
      "GD iter. 17/49: loss=77.94389272865318, w0=62.29289750971249, w1=11.456479913150911\n",
      "GD iter. 18/49: loss=66.05787180528668, w0=63.392999958951755, w1=11.658803165334724\n",
      "GD iter. 19/49: loss=56.43019485735972, w0=64.38309216326711, w1=11.840894092300157\n",
      "GD iter. 20/49: loss=48.63177652953899, w0=65.27417514715091, w1=12.004775926569046\n",
      "GD iter. 21/49: loss=42.31505768400419, w0=66.07614983264634, w1=12.152269577411046\n",
      "GD iter. 22/49: loss=37.19851541912101, w0=66.79792704959222, w1=12.285013863168846\n",
      "GD iter. 23/49: loss=33.054116184565615, w0=67.44752654484351, w1=12.404483720350866\n",
      "GD iter. 24/49: loss=29.69715280457577, w0=68.03216609056967, w1=12.512006591814686\n",
      "GD iter. 25/49: loss=26.97801246678398, w0=68.55834168172322, w1=12.608777176132122\n",
      "GD iter. 26/49: loss=24.775508793172605, w0=69.03189971376142, w1=12.695870702017814\n",
      "GD iter. 27/49: loss=22.991480817547423, w0=69.45810194259579, w1=12.774254875314934\n",
      "GD iter. 28/49: loss=21.546418157290994, w0=69.84168394854673, w1=12.844800631282347\n",
      "GD iter. 29/49: loss=20.3759174024833, w0=70.18690775390257, w1=12.908291811653017\n",
      "GD iter. 30/49: loss=19.42781179108905, w0=70.49760917872284, w1=12.965433873986619\n",
      "GD iter. 31/49: loss=18.659846245859725, w0=70.77724046106107, w1=13.016861730086863\n",
      "GD iter. 32/49: loss=18.03779415422397, w0=71.02890861516548, w1=13.063146800577082\n",
      "GD iter. 33/49: loss=17.533931959998984, w0=71.25540995385946, w1=13.104803364018277\n",
      "GD iter. 34/49: loss=17.12580358267677, w0=71.45926115868403, w1=13.142294271115354\n",
      "GD iter. 35/49: loss=16.79521959704577, w0=71.64272724302614, w1=13.176036087502723\n",
      "GD iter. 36/49: loss=16.52744656868467, w0=71.80784671893404, w1=13.206403722251356\n",
      "GD iter. 37/49: loss=16.310550415712164, w0=71.95645424725116, w1=13.233734593525124\n",
      "GD iter. 38/49: loss=16.13486453180444, w0=72.09020102273657, w1=13.258332377671517\n",
      "GD iter. 39/49: loss=15.99255896583918, w0=72.21057312067343, w1=13.28047038340327\n",
      "GD iter. 40/49: loss=15.877291457407319, w0=72.3189080088166, w1=13.300394588561847\n",
      "GD iter. 41/49: loss=15.78392477557751, w0=72.41640940814547, w1=13.318326373204567\n",
      "GD iter. 42/49: loss=15.708297763295374, w0=72.50416066754144, w1=13.334464979383014\n",
      "GD iter. 43/49: loss=15.647039883346832, w0=72.58313680099782, w1=13.348989724943618\n",
      "GD iter. 44/49: loss=15.597421000588522, w0=72.65421532110855, w1=13.36206199594816\n",
      "GD iter. 45/49: loss=15.557229705554297, w0=72.71818598920821, w1=13.373827039852248\n",
      "GD iter. 46/49: loss=15.524674756576562, w0=72.77575959049791, w1=13.384415579365928\n",
      "GD iter. 47/49: loss=15.498305247904606, w0=72.82757583165863, w1=13.39394526492824\n",
      "GD iter. 48/49: loss=15.476945945880315, w0=72.8742104487033, w1=13.40252198193432\n",
      "GD iter. 49/49: loss=15.459644911240636, w0=72.91618160404349, w1=13.410241027239794\n",
      "GD: execution time=0.015 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75308367cb414deea74454e5f3d81432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from just few examples n and their corresponding y_n labels.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ***************************************************\n",
    "    \n",
    "    e = y - np.ravel(np.dot(tx, w))\n",
    "    return -np.dot(np.transpose(tx), e)/e.shape[0]\n",
    "    \n",
    "    # ***************************************************\n",
    "    \n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        \n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            w = w - gamma * compute_stoch_gradient(minibatch_y, minibatch_tx, w)\n",
    "        \n",
    "        loss = compute_loss(y, tx, w)\n",
    "        \n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # TODO: implement stochastic gradient descent.\n",
    "        # ***************************************************\n",
    "        \n",
    "        print(\"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=2058.1169306992692, w0=9.408994787683554, w1=15.523767343480623\n",
      "SGD iter. 1/49: loss=1746.451265563301, w0=15.104243273038385, w1=22.202787855787157\n",
      "SGD iter. 2/49: loss=1396.10382880862, w0=20.985790447071015, w1=18.509151280508338\n",
      "SGD iter. 3/49: loss=1205.2044376841231, w0=25.616437663536217, w1=23.79933388037633\n",
      "SGD iter. 4/49: loss=1009.3749232036502, w0=29.545419764871703, w1=22.08474716289209\n",
      "SGD iter. 5/49: loss=919.5257839270308, w0=32.377433623067795, w1=25.060764668896795\n",
      "SGD iter. 6/49: loss=823.3440332223164, w0=35.48643639893345, w1=27.13659066806027\n",
      "SGD iter. 7/49: loss=730.783835264431, w0=38.554220038351595, w1=28.44463817021328\n",
      "SGD iter. 8/49: loss=660.8710496288124, w0=40.780996746908855, w1=28.772848636238983\n",
      "SGD iter. 9/49: loss=613.1971682041941, w0=42.61183908041311, w1=29.424377632387827\n",
      "SGD iter. 10/49: loss=395.88286592773267, w0=47.77859516908837, w1=23.965991695152674\n",
      "SGD iter. 11/49: loss=356.4647277570334, w0=49.903960047409754, w1=25.101560804947567\n",
      "SGD iter. 12/49: loss=235.13628966069743, w0=53.74503059115864, w1=21.052139271001128\n",
      "SGD iter. 13/49: loss=197.32862888403326, w0=55.91281744015761, w1=21.33990875804946\n",
      "SGD iter. 14/49: loss=155.09128642102047, w0=57.861819311002925, w1=19.90318529907559\n",
      "SGD iter. 15/49: loss=129.94186517494842, w0=60.02752430791977, w1=20.767692047470796\n",
      "SGD iter. 16/49: loss=129.9751888771285, w0=60.013566171791474, w1=20.746813028544256\n",
      "SGD iter. 17/49: loss=105.10344277368617, w0=61.87310494223554, w1=20.479715827467064\n",
      "SGD iter. 18/49: loss=105.42388678617358, w0=62.28040185587402, w1=21.14641784365373\n",
      "SGD iter. 19/49: loss=105.76308197888832, w0=62.021300366249335, w1=20.806539147485537\n",
      "SGD iter. 20/49: loss=108.88631067449973, w0=61.84493561602936, w1=20.957784134484232\n",
      "SGD iter. 21/49: loss=106.96116718588583, w0=62.416366711741766, w1=21.531379935525777\n",
      "SGD iter. 22/49: loss=100.2414502406078, w0=63.60368781827168, w1=22.186634176110942\n",
      "SGD iter. 23/49: loss=87.04148348256808, w0=65.03284397741126, w1=22.143763513866425\n",
      "SGD iter. 24/49: loss=81.42061983336289, w0=64.26313468073282, w1=20.587056825491512\n",
      "SGD iter. 25/49: loss=76.38565949957997, w0=65.09603958985464, w1=20.88202742534268\n",
      "SGD iter. 26/49: loss=70.4492748616918, w0=66.51288528643752, w1=21.488727046443554\n",
      "SGD iter. 27/49: loss=67.4550189635895, w0=67.71712047515678, w1=22.025913165053016\n",
      "SGD iter. 28/49: loss=63.12509564151144, w0=66.51694360067324, w1=20.518958082918154\n",
      "SGD iter. 29/49: loss=63.3606449142308, w0=66.54204906356605, w1=20.576312037096073\n",
      "SGD iter. 30/49: loss=51.68195214818916, w0=67.57238768230991, w1=19.79288703942731\n",
      "SGD iter. 31/49: loss=20.746918719853266, w0=70.03747016098075, w1=13.136808500212897\n",
      "SGD iter. 32/49: loss=24.05311953209637, w0=69.3357863210082, w1=12.188346779351253\n",
      "SGD iter. 33/49: loss=23.246196434462785, w0=69.57963393073537, w1=12.092383615225456\n",
      "SGD iter. 34/49: loss=21.440354012221377, w0=69.90004090050583, w1=12.711270273866516\n",
      "SGD iter. 35/49: loss=18.869823357072157, w0=70.67550937780679, w1=13.145367844184465\n",
      "SGD iter. 36/49: loss=19.230499034508203, w0=70.54334479836605, w1=13.128219365884995\n",
      "SGD iter. 37/49: loss=16.396266426343413, w0=71.87259637285806, w1=13.504014094876167\n",
      "SGD iter. 38/49: loss=16.093866637749812, w0=72.11379006839574, w1=13.63217933920117\n",
      "SGD iter. 39/49: loss=15.636416417685753, w0=72.65200431500628, w1=13.778039069237977\n",
      "SGD iter. 40/49: loss=16.187085957462315, w0=72.17664756238447, w1=14.07477041839284\n",
      "SGD iter. 41/49: loss=15.853826382045856, w0=72.37155366902522, w1=13.77145493200344\n",
      "SGD iter. 42/49: loss=15.638283690920597, w0=72.71569564892076, w1=13.892563411329697\n",
      "SGD iter. 43/49: loss=15.595388649814803, w0=72.81383878828265, w1=13.913903256836567\n",
      "SGD iter. 44/49: loss=15.540018474662057, w0=73.0790552511202, w1=13.991663107404337\n",
      "SGD iter. 45/49: loss=17.064770823475016, w0=73.78859546641536, w1=15.244100168169771\n",
      "SGD iter. 46/49: loss=17.222392738497927, w0=74.37012292903292, w1=15.06552494099881\n",
      "SGD iter. 47/49: loss=17.659410102923545, w0=74.90067032936852, w1=14.88164120672632\n",
      "SGD iter. 48/49: loss=18.1876746198049, w0=75.10969025578467, w1=14.99844847225187\n",
      "SGD iter. 49/49: loss=19.886567205459876, w0=75.48515375520746, w1=15.529068938249841\n",
      "SGD: execution time=0.082 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc72fd86a0d7407b99e820ad9e85e9c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "# ***************************************************\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=False)\n",
    "# TODO: reload the data by subsampling first, then by subsampling and adding outliers\n",
    "# ***************************************************\n",
    "\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200,), (200, 2))"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=22.089681737919364, w0=51.54259072181176, w1=10.132993413506084\n",
      "GD iter. 1/49: loss=7.092620376244781, w0=67.0053679383553, w1=13.172891437557825\n",
      "GD iter. 2/49: loss=4.4588458594701335, w0=71.64420110331838, w1=14.084860844773324\n",
      "GD iter. 3/49: loss=4.22393120262193, w0=73.03585105280729, w1=14.358451666937965\n",
      "GD iter. 4/49: loss=4.21395630714177, w0=73.45334603765397, w1=14.440528913587356\n",
      "GD iter. 5/49: loss=4.2173804644768875, w0=73.57859453310797, w1=14.46515208758217\n",
      "GD iter. 6/49: loss=4.218798885430453, w0=73.61616908174418, w1=14.472539039780616\n",
      "GD iter. 7/49: loss=4.219302422463519, w0=73.62744144633503, w1=14.474755125440149\n",
      "GD iter. 8/49: loss=4.219453483573439, w0=73.63082315571229, w1=14.47541995113801\n",
      "GD iter. 9/49: loss=4.219498801906414, w0=73.63183766852546, w1=14.475619398847368\n",
      "GD iter. 10/49: loss=4.219512397406308, w0=73.63214202236942, w1=14.475679233160175\n",
      "GD iter. 11/49: loss=4.219516476056276, w0=73.6322333285226, w1=14.475697183454017\n",
      "GD iter. 12/49: loss=4.219517699651266, w0=73.63226072036856, w1=14.47570256854217\n",
      "GD iter. 13/49: loss=4.219518066729762, w0=73.63226893792235, w1=14.475704184068615\n",
      "GD iter. 14/49: loss=4.219518176853312, w0=73.63227140318848, w1=14.475704668726548\n",
      "GD iter. 15/49: loss=4.219518209890376, w0=73.63227214276833, w1=14.47570481412393\n",
      "GD iter. 16/49: loss=4.219518219801496, w0=73.63227236464228, w1=14.475704857743143\n",
      "GD iter. 17/49: loss=4.219518222774831, w0=73.63227243120446, w1=14.475704870828908\n",
      "GD iter. 18/49: loss=4.2195182236668325, w0=73.63227245117312, w1=14.475704874754637\n",
      "GD iter. 19/49: loss=4.219518223934433, w0=73.63227245716372, w1=14.475704875932356\n",
      "GD iter. 20/49: loss=4.2195182240147115, w0=73.6322724589609, w1=14.475704876285672\n",
      "GD iter. 21/49: loss=4.219518224038796, w0=73.63227245950004, w1=14.475704876391665\n",
      "GD iter. 22/49: loss=4.219518224046022, w0=73.63227245966179, w1=14.475704876423464\n",
      "GD iter. 23/49: loss=4.2195182240481905, w0=73.63227245971032, w1=14.475704876433003\n",
      "GD iter. 24/49: loss=4.21951822404884, w0=73.63227245972487, w1=14.475704876435865\n",
      "GD iter. 25/49: loss=4.219518224049034, w0=73.63227245972924, w1=14.475704876436724\n",
      "GD iter. 26/49: loss=4.219518224049093, w0=73.63227245973054, w1=14.475704876436982\n",
      "GD iter. 27/49: loss=4.219518224049111, w0=73.63227245973094, w1=14.47570487643706\n",
      "GD iter. 28/49: loss=4.219518224049116, w0=73.63227245973106, w1=14.475704876437083\n",
      "GD iter. 29/49: loss=4.219518224049118, w0=73.6322724597311, w1=14.475704876437089\n",
      "GD iter. 30/49: loss=4.219518224049118, w0=73.63227245973111, w1=14.47570487643709\n",
      "GD iter. 31/49: loss=4.219518224049118, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 32/49: loss=4.219518224049118, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 33/49: loss=4.219518224049118, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 34/49: loss=4.219518224049118, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 35/49: loss=4.219518224049118, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 36/49: loss=4.219518224049118, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 37/49: loss=4.219518224049118, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 38/49: loss=4.219518224049118, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 39/49: loss=4.219518224049118, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 40/49: loss=4.219518224049118, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 41/49: loss=4.219518224049118, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 42/49: loss=4.219518224049118, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 43/49: loss=4.219518224049118, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 44/49: loss=4.219518224049118, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 45/49: loss=4.219518224049118, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 46/49: loss=4.219518224049118, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 47/49: loss=4.219518224049118, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 48/49: loss=4.219518224049118, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 49/49: loss=4.219518224049118, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD: execution time=0.003 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# ***************************************************\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "# TODO: fit the model to the subsampled data / subsampled data with outliers and visualize the cloud of points \n",
    "#       and the model fit\n",
    "# ***************************************************\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76797793b2d943058e513b0db2e87b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    e = y - np.ravel(np.dot(tx, w))\n",
    "    \n",
    "    def subgradient_abs(x):\n",
    "        if x>0: \n",
    "            return 1 \n",
    "        elif x<0:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0.1\n",
    "    \n",
    "    subgradient_abs = np.vectorize(subgradient_abs)    \n",
    "    return np.dot(subgradient_abs(e), -tx)\n",
    "\n",
    "    # TODO: compute subgradient gradient vector for MAE\n",
    "    # ***************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        g = compute_subgradient_mae(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        # TODO: compute subgradient and loss\n",
    "        # ***************************************************\n",
    "\n",
    "        # ***************************************************\n",
    "        w = w - gamma * g\n",
    "        # TODO: update w by subgradient\n",
    "        # ***************************************************\n",
    "        \n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=73.63227245973114, w0=10.0, w1=2.4136248555350904e-14\n",
      "SubGD iter. 1/499: loss=63.63227245973115, w0=20.0, w1=4.827249711070181e-14\n",
      "SubGD iter. 2/499: loss=53.63227245973114, w0=30.0, w1=7.240874566605271e-14\n",
      "SubGD iter. 3/499: loss=43.63227245973115, w0=40.0, w1=9.654499422140362e-14\n",
      "SubGD iter. 4/499: loss=33.63227245973114, w0=50.0, w1=1.2068124277675452e-13\n",
      "SubGD iter. 5/499: loss=23.751731228250335, w0=59.4, w1=1.022237518737621\n",
      "SubGD iter. 6/499: loss=16.02870897418529, w0=64.9, w1=6.175925904309658\n",
      "SubGD iter. 7/499: loss=10.531904466158162, w0=69.7, w1=11.451003472855309\n",
      "SubGD iter. 8/499: loss=5.895623282178572, w0=73.60000000000001, w1=15.238509620022757\n",
      "SubGD iter. 9/499: loss=4.247240766636315, w0=73.10000000000001, w1=14.186802331885895\n",
      "SubGD iter. 10/499: loss=4.232391523234558, w0=73.60000000000001, w1=14.940203697756106\n",
      "SubGD iter. 11/499: loss=4.218705622413471, w0=73.10000000000001, w1=14.247472988971072\n",
      "SubGD iter. 12/499: loss=4.227952953815415, w0=73.4, w1=14.890687572840054\n",
      "SubGD iter. 13/499: loss=4.208063554072037, w0=73.2, w1=14.187908637580508\n",
      "SubGD iter. 14/499: loss=4.228784219764727, w0=73.5, w1=14.83112322144949\n",
      "SubGD iter. 15/499: loss=4.2065927843830755, w0=73.2, w1=14.32745177488359\n",
      "SubGD iter. 16/499: loss=4.219808601665509, w0=73.5, w1=14.970666358752572\n",
      "SubGD iter. 17/499: loss=4.216384094053089, w0=73.2, w1=14.218317463448273\n",
      "SubGD iter. 18/499: loss=4.226828279737078, w0=73.5, w1=14.861532047317255\n",
      "SubGD iter. 19/499: loss=4.208575405518151, w0=73.1, w1=14.1464879828452\n",
      "SubGD iter. 20/499: loss=4.235428811797701, w0=73.6, w1=14.89988934871541\n",
      "SubGD iter. 21/499: loss=4.215912923654954, w0=73.1, w1=14.207158639930377\n",
      "SubGD iter. 22/499: loss=4.2308578762060804, w0=73.6, w1=14.960560005800588\n",
      "SubGD iter. 23/499: loss=4.2201157663834605, w0=73.1, w1=14.267829297015554\n",
      "SubGD iter. 24/499: loss=4.226643606394623, w0=73.39999999999999, w1=14.911043880884536\n",
      "SubGD iter. 25/499: loss=4.209494152521368, w0=73.19999999999999, w1=14.20826494562499\n",
      "SubGD iter. 26/499: loss=4.227474872343932, w0=73.49999999999999, w1=14.851479529493972\n",
      "SubGD iter. 27/499: loss=4.207856606197897, w0=73.09999999999998, w1=14.136435465021917\n",
      "SubGD iter. 28/499: loss=4.236186169863553, w0=73.59999999999998, w1=14.889836830892127\n",
      "SubGD iter. 29/499: loss=4.215216554875274, w0=73.09999999999998, w1=14.197106122107094\n",
      "SubGD iter. 30/499: loss=4.23161523427193, w0=73.59999999999998, w1=14.950507487977305\n",
      "SubGD iter. 31/499: loss=4.21941939760378, w0=73.09999999999998, w1=14.257776779192271\n",
      "SubGD iter. 32/499: loss=4.227290199001477, w0=73.39999999999998, w1=14.900991363061253\n",
      "SubGD iter. 33/499: loss=4.2087876827441155, w0=73.19999999999997, w1=14.198212427801707\n",
      "SubGD iter. 34/499: loss=4.228121464950787, w0=73.49999999999997, w1=14.841427011670689\n",
      "SubGD iter. 35/499: loss=4.207137806877642, w0=73.09999999999997, w1=14.126382947198634\n",
      "SubGD iter. 36/499: loss=4.236999133343713, w0=73.69999999999996, w1=14.945974931614284\n",
      "SubGD iter. 37/499: loss=4.2250536564987025, w0=72.99999999999996, w1=13.953885829996688\n",
      "SubGD iter. 38/499: loss=4.257250005692365, w0=73.79999999999995, w1=15.057368515333428\n",
      "SubGD iter. 39/499: loss=4.243104892538491, w0=73.09999999999995, w1=14.065279413715832\n",
      "SubGD iter. 40/499: loss=4.242007129969912, w0=73.69999999999995, w1=14.884871398131482\n",
      "SubGD iter. 41/499: loss=4.221611866384504, w0=72.89999999999995, w1=14.656003652917544\n",
      "SubGD iter. 42/499: loss=4.220350033703662, w0=73.59999999999995, w1=14.647921584245557\n",
      "SubGD iter. 43/499: loss=4.213335518921779, w0=72.99999999999996, w1=14.869465677120353\n",
      "SubGD iter. 44/499: loss=4.215593906939569, w0=73.49999999999996, w1=14.737599159493747\n",
      "SubGD iter. 45/499: loss=4.2061427714423605, w0=73.19999999999996, w1=14.883177405355791\n",
      "SubGD iter. 46/499: loss=4.207847815978823, w0=73.39999999999996, w1=14.646810070482886\n",
      "SubGD iter. 47/499: loss=4.204464463074522, w0=73.09999999999997, w1=14.79238831634493\n",
      "SubGD iter. 48/499: loss=4.209690915961633, w0=73.49999999999997, w1=14.769418651736833\n",
      "SubGD iter. 49/499: loss=4.205679548855864, w0=73.19999999999997, w1=14.914996897598877\n",
      "SubGD iter. 50/499: loss=4.209123295202678, w0=73.39999999999998, w1=14.330693489524885\n",
      "SubGD iter. 51/499: loss=4.21781271203649, w0=73.19999999999997, w1=14.79289185702222\n",
      "SubGD iter. 52/499: loss=4.206050003238306, w0=73.49999999999997, w1=14.635549979284457\n",
      "SubGD iter. 53/499: loss=4.2085709756953325, w0=72.99999999999997, w1=14.906664032204006\n",
      "SubGD iter. 54/499: loss=4.2160844286942, w0=73.49999999999997, w1=14.7747975145774\n",
      "SubGD iter. 55/499: loss=4.205601244314157, w0=73.19999999999997, w1=14.920375760439445\n",
      "SubGD iter. 56/499: loss=4.209437583991607, w0=73.39999999999998, w1=14.336072352365452\n",
      "SubGD iter. 57/499: loss=4.2175641018741, w0=73.19999999999997, w1=14.798270719862787\n",
      "SubGD iter. 58/499: loss=4.20613463527625, w0=73.49999999999997, w1=14.640928842125025\n",
      "SubGD iter. 59/499: loss=4.208425147164851, w0=72.99999999999997, w1=14.912042895044573\n",
      "SubGD iter. 60/499: loss=4.216162040744292, w0=73.59999999999997, w1=14.707051234621431\n",
      "SubGD iter. 61/499: loss=4.212025536446327, w0=72.99999999999997, w1=14.928595327496227\n",
      "SubGD iter. 62/499: loss=4.216682760031847, w0=73.49999999999997, w1=14.58923145394342\n",
      "SubGD iter. 63/499: loss=4.209826736008378, w0=72.99999999999997, w1=14.860345506862968\n",
      "SubGD iter. 64/499: loss=4.215473642430368, w0=73.49999999999997, w1=14.728478989236363\n",
      "SubGD iter. 65/499: loss=4.206275541281164, w0=73.19999999999997, w1=14.874057235098407\n",
      "SubGD iter. 66/499: loss=4.207632244945091, w0=73.39999999999998, w1=14.637689900225501\n",
      "SubGD iter. 67/499: loss=4.204597232913326, w0=73.09999999999998, w1=14.783268146087545\n",
      "SubGD iter. 68/499: loss=4.209669967236433, w0=73.49999999999999, w1=14.760298481479449\n",
      "SubGD iter. 69/499: loss=4.205812318694668, w0=73.19999999999999, w1=14.905876727341493\n",
      "SubGD iter. 70/499: loss=4.208590400546316, w0=73.39999999999999, w1=14.3215733192675\n",
      "SubGD iter. 71/499: loss=4.2183017228842505, w0=73.3, w1=14.876134262479365\n",
      "SubGD iter. 72/499: loss=4.206427791681113, w0=73.3, w1=14.388123953553375\n",
      "SubGD iter. 73/499: loss=4.214164750225292, w0=73.39999999999999, w1=14.958684659063147\n",
      "SubGD iter. 74/499: loss=4.212842246057698, w0=73.19999999999999, w1=14.2559057238036\n",
      "SubGD iter. 75/499: loss=4.2244105480127985, w0=73.49999999999999, w1=14.899120307672582\n",
      "SubGD iter. 76/499: loss=4.211263131764242, w0=73.09999999999998, w1=14.184076243200527\n",
      "SubGD iter. 77/499: loss=4.232596907128464, w0=73.59999999999998, w1=14.937477609070738\n",
      "SubGD iter. 78/499: loss=4.2185167778787465, w0=73.09999999999998, w1=14.244746900285705\n",
      "SubGD iter. 79/499: loss=4.228141343006037, w0=73.49999999999999, w1=14.966726392915827\n",
      "SubGD iter. 80/499: loss=4.216097264761778, w0=73.09999999999998, w1=14.251682328443772\n",
      "SubGD iter. 81/499: loss=4.227682202961687, w0=73.39999999999998, w1=14.894896912312754\n",
      "SubGD iter. 82/499: loss=4.208359377583314, w0=73.19999999999997, w1=14.192117977053208\n",
      "SubGD iter. 83/499: loss=4.2285134689109976, w0=73.49999999999997, w1=14.83533256092219\n",
      "SubGD iter. 84/499: loss=4.206804796793204, w0=73.19999999999997, w1=14.33166111435629\n",
      "SubGD iter. 85/499: loss=4.21953785081178, w0=73.49999999999997, w1=14.974875698225272\n",
      "SubGD iter. 86/499: loss=4.216700783243312, w0=73.19999999999997, w1=14.222526802920973\n",
      "SubGD iter. 87/499: loss=4.22655752888335, w0=73.49999999999997, w1=14.865741386789955\n",
      "SubGD iter. 88/499: loss=4.208876391838681, w0=73.09999999999997, w1=14.1506973223179\n",
      "SubGD iter. 89/499: loss=4.235111679586888, w0=73.59999999999997, w1=14.90409868818811\n",
      "SubGD iter. 90/499: loss=4.216204517526595, w0=73.09999999999997, w1=14.211367979403077\n",
      "SubGD iter. 91/499: loss=4.230551232642377, w0=73.49999999999997, w1=14.9333474720332\n",
      "SubGD iter. 92/499: loss=4.213710524836219, w0=73.09999999999997, w1=14.218303407561145\n",
      "SubGD iter. 93/499: loss=4.230050508952102, w0=73.49999999999997, w1=14.940282900191267\n",
      "SubGD iter. 94/499: loss=4.214206438510117, w0=73.09999999999997, w1=14.225238835719212\n",
      "SubGD iter. 95/499: loss=4.229549785261829, w0=73.49999999999997, w1=14.947218328349335\n",
      "SubGD iter. 96/499: loss=4.214702352184017, w0=73.09999999999997, w1=14.23217426387728\n",
      "SubGD iter. 97/499: loss=4.229049061571556, w0=73.49999999999997, w1=14.954153756507402\n",
      "SubGD iter. 98/499: loss=4.215198265857918, w0=73.09999999999997, w1=14.239109692035347\n",
      "SubGD iter. 99/499: loss=4.228548337881282, w0=73.49999999999997, w1=14.96108918466547\n",
      "SubGD iter. 100/499: loss=4.215694179531818, w0=73.09999999999997, w1=14.246045120193415\n",
      "SubGD iter. 101/499: loss=4.228047614191008, w0=73.49999999999997, w1=14.968024612823537\n",
      "SubGD iter. 102/499: loss=4.216190093205716, w0=73.09999999999997, w1=14.252980548351482\n",
      "SubGD iter. 103/499: loss=4.227598699563916, w0=73.39999999999996, w1=14.896195132220464\n",
      "SubGD iter. 104/499: loss=4.208450613743761, w0=73.19999999999996, w1=14.193416196960918\n",
      "SubGD iter. 105/499: loss=4.228429965513228, w0=73.49999999999996, w1=14.8366307808299\n",
      "SubGD iter. 106/499: loss=4.206870184423092, w0=73.19999999999996, w1=14.332959334264\n",
      "SubGD iter. 107/499: loss=4.219454347414011, w0=73.49999999999996, w1=14.976173918132982\n",
      "SubGD iter. 108/499: loss=4.216798454674655, w0=73.19999999999996, w1=14.223825022828683\n",
      "SubGD iter. 109/499: loss=4.2264740254855795, w0=73.49999999999996, w1=14.867039606697665\n",
      "SubGD iter. 110/499: loss=4.2089692202826186, w0=73.09999999999995, w1=14.15199554222561\n",
      "SubGD iter. 111/499: loss=4.235013871521723, w0=73.59999999999995, w1=14.90539690809582\n",
      "SubGD iter. 112/499: loss=4.216294449206279, w0=73.09999999999995, w1=14.212666199310787\n",
      "SubGD iter. 113/499: loss=4.230457503827347, w0=73.49999999999996, w1=14.93464569194091\n",
      "SubGD iter. 114/499: loss=4.213803353280157, w0=73.09999999999995, w1=14.219601627468855\n",
      "SubGD iter. 115/499: loss=4.229956780137074, w0=73.49999999999996, w1=14.941581120098977\n",
      "SubGD iter. 116/499: loss=4.214299266954056, w0=73.09999999999995, w1=14.226537055626922\n",
      "SubGD iter. 117/499: loss=4.229456056446801, w0=73.49999999999996, w1=14.948516548257045\n",
      "SubGD iter. 118/499: loss=4.214795180627955, w0=73.09999999999995, w1=14.23347248378499\n",
      "SubGD iter. 119/499: loss=4.228955332756527, w0=73.49999999999996, w1=14.955451976415112\n",
      "SubGD iter. 120/499: loss=4.215291094301855, w0=73.09999999999995, w1=14.240407911943057\n",
      "SubGD iter. 121/499: loss=4.228454609066253, w0=73.49999999999996, w1=14.96238740457318\n",
      "SubGD iter. 122/499: loss=4.215787007975755, w0=73.09999999999995, w1=14.247343340101125\n",
      "SubGD iter. 123/499: loss=4.227961293019811, w0=73.39999999999995, w1=14.890557923970107\n",
      "SubGD iter. 124/499: loss=4.208054442622557, w0=73.19999999999995, w1=14.18777898871056\n",
      "SubGD iter. 125/499: loss=4.228792558969121, w0=73.49999999999994, w1=14.830993572579542\n",
      "SubGD iter. 126/499: loss=4.206586254339686, w0=73.19999999999995, w1=14.327322126013643\n",
      "SubGD iter. 127/499: loss=4.2198169408699036, w0=73.49999999999994, w1=14.970536709882625\n",
      "SubGD iter. 128/499: loss=4.21637433993468, w0=73.19999999999995, w1=14.218187814578325\n",
      "SubGD iter. 129/499: loss=4.226836618941474, w0=73.49999999999994, w1=14.861402398447307\n",
      "SubGD iter. 130/499: loss=4.208566135052657, w0=73.09999999999994, w1=14.146358333975252\n",
      "SubGD iter. 131/499: loss=4.235438579561275, w0=73.59999999999994, w1=14.899759699845463\n",
      "SubGD iter. 132/499: loss=4.215903942479594, w0=73.09999999999994, w1=14.20702899106043\n",
      "SubGD iter. 133/499: loss=4.230867643969653, w0=73.59999999999994, w1=14.96043035693064\n",
      "SubGD iter. 134/499: loss=4.220106785208101, w0=73.09999999999994, w1=14.267699648145607\n",
      "SubGD iter. 135/499: loss=4.226651945599016, w0=73.39999999999993, w1=14.910914232014589\n",
      "SubGD iter. 136/499: loss=4.209485041071889, w0=73.19999999999993, w1=14.208135296755042\n",
      "SubGD iter. 137/499: loss=4.227483211548328, w0=73.49999999999993, w1=14.851349880624024\n",
      "SubGD iter. 138/499: loss=4.207847335732403, w0=73.09999999999992, w1=14.13630581615197\n",
      "SubGD iter. 139/499: loss=4.236195937627124, w0=73.59999999999992, w1=14.88970718202218\n",
      "SubGD iter. 140/499: loss=4.215207573699913, w0=73.09999999999992, w1=14.196976473237147\n",
      "SubGD iter. 141/499: loss=4.2316250020355035, w0=73.59999999999992, w1=14.950377839107357\n",
      "SubGD iter. 142/499: loss=4.2194104164284205, w0=73.09999999999992, w1=14.257647130322324\n",
      "SubGD iter. 143/499: loss=4.227298538205871, w0=73.39999999999992, w1=14.900861714191306\n",
      "SubGD iter. 144/499: loss=4.208778571294635, w0=73.19999999999992, w1=14.19808277893176\n",
      "SubGD iter. 145/499: loss=4.2281298041551825, w0=73.49999999999991, w1=14.841297362800741\n",
      "SubGD iter. 146/499: loss=4.207128536412148, w0=73.09999999999991, w1=14.126253298328686\n",
      "SubGD iter. 147/499: loss=4.237009759261177, w0=73.6999999999999, w1=14.945845282744337\n",
      "SubGD iter. 148/499: loss=4.225040794175607, w0=72.9999999999999, w1=13.95375618112674\n",
      "SubGD iter. 149/499: loss=4.257264312220685, w0=73.7999999999999, w1=15.05723886646348\n",
      "SubGD iter. 150/499: loss=4.243092030215395, w0=73.0999999999999, w1=14.065149764845884\n",
      "SubGD iter. 151/499: loss=4.242017755887375, w0=73.69999999999989, w1=14.884741749261535\n",
      "SubGD iter. 152/499: loss=4.221608899140046, w0=72.89999999999989, w1=14.655874004047597\n",
      "SubGD iter. 153/499: loss=4.22034992892056, w0=73.5999999999999, w1=14.64779193537561\n",
      "SubGD iter. 154/499: loss=4.2133383912159035, w0=72.9999999999999, w1=14.869336028250405\n",
      "SubGD iter. 155/499: loss=4.215592197305072, w0=73.4999999999999, w1=14.7374695106238\n",
      "SubGD iter. 156/499: loss=4.206144658847864, w0=73.1999999999999, w1=14.883047756485844\n",
      "SubGD iter. 157/499: loss=4.207844751503039, w0=73.3999999999999, w1=14.646680421612938\n",
      "SubGD iter. 158/499: loss=4.204466350480027, w0=73.09999999999991, w1=14.792258667474982\n",
      "SubGD iter. 159/499: loss=4.209690618162528, w0=73.49999999999991, w1=14.769289002866886\n",
      "SubGD iter. 160/499: loss=4.205681436261369, w0=73.19999999999992, w1=14.91486724872893\n",
      "SubGD iter. 161/499: loss=4.209115719775022, w0=73.39999999999992, w1=14.330563840654937\n",
      "SubGD iter. 162/499: loss=4.217818704386093, w0=73.19999999999992, w1=14.792762208152272\n",
      "SubGD iter. 163/499: loss=4.206047963318644, w0=73.49999999999991, w1=14.63542033041451\n",
      "SubGD iter. 164/499: loss=4.208574490658387, w0=72.99999999999991, w1=14.906534383334058\n",
      "SubGD iter. 165/499: loss=4.216082719059703, w0=73.49999999999991, w1=14.774667865707453\n",
      "SubGD iter. 166/499: loss=4.205603131719663, w0=73.19999999999992, w1=14.920246111569497\n",
      "SubGD iter. 167/499: loss=4.209430008563953, w0=73.39999999999992, w1=14.335942703495505\n",
      "SubGD iter. 168/499: loss=4.217570094223702, w0=73.19999999999992, w1=14.79814107099284\n",
      "SubGD iter. 169/499: loss=4.2061325953565865, w0=73.49999999999991, w1=14.640799193255077\n",
      "SubGD iter. 170/499: loss=4.208428662127908, w0=72.99999999999991, w1=14.911913246174626\n",
      "SubGD iter. 171/499: loss=4.2161593830505835, w0=73.59999999999991, w1=14.706921585751484\n",
      "SubGD iter. 172/499: loss=4.212028408740452, w0=72.99999999999991, w1=14.92846567862628\n",
      "SubGD iter. 173/499: loss=4.216678360217578, w0=73.49999999999991, w1=14.589101805073472\n",
      "SubGD iter. 174/499: loss=4.209830250971435, w0=72.99999999999991, w1=14.86021585799302\n",
      "SubGD iter. 175/499: loss=4.215471932795872, w0=73.49999999999991, w1=14.728349340366416\n",
      "SubGD iter. 176/499: loss=4.206277428686668, w0=73.19999999999992, w1=14.87392758622846\n",
      "SubGD iter. 177/499: loss=4.207629180469307, w0=73.39999999999992, w1=14.637560251355554\n",
      "SubGD iter. 178/499: loss=4.2045991203188295, w0=73.09999999999992, w1=14.783138497217598\n",
      "SubGD iter. 179/499: loss=4.20966966943733, w0=73.49999999999993, w1=14.760168832609502\n",
      "SubGD iter. 180/499: loss=4.205814206100173, w0=73.19999999999993, w1=14.905747078471546\n",
      "SubGD iter. 181/499: loss=4.208582825118661, w0=73.39999999999993, w1=14.321443670397553\n",
      "SubGD iter. 182/499: loss=4.21830891270421, w0=73.29999999999994, w1=14.876004613609418\n",
      "SubGD iter. 183/499: loss=4.206421464682607, w0=73.29999999999994, w1=14.387994304683428\n",
      "SubGD iter. 184/499: loss=4.2141721474803635, w0=73.39999999999993, w1=14.9585550101932\n",
      "SubGD iter. 185/499: loss=4.212833134608219, w0=73.19999999999993, w1=14.255776074933653\n",
      "SubGD iter. 186/499: loss=4.224418887217193, w0=73.49999999999993, w1=14.898990658802635\n",
      "SubGD iter. 187/499: loss=4.211253861298747, w0=73.09999999999992, w1=14.18394659433058\n",
      "SubGD iter. 188/499: loss=4.232606674892036, w0=73.59999999999992, w1=14.93734796020079\n",
      "SubGD iter. 189/499: loss=4.218507796703387, w0=73.09999999999992, w1=14.244617251415757\n",
      "SubGD iter. 190/499: loss=4.2281507033885735, w0=73.49999999999993, w1=14.96659674404588\n",
      "SubGD iter. 191/499: loss=4.216087994296284, w0=73.09999999999992, w1=14.251552679573825\n",
      "SubGD iter. 192/499: loss=4.227690542166083, w0=73.39999999999992, w1=14.894767263442807\n",
      "SubGD iter. 193/499: loss=4.2083502661338335, w0=73.19999999999992, w1=14.19198832818326\n",
      "SubGD iter. 194/499: loss=4.228521808115392, w0=73.49999999999991, w1=14.835202912052242\n",
      "SubGD iter. 195/499: loss=4.206798266749815, w0=73.19999999999992, w1=14.331531465486343\n",
      "SubGD iter. 196/499: loss=4.2195461900161755, w0=73.49999999999991, w1=14.974746049355325\n",
      "SubGD iter. 197/499: loss=4.216691029124903, w0=73.19999999999992, w1=14.222397154051025\n",
      "SubGD iter. 198/499: loss=4.226565868087746, w0=73.49999999999991, w1=14.865611737920007\n",
      "SubGD iter. 199/499: loss=4.208867121373186, w0=73.09999999999991, w1=14.150567673447952\n",
      "SubGD iter. 200/499: loss=4.235121447350461, w0=73.59999999999991, w1=14.903969039318163\n",
      "SubGD iter. 201/499: loss=4.216195536351235, w0=73.09999999999991, w1=14.21123833053313\n",
      "SubGD iter. 202/499: loss=4.2305605930249115, w0=73.49999999999991, w1=14.933217823163252\n",
      "SubGD iter. 203/499: loss=4.213701254370722, w0=73.09999999999991, w1=14.218173758691197\n",
      "SubGD iter. 204/499: loss=4.230059869334639, w0=73.49999999999991, w1=14.94015325132132\n",
      "SubGD iter. 205/499: loss=4.214197168044623, w0=73.09999999999991, w1=14.225109186849265\n",
      "SubGD iter. 206/499: loss=4.229559145644366, w0=73.49999999999991, w1=14.947088679479387\n",
      "SubGD iter. 207/499: loss=4.214693081718523, w0=73.09999999999991, w1=14.232044615007332\n",
      "SubGD iter. 208/499: loss=4.229058421954092, w0=73.49999999999991, w1=14.954024107637455\n",
      "SubGD iter. 209/499: loss=4.215188995392423, w0=73.09999999999991, w1=14.2389800431654\n",
      "SubGD iter. 210/499: loss=4.228557698263819, w0=73.49999999999991, w1=14.960959535795523\n",
      "SubGD iter. 211/499: loss=4.215684909066323, w0=73.09999999999991, w1=14.245915471323467\n",
      "SubGD iter. 212/499: loss=4.228056974573545, w0=73.49999999999991, w1=14.96789496395359\n",
      "SubGD iter. 213/499: loss=4.216180822740222, w0=73.09999999999991, w1=14.252850899481535\n",
      "SubGD iter. 214/499: loss=4.227607038768311, w0=73.3999999999999, w1=14.896065483350517\n",
      "SubGD iter. 215/499: loss=4.208441502294281, w0=73.1999999999999, w1=14.19328654809097\n",
      "SubGD iter. 216/499: loss=4.228438304717622, w0=73.4999999999999, w1=14.836501131959952\n",
      "SubGD iter. 217/499: loss=4.206863654379703, w0=73.1999999999999, w1=14.332829685394053\n",
      "SubGD iter. 218/499: loss=4.219462686618405, w0=73.4999999999999, w1=14.976044269263035\n",
      "SubGD iter. 219/499: loss=4.216788700556245, w0=73.1999999999999, w1=14.223695373958735\n",
      "SubGD iter. 220/499: loss=4.226482364689975, w0=73.4999999999999, w1=14.866909957827717\n",
      "SubGD iter. 221/499: loss=4.208959949817125, w0=73.0999999999999, w1=14.151865893355662\n",
      "SubGD iter. 222/499: loss=4.235023639285295, w0=73.5999999999999, w1=14.905267259225873\n",
      "SubGD iter. 223/499: loss=4.216285468030918, w0=73.0999999999999, w1=14.21253655044084\n",
      "SubGD iter. 224/499: loss=4.230466864209884, w0=73.4999999999999, w1=14.934516043070962\n",
      "SubGD iter. 225/499: loss=4.213794082814661, w0=73.0999999999999, w1=14.219471978598907\n",
      "SubGD iter. 226/499: loss=4.22996614051961, w0=73.4999999999999, w1=14.94145147122903\n",
      "SubGD iter. 227/499: loss=4.214289996488561, w0=73.0999999999999, w1=14.226407406756975\n",
      "SubGD iter. 228/499: loss=4.229465416829337, w0=73.4999999999999, w1=14.948386899387097\n",
      "SubGD iter. 229/499: loss=4.214785910162461, w0=73.0999999999999, w1=14.233342834915042\n",
      "SubGD iter. 230/499: loss=4.228964693139065, w0=73.4999999999999, w1=14.955322327545165\n",
      "SubGD iter. 231/499: loss=4.215281823836361, w0=73.0999999999999, w1=14.24027826307311\n",
      "SubGD iter. 232/499: loss=4.228463969448789, w0=73.4999999999999, w1=14.962257755703233\n",
      "SubGD iter. 233/499: loss=4.215777737510262, w0=73.0999999999999, w1=14.247213691231178\n",
      "SubGD iter. 234/499: loss=4.227969632224205, w0=73.39999999999989, w1=14.89042827510016\n",
      "SubGD iter. 235/499: loss=4.2080453311730786, w0=73.19999999999989, w1=14.187649339840613\n",
      "SubGD iter. 236/499: loss=4.228800898173517, w0=73.49999999999989, w1=14.830863923709595\n",
      "SubGD iter. 237/499: loss=4.206579724296297, w0=73.19999999999989, w1=14.327192477143695\n",
      "SubGD iter. 238/499: loss=4.219825280074299, w0=73.49999999999989, w1=14.970407061012677\n",
      "SubGD iter. 239/499: loss=4.216364585816269, w0=73.19999999999989, w1=14.218058165708378\n",
      "SubGD iter. 240/499: loss=4.226844958145868, w0=73.49999999999989, w1=14.86127274957736\n",
      "SubGD iter. 241/499: loss=4.2085568645871625, w0=73.09999999999988, w1=14.146228685105305\n",
      "SubGD iter. 242/499: loss=4.235448347324847, w0=73.59999999999988, w1=14.899630050975516\n",
      "SubGD iter. 243/499: loss=4.215894961304233, w0=73.09999999999988, w1=14.206899342190482\n",
      "SubGD iter. 244/499: loss=4.230877411733226, w0=73.59999999999988, w1=14.960300708060693\n",
      "SubGD iter. 245/499: loss=4.22009780403274, w0=73.09999999999988, w1=14.26756999927566\n",
      "SubGD iter. 246/499: loss=4.226660284803412, w0=73.39999999999988, w1=14.910784583144642\n",
      "SubGD iter. 247/499: loss=4.2094759296224105, w0=73.19999999999987, w1=14.208005647885095\n",
      "SubGD iter. 248/499: loss=4.227491550752722, w0=73.49999999999987, w1=14.851220231754077\n",
      "SubGD iter. 249/499: loss=4.207838065266908, w0=73.09999999999987, w1=14.136176167282022\n",
      "SubGD iter. 250/499: loss=4.236205705390697, w0=73.59999999999987, w1=14.889577533152233\n",
      "SubGD iter. 251/499: loss=4.215198592524553, w0=73.09999999999987, w1=14.1968468243672\n",
      "SubGD iter. 252/499: loss=4.231634769799077, w0=73.59999999999987, w1=14.95024819023741\n",
      "SubGD iter. 253/499: loss=4.219401435253061, w0=73.09999999999987, w1=14.257517481452377\n",
      "SubGD iter. 254/499: loss=4.227306877410267, w0=73.39999999999986, w1=14.900732065321359\n",
      "SubGD iter. 255/499: loss=4.208769459845157, w0=73.19999999999986, w1=14.197953130061812\n",
      "SubGD iter. 256/499: loss=4.228138143359578, w0=73.49999999999986, w1=14.841167713930794\n",
      "SubGD iter. 257/499: loss=4.207119265946654, w0=73.09999999999985, w1=14.126123649458739\n",
      "SubGD iter. 258/499: loss=4.237020385178639, w0=73.69999999999985, w1=14.94571563387439\n",
      "SubGD iter. 259/499: loss=4.2250279318525115, w0=72.99999999999984, w1=13.953626532256793\n",
      "SubGD iter. 260/499: loss=4.257278618749006, w0=73.79999999999984, w1=15.057109217593533\n",
      "SubGD iter. 261/499: loss=4.243079167892301, w0=73.09999999999984, w1=14.065020115975937\n",
      "SubGD iter. 262/499: loss=4.24202838180484, w0=73.69999999999983, w1=14.884612100391587\n",
      "SubGD iter. 263/499: loss=4.221605931895588, w0=72.89999999999984, w1=14.65574435517765\n",
      "SubGD iter. 264/499: loss=4.220349824137457, w0=73.59999999999984, w1=14.647662286505662\n",
      "SubGD iter. 265/499: loss=4.21334126351003, w0=72.99999999999984, w1=14.869206379380458\n",
      "SubGD iter. 266/499: loss=4.215590487670576, w0=73.49999999999984, w1=14.737339861753853\n",
      "SubGD iter. 267/499: loss=4.206146546253369, w0=73.19999999999985, w1=14.882918107615897\n",
      "SubGD iter. 268/499: loss=4.207841687027255, w0=73.39999999999985, w1=14.64655077274299\n",
      "SubGD iter. 269/499: loss=4.204468237885532, w0=73.09999999999985, w1=14.792129018605035\n",
      "SubGD iter. 270/499: loss=4.209690320363425, w0=73.49999999999986, w1=14.769159353996939\n",
      "SubGD iter. 271/499: loss=4.2056833236668725, w0=73.19999999999986, w1=14.914737599858983\n",
      "SubGD iter. 272/499: loss=4.2091081443473675, w0=73.39999999999986, w1=14.33043419178499\n",
      "SubGD iter. 273/499: loss=4.217824696735695, w0=73.19999999999986, w1=14.792632559282325\n",
      "SubGD iter. 274/499: loss=4.206045923398982, w0=73.49999999999986, w1=14.635290681544562\n",
      "SubGD iter. 275/499: loss=4.208578005621444, w0=72.99999999999986, w1=14.906404734464111\n",
      "SubGD iter. 276/499: loss=4.216081009425207, w0=73.49999999999986, w1=14.774538216837506\n",
      "SubGD iter. 277/499: loss=4.205605019125167, w0=73.19999999999986, w1=14.92011646269955\n",
      "SubGD iter. 278/499: loss=4.2094224331362975, w0=73.39999999999986, w1=14.335813054625557\n",
      "SubGD iter. 279/499: loss=4.217576086573305, w0=73.19999999999986, w1=14.798011422122892\n",
      "SubGD iter. 280/499: loss=4.206130555436923, w0=73.49999999999986, w1=14.64066954438513\n",
      "SubGD iter. 281/499: loss=4.208432177090964, w0=72.99999999999986, w1=14.911783597304678\n",
      "SubGD iter. 282/499: loss=4.216156725356874, w0=73.59999999999985, w1=14.706791936881537\n",
      "SubGD iter. 283/499: loss=4.2120312810345775, w0=72.99999999999986, w1=14.928336029756332\n",
      "SubGD iter. 284/499: loss=4.21667396040331, w0=73.49999999999986, w1=14.588972156203525\n",
      "SubGD iter. 285/499: loss=4.209833765934491, w0=72.99999999999986, w1=14.860086209123073\n",
      "SubGD iter. 286/499: loss=4.2154702231613745, w0=73.49999999999986, w1=14.728219691496468\n",
      "SubGD iter. 287/499: loss=4.206279316092173, w0=73.19999999999986, w1=14.873797937358512\n",
      "SubGD iter. 288/499: loss=4.207626115993523, w0=73.39999999999986, w1=14.637430602485606\n",
      "SubGD iter. 289/499: loss=4.204601007724334, w0=73.09999999999987, w1=14.78300884834765\n",
      "SubGD iter. 290/499: loss=4.209669371638227, w0=73.49999999999987, w1=14.760039183739554\n",
      "SubGD iter. 291/499: loss=4.205816093505677, w0=73.19999999999987, w1=14.905617429601598\n",
      "SubGD iter. 292/499: loss=4.208575249691005, w0=73.39999999999988, w1=14.321314021527606\n",
      "SubGD iter. 293/499: loss=4.218316102524171, w0=73.29999999999988, w1=14.87587496473947\n",
      "SubGD iter. 294/499: loss=4.206415137684099, w0=73.29999999999988, w1=14.38786465581348\n",
      "SubGD iter. 295/499: loss=4.214179544735434, w0=73.39999999999988, w1=14.958425361323252\n",
      "SubGD iter. 296/499: loss=4.21282402315874, w0=73.19999999999987, w1=14.255646426063706\n",
      "SubGD iter. 297/499: loss=4.2244272264215885, w0=73.49999999999987, w1=14.898861009932688\n",
      "SubGD iter. 298/499: loss=4.211244590833252, w0=73.09999999999987, w1=14.183816945460633\n",
      "SubGD iter. 299/499: loss=4.232616442655609, w0=73.59999999999987, w1=14.937218311330843\n",
      "SubGD iter. 300/499: loss=4.218498815528027, w0=73.09999999999987, w1=14.24448760254581\n",
      "SubGD iter. 301/499: loss=4.22816006377111, w0=73.49999999999987, w1=14.966467095175933\n",
      "SubGD iter. 302/499: loss=4.216078723830791, w0=73.09999999999987, w1=14.251423030703878\n",
      "SubGD iter. 303/499: loss=4.227698881370476, w0=73.39999999999986, w1=14.89463761457286\n",
      "SubGD iter. 304/499: loss=4.208341154684355, w0=73.19999999999986, w1=14.191858679313313\n",
      "SubGD iter. 305/499: loss=4.2285301473197885, w0=73.49999999999986, w1=14.835073263182295\n",
      "SubGD iter. 306/499: loss=4.2067917367064265, w0=73.19999999999986, w1=14.331401816616395\n",
      "SubGD iter. 307/499: loss=4.21955452922057, w0=73.49999999999986, w1=14.974616400485377\n",
      "SubGD iter. 308/499: loss=4.2166812750064935, w0=73.19999999999986, w1=14.222267505181078\n",
      "SubGD iter. 309/499: loss=4.226574207292139, w0=73.49999999999986, w1=14.86548208905006\n",
      "SubGD iter. 310/499: loss=4.208857850907691, w0=73.09999999999985, w1=14.150438024578005\n",
      "SubGD iter. 311/499: loss=4.235131215114034, w0=73.59999999999985, w1=14.903839390448216\n",
      "SubGD iter. 312/499: loss=4.2161865551758755, w0=73.09999999999985, w1=14.211108681663182\n",
      "SubGD iter. 313/499: loss=4.23056995340745, w0=73.49999999999986, w1=14.933088174293305\n",
      "SubGD iter. 314/499: loss=4.213691983905229, w0=73.09999999999985, w1=14.21804410982125\n",
      "SubGD iter. 315/499: loss=4.230069229717176, w0=73.49999999999986, w1=14.940023602451372\n",
      "SubGD iter. 316/499: loss=4.214187897579129, w0=73.09999999999985, w1=14.224979537979317\n",
      "SubGD iter. 317/499: loss=4.229568506026903, w0=73.49999999999986, w1=14.94695903060944\n",
      "SubGD iter. 318/499: loss=4.214683811253029, w0=73.09999999999985, w1=14.231914966137385\n",
      "SubGD iter. 319/499: loss=4.229067782336629, w0=73.49999999999986, w1=14.953894458767508\n",
      "SubGD iter. 320/499: loss=4.215179724926929, w0=73.09999999999985, w1=14.238850394295453\n",
      "SubGD iter. 321/499: loss=4.228567058646355, w0=73.49999999999986, w1=14.960829886925575\n",
      "SubGD iter. 322/499: loss=4.215675638600828, w0=73.09999999999985, w1=14.24578582245352\n",
      "SubGD iter. 323/499: loss=4.228066334956082, w0=73.49999999999986, w1=14.967765315083643\n",
      "SubGD iter. 324/499: loss=4.216171552274728, w0=73.09999999999985, w1=14.252721250611588\n",
      "SubGD iter. 325/499: loss=4.227615377972707, w0=73.39999999999985, w1=14.89593583448057\n",
      "SubGD iter. 326/499: loss=4.208432390844803, w0=73.19999999999985, w1=14.193156899221023\n",
      "SubGD iter. 327/499: loss=4.228446643922017, w0=73.49999999999984, w1=14.836371483090005\n",
      "SubGD iter. 328/499: loss=4.206857124336315, w0=73.19999999999985, w1=14.332700036524106\n",
      "SubGD iter. 329/499: loss=4.2194710258228, w0=73.49999999999984, w1=14.975914620393088\n",
      "SubGD iter. 330/499: loss=4.2167789464378345, w0=73.19999999999985, w1=14.223565725088788\n",
      "SubGD iter. 331/499: loss=4.2264907038943695, w0=73.49999999999984, w1=14.86678030895777\n",
      "SubGD iter. 332/499: loss=4.20895067935163, w0=73.09999999999984, w1=14.151736244485715\n",
      "SubGD iter. 333/499: loss=4.235033407048868, w0=73.59999999999984, w1=14.905137610355926\n",
      "SubGD iter. 334/499: loss=4.216276486855558, w0=73.09999999999984, w1=14.212406901570892\n",
      "SubGD iter. 335/499: loss=4.2304762245924215, w0=73.49999999999984, w1=14.934386394201015\n",
      "SubGD iter. 336/499: loss=4.213784812349168, w0=73.09999999999984, w1=14.21934232972896\n",
      "SubGD iter. 337/499: loss=4.229975500902148, w0=73.49999999999984, w1=14.941321822359082\n",
      "SubGD iter. 338/499: loss=4.214280726023066, w0=73.09999999999984, w1=14.226277757887027\n",
      "SubGD iter. 339/499: loss=4.229474777211873, w0=73.49999999999984, w1=14.94825725051715\n",
      "SubGD iter. 340/499: loss=4.214776639696967, w0=73.09999999999984, w1=14.233213186045095\n",
      "SubGD iter. 341/499: loss=4.2289740535216005, w0=73.49999999999984, w1=14.955192678675218\n",
      "SubGD iter. 342/499: loss=4.215272553370867, w0=73.09999999999984, w1=14.240148614203163\n",
      "SubGD iter. 343/499: loss=4.228473329831326, w0=73.49999999999984, w1=14.962128106833285\n",
      "SubGD iter. 344/499: loss=4.215768467044765, w0=73.09999999999984, w1=14.24708404236123\n",
      "SubGD iter. 345/499: loss=4.2279779714286, w0=73.39999999999984, w1=14.890298626230212\n",
      "SubGD iter. 346/499: loss=4.208036219723599, w0=73.19999999999983, w1=14.187519690970666\n",
      "SubGD iter. 347/499: loss=4.228809237377911, w0=73.49999999999983, w1=14.830734274839648\n",
      "SubGD iter. 348/499: loss=4.2065731942529085, w0=73.19999999999983, w1=14.327062828273748\n",
      "SubGD iter. 349/499: loss=4.2198336192786945, w0=73.49999999999983, w1=14.97027741214273\n",
      "SubGD iter. 350/499: loss=4.216354831697859, w0=73.19999999999983, w1=14.21792851683843\n",
      "SubGD iter. 351/499: loss=4.226853297350264, w0=73.49999999999983, w1=14.861143100707412\n",
      "SubGD iter. 352/499: loss=4.208547594121668, w0=73.09999999999982, w1=14.146099036235357\n",
      "SubGD iter. 353/499: loss=4.235458115088421, w0=73.59999999999982, w1=14.899500402105568\n",
      "SubGD iter. 354/499: loss=4.215885980128873, w0=73.09999999999982, w1=14.206769693320535\n",
      "SubGD iter. 355/499: loss=4.230887179496799, w0=73.59999999999982, w1=14.960171059190746\n",
      "SubGD iter. 356/499: loss=4.22008882285738, w0=73.09999999999982, w1=14.267440350405712\n",
      "SubGD iter. 357/499: loss=4.226668624007806, w0=73.39999999999982, w1=14.910654934274694\n",
      "SubGD iter. 358/499: loss=4.209466818172931, w0=73.19999999999982, w1=14.207875999015148\n",
      "SubGD iter. 359/499: loss=4.227499889957118, w0=73.49999999999982, w1=14.85109058288413\n",
      "SubGD iter. 360/499: loss=4.207828794801414, w0=73.09999999999981, w1=14.136046518412074\n",
      "SubGD iter. 361/499: loss=4.236215473154271, w0=73.59999999999981, w1=14.889447884282285\n",
      "SubGD iter. 362/499: loss=4.215189611349193, w0=73.09999999999981, w1=14.196717175497252\n",
      "SubGD iter. 363/499: loss=4.23164453756265, w0=73.59999999999981, w1=14.950118541367463\n",
      "SubGD iter. 364/499: loss=4.2193924540777, w0=73.09999999999981, w1=14.25738783258243\n",
      "SubGD iter. 365/499: loss=4.227315216614661, w0=73.3999999999998, w1=14.900602416451411\n",
      "SubGD iter. 366/499: loss=4.208760348395678, w0=73.1999999999998, w1=14.197823481191865\n",
      "SubGD iter. 367/499: loss=4.2281464825639725, w0=73.4999999999998, w1=14.841038065060847\n",
      "SubGD iter. 368/499: loss=4.20710999548116, w0=73.0999999999998, w1=14.125994000588792\n",
      "SubGD iter. 369/499: loss=4.237031011096104, w0=73.69999999999979, w1=14.945585985004442\n",
      "SubGD iter. 370/499: loss=4.225015069529417, w0=72.99999999999979, w1=13.953496883386846\n",
      "SubGD iter. 371/499: loss=4.257292925277326, w0=73.79999999999978, w1=15.056979568723586\n",
      "SubGD iter. 372/499: loss=4.243066305569204, w0=73.09999999999978, w1=14.06489046710599\n",
      "SubGD iter. 373/499: loss=4.242039007722302, w0=73.69999999999978, w1=14.88448245152164\n",
      "SubGD iter. 374/499: loss=4.2216029646511295, w0=72.89999999999978, w1=14.655614706307702\n",
      "SubGD iter. 375/499: loss=4.2203497193543535, w0=73.59999999999978, w1=14.647532637635715\n",
      "SubGD iter. 376/499: loss=4.213344135804155, w0=72.99999999999979, w1=14.86907673051051\n",
      "SubGD iter. 377/499: loss=4.2155887780360795, w0=73.49999999999979, w1=14.737210212883905\n",
      "SubGD iter. 378/499: loss=4.206148433658875, w0=73.19999999999979, w1=14.88278845874595\n",
      "SubGD iter. 379/499: loss=4.207838622551471, w0=73.39999999999979, w1=14.646421123873044\n",
      "SubGD iter. 380/499: loss=4.204470125291036, w0=73.0999999999998, w1=14.791999369735088\n",
      "SubGD iter. 381/499: loss=4.209690022564321, w0=73.4999999999998, w1=14.769029705126991\n",
      "SubGD iter. 382/499: loss=4.205685211072378, w0=73.1999999999998, w1=14.914607950989035\n",
      "SubGD iter. 383/499: loss=4.209100568919712, w0=73.3999999999998, w1=14.330304542915043\n",
      "SubGD iter. 384/499: loss=4.2178306890852975, w0=73.1999999999998, w1=14.792502910412377\n",
      "SubGD iter. 385/499: loss=4.206043883479318, w0=73.4999999999998, w1=14.635161032674615\n",
      "SubGD iter. 386/499: loss=4.2085815205845, w0=72.9999999999998, w1=14.906275085594164\n",
      "SubGD iter. 387/499: loss=4.21607929979071, w0=73.4999999999998, w1=14.774408567967559\n",
      "SubGD iter. 388/499: loss=4.205606906530672, w0=73.1999999999998, w1=14.919986813829603\n",
      "SubGD iter. 389/499: loss=4.2094148577086425, w0=73.3999999999998, w1=14.33568340575561\n",
      "SubGD iter. 390/499: loss=4.217582078922907, w0=73.1999999999998, w1=14.797881773252945\n",
      "SubGD iter. 391/499: loss=4.2061285155172605, w0=73.4999999999998, w1=14.640539895515182\n",
      "SubGD iter. 392/499: loss=4.208435692054019, w0=72.9999999999998, w1=14.911653948434731\n",
      "SubGD iter. 393/499: loss=4.216154067663165, w0=73.5999999999998, w1=14.70666228801159\n",
      "SubGD iter. 394/499: loss=4.212034153328703, w0=72.9999999999998, w1=14.928206380886385\n",
      "SubGD iter. 395/499: loss=4.216669560589043, w0=73.4999999999998, w1=14.588842507333577\n",
      "SubGD iter. 396/499: loss=4.209837280897546, w0=72.9999999999998, w1=14.859956560253126\n",
      "SubGD iter. 397/499: loss=4.2154685135268775, w0=73.4999999999998, w1=14.72809004262652\n",
      "SubGD iter. 398/499: loss=4.206281203497678, w0=73.1999999999998, w1=14.873668288488565\n",
      "SubGD iter. 399/499: loss=4.2076230515177375, w0=73.3999999999998, w1=14.637300953615659\n",
      "SubGD iter. 400/499: loss=4.204602895129841, w0=73.09999999999981, w1=14.782879199477703\n",
      "SubGD iter. 401/499: loss=4.209669073839122, w0=73.49999999999982, w1=14.759909534869607\n",
      "SubGD iter. 402/499: loss=4.205817980911182, w0=73.19999999999982, w1=14.90548778073165\n",
      "SubGD iter. 403/499: loss=4.20856767426335, w0=73.39999999999982, w1=14.321184372657658\n",
      "SubGD iter. 404/499: loss=4.21832329234413, w0=73.29999999999983, w1=14.875745315869523\n",
      "SubGD iter. 405/499: loss=4.206408810685591, w0=73.29999999999983, w1=14.387735006943533\n",
      "SubGD iter. 406/499: loss=4.214186941990506, w0=73.39999999999982, w1=14.958295712453305\n",
      "SubGD iter. 407/499: loss=4.21281491170926, w0=73.19999999999982, w1=14.255516777193758\n",
      "SubGD iter. 408/499: loss=4.224435565625982, w0=73.49999999999982, w1=14.89873136106274\n",
      "SubGD iter. 409/499: loss=4.211235320367758, w0=73.09999999999981, w1=14.183687296590685\n",
      "SubGD iter. 410/499: loss=4.232626210419182, w0=73.59999999999981, w1=14.937088662460896\n",
      "SubGD iter. 411/499: loss=4.218489834352668, w0=73.09999999999981, w1=14.244357953675863\n",
      "SubGD iter. 412/499: loss=4.228169424153647, w0=73.49999999999982, w1=14.966337446305985\n",
      "SubGD iter. 413/499: loss=4.216069453365296, w0=73.09999999999981, w1=14.25129338183393\n",
      "SubGD iter. 414/499: loss=4.227707220574872, w0=73.3999999999998, w1=14.894507965702912\n",
      "SubGD iter. 415/499: loss=4.208332043234875, w0=73.1999999999998, w1=14.191729030443366\n",
      "SubGD iter. 416/499: loss=4.228538486524183, w0=73.4999999999998, w1=14.834943614312348\n",
      "SubGD iter. 417/499: loss=4.206785206663039, w0=73.1999999999998, w1=14.331272167746448\n",
      "SubGD iter. 418/499: loss=4.2195628684249655, w0=73.4999999999998, w1=14.97448675161543\n",
      "SubGD iter. 419/499: loss=4.216671520888083, w0=73.1999999999998, w1=14.22213785631113\n",
      "SubGD iter. 420/499: loss=4.226582546496535, w0=73.4999999999998, w1=14.865352440180112\n",
      "SubGD iter. 421/499: loss=4.208848580442197, w0=73.0999999999998, w1=14.150308375708057\n",
      "SubGD iter. 422/499: loss=4.235140982877607, w0=73.5999999999998, w1=14.903709741578268\n",
      "SubGD iter. 423/499: loss=4.216177574000516, w0=73.0999999999998, w1=14.210979032793235\n",
      "SubGD iter. 424/499: loss=4.230579313789986, w0=73.4999999999998, w1=14.932958525423357\n",
      "SubGD iter. 425/499: loss=4.213682713439734, w0=73.0999999999998, w1=14.217914460951302\n",
      "SubGD iter. 426/499: loss=4.230078590099712, w0=73.4999999999998, w1=14.939893953581425\n",
      "SubGD iter. 427/499: loss=4.2141786271136334, w0=73.0999999999998, w1=14.22484988910937\n",
      "SubGD iter. 428/499: loss=4.229577866409439, w0=73.4999999999998, w1=14.946829381739493\n",
      "SubGD iter. 429/499: loss=4.214674540787534, w0=73.0999999999998, w1=14.231785317267438\n",
      "SubGD iter. 430/499: loss=4.229077142719166, w0=73.4999999999998, w1=14.95376480989756\n",
      "SubGD iter. 431/499: loss=4.2151704544614335, w0=73.0999999999998, w1=14.238720745425505\n",
      "SubGD iter. 432/499: loss=4.228576419028892, w0=73.4999999999998, w1=14.960700238055628\n",
      "SubGD iter. 433/499: loss=4.2156663681353335, w0=73.0999999999998, w1=14.245656173583573\n",
      "SubGD iter. 434/499: loss=4.228075695338618, w0=73.4999999999998, w1=14.967635666213695\n",
      "SubGD iter. 435/499: loss=4.2161622818092335, w0=73.0999999999998, w1=14.25259160174164\n",
      "SubGD iter. 436/499: loss=4.227623717177101, w0=73.39999999999979, w1=14.895806185610622\n",
      "SubGD iter. 437/499: loss=4.208423279395323, w0=73.19999999999979, w1=14.193027250351076\n",
      "SubGD iter. 438/499: loss=4.228454983126412, w0=73.49999999999979, w1=14.836241834220058\n",
      "SubGD iter. 439/499: loss=4.206850594292925, w0=73.19999999999979, w1=14.332570387654158\n",
      "SubGD iter. 440/499: loss=4.219479365027195, w0=73.49999999999979, w1=14.97578497152314\n",
      "SubGD iter. 441/499: loss=4.2167691923194255, w0=73.19999999999979, w1=14.22343607621884\n",
      "SubGD iter. 442/499: loss=4.226499043098765, w0=73.49999999999979, w1=14.866650660087823\n",
      "SubGD iter. 443/499: loss=4.208941408886136, w0=73.09999999999978, w1=14.151606595615768\n",
      "SubGD iter. 444/499: loss=4.235043174812441, w0=73.59999999999978, w1=14.905007961485978\n",
      "SubGD iter. 445/499: loss=4.2162675056801975, w0=73.09999999999978, w1=14.212277252700945\n",
      "SubGD iter. 446/499: loss=4.230485584974958, w0=73.49999999999979, w1=14.934256745331068\n",
      "SubGD iter. 447/499: loss=4.213775541883672, w0=73.09999999999978, w1=14.219212680859012\n",
      "SubGD iter. 448/499: loss=4.2299848612846835, w0=73.49999999999979, w1=14.941192173489135\n",
      "SubGD iter. 449/499: loss=4.214271455557572, w0=73.09999999999978, w1=14.22614810901708\n",
      "SubGD iter. 450/499: loss=4.229484137594411, w0=73.49999999999979, w1=14.948127601647203\n",
      "SubGD iter. 451/499: loss=4.214767369231472, w0=73.09999999999978, w1=14.233083537175148\n",
      "SubGD iter. 452/499: loss=4.228983413904137, w0=73.49999999999979, w1=14.95506302980527\n",
      "SubGD iter. 453/499: loss=4.215263282905372, w0=73.09999999999978, w1=14.240018965333215\n",
      "SubGD iter. 454/499: loss=4.2284826902138635, w0=73.49999999999979, w1=14.961998457963338\n",
      "SubGD iter. 455/499: loss=4.215759196579272, w0=73.09999999999978, w1=14.246954393491283\n",
      "SubGD iter. 456/499: loss=4.227986310632996, w0=73.39999999999978, w1=14.890168977360265\n",
      "SubGD iter. 457/499: loss=4.208027108274121, w0=73.19999999999978, w1=14.187390042100718\n",
      "SubGD iter. 458/499: loss=4.228817576582307, w0=73.49999999999977, w1=14.8306046259697\n",
      "SubGD iter. 459/499: loss=4.20656666420952, w0=73.19999999999978, w1=14.3269331794038\n",
      "SubGD iter. 460/499: loss=4.219841958483088, w0=73.49999999999977, w1=14.970147763272783\n",
      "SubGD iter. 461/499: loss=4.216345077579449, w0=73.19999999999978, w1=14.217798867968483\n",
      "SubGD iter. 462/499: loss=4.226861636554658, w0=73.49999999999977, w1=14.861013451837465\n",
      "SubGD iter. 463/499: loss=4.208538323656174, w0=73.09999999999977, w1=14.14596938736541\n",
      "SubGD iter. 464/499: loss=4.235467882851993, w0=73.59999999999977, w1=14.89937075323562\n",
      "SubGD iter. 465/499: loss=4.215876998953513, w0=73.09999999999977, w1=14.206640044450587\n",
      "SubGD iter. 466/499: loss=4.230896947260373, w0=73.59999999999977, w1=14.960041410320798\n",
      "SubGD iter. 467/499: loss=4.22007984168202, w0=73.09999999999977, w1=14.267310701535765\n",
      "SubGD iter. 468/499: loss=4.226676963212202, w0=73.39999999999976, w1=14.910525285404747\n",
      "SubGD iter. 469/499: loss=4.209457706723452, w0=73.19999999999976, w1=14.2077463501452\n",
      "SubGD iter. 470/499: loss=4.227508229161512, w0=73.49999999999976, w1=14.850960934014182\n",
      "SubGD iter. 471/499: loss=4.207819524335918, w0=73.09999999999975, w1=14.135916869542127\n",
      "SubGD iter. 472/499: loss=4.236225240917844, w0=73.59999999999975, w1=14.889318235412338\n",
      "SubGD iter. 473/499: loss=4.215180630173833, w0=73.09999999999975, w1=14.196587526627304\n",
      "SubGD iter. 474/499: loss=4.231654305326222, w0=73.59999999999975, w1=14.949988892497515\n",
      "SubGD iter. 475/499: loss=4.219383472902339, w0=73.09999999999975, w1=14.257258183712482\n",
      "SubGD iter. 476/499: loss=4.227323555819056, w0=73.39999999999975, w1=14.900472767581464\n",
      "SubGD iter. 477/499: loss=4.208751236946199, w0=73.19999999999975, w1=14.197693832321917\n",
      "SubGD iter. 478/499: loss=4.228154821768367, w0=73.49999999999974, w1=14.8409084161909\n",
      "SubGD iter. 479/499: loss=4.207100725015665, w0=73.09999999999974, w1=14.125864351718844\n",
      "SubGD iter. 480/499: loss=4.237041637013566, w0=73.69999999999973, w1=14.945456336134495\n",
      "SubGD iter. 481/499: loss=4.225002207206321, w0=72.99999999999973, w1=13.953367234516898\n",
      "SubGD iter. 482/499: loss=4.2573072318056475, w0=73.79999999999973, w1=15.056849919853638\n",
      "SubGD iter. 483/499: loss=4.243053443246109, w0=73.09999999999972, w1=14.064760818236042\n",
      "SubGD iter. 484/499: loss=4.242049633639765, w0=73.69999999999972, w1=14.884352802651692\n",
      "SubGD iter. 485/499: loss=4.221599997406672, w0=72.89999999999972, w1=14.655485057437755\n",
      "SubGD iter. 486/499: loss=4.2203496145712505, w0=73.59999999999972, w1=14.647402988765768\n",
      "SubGD iter. 487/499: loss=4.213347008098279, w0=72.99999999999973, w1=14.868947081640563\n",
      "SubGD iter. 488/499: loss=4.215587068401582, w0=73.49999999999973, w1=14.737080564013958\n",
      "SubGD iter. 489/499: loss=4.20615032106438, w0=73.19999999999973, w1=14.882658809876002\n",
      "SubGD iter. 490/499: loss=4.207835558075685, w0=73.39999999999974, w1=14.646291475003096\n",
      "SubGD iter. 491/499: loss=4.204472012696541, w0=73.09999999999974, w1=14.79186972086514\n",
      "SubGD iter. 492/499: loss=4.209689724765218, w0=73.49999999999974, w1=14.768900056257044\n",
      "SubGD iter. 493/499: loss=4.205687098477884, w0=73.19999999999975, w1=14.914478302119088\n",
      "SubGD iter. 494/499: loss=4.209092993492057, w0=73.39999999999975, w1=14.330174894045095\n",
      "SubGD iter. 495/499: loss=4.217836681434901, w0=73.19999999999975, w1=14.79237326154243\n",
      "SubGD iter. 496/499: loss=4.206041843559656, w0=73.49999999999974, w1=14.635031383804668\n",
      "SubGD iter. 497/499: loss=4.208585035547556, w0=72.99999999999974, w1=14.906145436724216\n",
      "SubGD iter. 498/499: loss=4.216077590156214, w0=73.49999999999974, w1=14.774278919097611\n",
      "SubGD iter. 499/499: loss=4.205608793936177, w0=73.19999999999975, w1=14.919857164959655\n",
      "SubGD: execution time=0.060 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.05\n",
    "#gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(\n",
    "    y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a3470a11814583ab326cec37debc3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses, subgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic SubGradient Descent algorithm (SubSGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "\n",
    "        # ***************************************************\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "            g = compute_subgradient_mae(minibatch_y, minibatch_tx, w)\n",
    "            \n",
    "        loss = compute_loss(y, tx, w)\n",
    "        w = w - gamma * g\n",
    "        \n",
    "        losses.append(loss)\n",
    "        ws.append(w)\n",
    "        # TODO: implement stochastic subgradient descent.\n",
    "        # ***************************************************\n",
    "        \n",
    "        print(\"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=73.63227245973114, w0=0.7, w1=-0.42952274852148203\n",
      "SubSGD iter. 1/499: loss=72.93227245973114, w0=1.4, w1=-0.8519630501844514\n",
      "SubSGD iter. 2/499: loss=72.23227245973113, w0=2.0999999999999996, w1=0.35375303057666574\n",
      "SubSGD iter. 3/499: loss=71.53227245973115, w0=2.8, w1=0.7054592476574499\n",
      "SubSGD iter. 4/499: loss=70.83227245973114, w0=3.5, w1=1.4751702119976202\n",
      "SubSGD iter. 5/499: loss=70.13227245973115, w0=4.2, w1=1.20026360179612\n",
      "SubSGD iter. 6/499: loss=69.43227245973115, w0=4.9, w1=1.8671557214048153\n",
      "SubSGD iter. 7/499: loss=68.73227245973113, w0=5.6000000000000005, w1=3.346764046747902\n",
      "SubSGD iter. 8/499: loss=68.03227245973113, w0=6.300000000000001, w1=3.601738724295865\n",
      "SubSGD iter. 9/499: loss=67.33227245973114, w0=7.000000000000001, w1=2.8883313662959695\n",
      "SubSGD iter. 10/499: loss=66.63227245973115, w0=7.700000000000001, w1=1.5112828805434575\n",
      "SubSGD iter. 11/499: loss=65.93227245973114, w0=8.4, w1=1.7539367381433222\n",
      "SubSGD iter. 12/499: loss=65.23227245973113, w0=9.1, w1=2.4550515135816986\n",
      "SubSGD iter. 13/499: loss=64.53227245973113, w0=9.799999999999999, w1=3.3956570054893693\n",
      "SubSGD iter. 14/499: loss=63.832272459731136, w0=10.499999999999998, w1=4.6013730862504865\n",
      "SubSGD iter. 15/499: loss=63.13227245973114, w0=11.199999999999998, w1=5.248738768454661\n",
      "SubSGD iter. 16/499: loss=62.43227245973114, w0=11.899999999999997, w1=4.726680666147843\n",
      "SubSGD iter. 17/499: loss=61.73227245973114, w0=12.599999999999996, w1=4.721864238154448\n",
      "SubSGD iter. 18/499: loss=61.03227245973114, w0=13.299999999999995, w1=4.314915620555705\n",
      "SubSGD iter. 19/499: loss=60.33227245973115, w0=13.999999999999995, w1=4.0103317294408285\n",
      "SubSGD iter. 20/499: loss=59.63227245973115, w0=14.699999999999994, w1=3.5848108031540935\n",
      "SubSGD iter. 21/499: loss=58.932272459731145, w0=15.399999999999993, w1=4.068014840075621\n",
      "SubSGD iter. 22/499: loss=58.23227245973115, w0=16.099999999999994, w1=3.944369919999608\n",
      "SubSGD iter. 23/499: loss=57.532272459731146, w0=16.799999999999994, w1=3.5223719556733544\n",
      "SubSGD iter. 24/499: loss=56.83227245973115, w0=17.499999999999993, w1=4.1880345291016585\n",
      "SubSGD iter. 25/499: loss=56.13227245973114, w0=18.199999999999992, w1=3.222718468845965\n",
      "SubSGD iter. 26/499: loss=55.432272459731145, w0=18.89999999999999, w1=3.917224141743975\n",
      "SubSGD iter. 27/499: loss=54.73227245973115, w0=19.59999999999999, w1=4.686935106084145\n",
      "SubSGD iter. 28/499: loss=54.032272459731146, w0=20.29999999999999, w1=4.6144230827155\n",
      "SubSGD iter. 29/499: loss=53.33227245973115, w0=20.99999999999999, w1=4.141386602504167\n",
      "SubSGD iter. 30/499: loss=52.63227245973115, w0=21.69999999999999, w1=5.620994927847254\n",
      "SubSGD iter. 31/499: loss=51.93227245973114, w0=22.399999999999988, w1=5.811365011702723\n",
      "SubSGD iter. 32/499: loss=51.23227245973114, w0=23.099999999999987, w1=6.658844426336976\n",
      "SubSGD iter. 33/499: loss=50.53227245973114, w0=23.799999999999986, w1=6.918334346544141\n",
      "SubSGD iter. 34/499: loss=49.832272459731136, w0=24.499999999999986, w1=6.488811598022659\n",
      "SubSGD iter. 35/499: loss=49.13227245973115, w0=25.199999999999985, w1=7.183317270920669\n",
      "SubSGD iter. 36/499: loss=48.43227245973114, w0=25.899999999999984, w1=6.253381190493119\n",
      "SubSGD iter. 37/499: loss=47.73227245973115, w0=26.599999999999984, w1=5.8345471380397695\n",
      "SubSGD iter. 38/499: loss=47.032272459731146, w0=27.299999999999983, w1=6.297881467857847\n",
      "SubSGD iter. 39/499: loss=46.33227245973115, w0=27.999999999999982, w1=6.386107744951401\n",
      "SubSGD iter. 40/499: loss=45.632272459731155, w0=28.69999999999998, w1=6.035907150675006\n",
      "SubSGD iter. 41/499: loss=44.932272459731145, w0=29.39999999999998, w1=5.115153691575077\n",
      "SubSGD iter. 42/499: loss=44.232272459731156, w0=30.09999999999998, w1=5.042641668206431\n",
      "SubSGD iter. 43/499: loss=43.53227245973116, w0=30.79999999999998, w1=5.678778381707621\n",
      "SubSGD iter. 44/499: loss=42.83227245973116, w0=31.49999999999998, w1=5.166035567427184\n",
      "SubSGD iter. 45/499: loss=42.132272459731155, w0=32.19999999999998, w1=4.452628209427289\n",
      "SubSGD iter. 46/499: loss=41.43227245973116, w0=32.899999999999984, w1=5.018285005773368\n",
      "SubSGD iter. 47/499: loss=40.73227245973115, w0=33.59999999999999, w1=5.366112420002298\n",
      "SubSGD iter. 48/499: loss=40.032272459731146, w0=34.29999999999999, w1=5.713939834231228\n",
      "SubSGD iter. 49/499: loss=39.33227245973114, w0=34.99999999999999, w1=5.478270422819342\n",
      "SubSGD iter. 50/499: loss=38.63227245973114, w0=35.699999999999996, w1=5.242601011407457\n",
      "SubSGD iter. 51/499: loss=37.93227245973114, w0=36.4, w1=5.50802569712576\n",
      "SubSGD iter. 52/499: loss=37.232272459731135, w0=37.1, w1=5.48109830526286\n",
      "SubSGD iter. 53/499: loss=36.53227245973113, w0=37.800000000000004, w1=4.566399395252101\n",
      "SubSGD iter. 54/499: loss=35.83227245973113, w0=38.50000000000001, w1=5.772115476013218\n",
      "SubSGD iter. 55/499: loss=35.132272459731126, w0=39.20000000000001, w1=5.972160955457642\n",
      "SubSGD iter. 56/499: loss=34.432272459731124, w0=39.90000000000001, w1=6.715169734607384\n",
      "SubSGD iter. 57/499: loss=33.73227245973112, w0=40.600000000000016, w1=8.05503432171802\n",
      "SubSGD iter. 58/499: loss=33.03227245973111, w0=41.30000000000002, w1=9.222575624969537\n",
      "SubSGD iter. 59/499: loss=32.33227245973111, w0=42.00000000000002, w1=8.498095212523856\n",
      "SubSGD iter. 60/499: loss=31.632272459731112, w0=42.700000000000024, w1=8.981299249445383\n",
      "SubSGD iter. 61/499: loss=30.932272459731102, w0=43.40000000000003, w1=9.144968726753765\n",
      "SubSGD iter. 62/499: loss=30.2322724597311, w0=44.10000000000003, w1=9.08286997782496\n",
      "SubSGD iter. 63/499: loss=29.532272459731097, w0=44.80000000000003, w1=8.632438496946241\n",
      "SubSGD iter. 64/499: loss=28.832272459731094, w0=45.500000000000036, w1=8.110380394639423\n",
      "SubSGD iter. 65/499: loss=28.132272459731094, w0=46.20000000000004, w1=9.783654935917461\n",
      "SubSGD iter. 66/499: loss=27.432272459731088, w0=46.90000000000004, w1=9.983700415361884\n",
      "SubSGD iter. 67/499: loss=26.732272459731085, w0=47.600000000000044, w1=9.195128208255493\n",
      "SubSGD iter. 68/499: loss=26.032272459731086, w0=48.30000000000005, w1=7.912855285982846\n",
      "SubSGD iter. 69/499: loss=25.332272459731083, w0=49.00000000000005, w1=8.853544648272715\n",
      "SubSGD iter. 70/499: loss=24.632272459731077, w0=49.70000000000005, w1=9.096955302423433\n",
      "SubSGD iter. 71/499: loss=23.932272459731074, w0=50.400000000000055, w1=8.835821486597728\n",
      "SubSGD iter. 72/499: loss=23.232272459731075, w0=51.10000000000006, w1=9.401478282943808\n",
      "SubSGD iter. 73/499: loss=22.532272459731065, w0=51.80000000000006, w1=8.615402866144906\n",
      "SubSGD iter. 74/499: loss=21.832272459731065, w0=52.500000000000064, w1=8.87489278635207\n",
      "SubSGD iter. 75/499: loss=21.132272459731066, w0=53.20000000000007, w1=9.540555359780374\n",
      "SubSGD iter. 76/499: loss=20.43227245973106, w0=53.90000000000007, w1=10.746271440541491\n",
      "SubSGD iter. 77/499: loss=19.732272459731053, w0=54.60000000000007, w1=11.424976095427466\n",
      "SubSGD iter. 78/499: loss=19.032272459731047, w0=55.300000000000075, w1=10.71156873742757\n",
      "SubSGD iter. 79/499: loss=18.332272459731044, w0=56.00000000000008, w1=11.194772774349097\n",
      "SubSGD iter. 80/499: loss=17.632272459731045, w0=56.70000000000008, w1=10.291311414982752\n",
      "SubSGD iter. 81/499: loss=16.932272459731042, w0=57.400000000000084, w1=10.475122069310236\n",
      "SubSGD iter. 82/499: loss=16.23227245973104, w0=58.10000000000009, w1=10.67516754875466\n",
      "SubSGD iter. 83/499: loss=15.53227245973104, w0=58.80000000000009, w1=10.52994598804006\n",
      "SubSGD iter. 84/499: loss=14.832272459731035, w0=59.50000000000009, w1=11.09560278438614\n",
      "SubSGD iter. 85/499: loss=14.13227245973103, w0=60.200000000000095, w1=10.467867102803268\n",
      "SubSGD iter. 86/499: loss=13.437826426589568, w0=60.9000000000001, w1=8.932804044846137\n",
      "SubSGD iter. 87/499: loss=12.780374719230332, w0=61.6000000000001, w1=9.948506697323355\n",
      "SubSGD iter. 88/499: loss=12.086794432813644, w0=62.300000000000104, w1=10.571102692477305\n",
      "SubSGD iter. 89/499: loss=11.401249210200653, w0=63.00000000000011, w1=11.265608365375314\n",
      "SubSGD iter. 90/499: loss=10.721639576278246, w0=63.70000000000011, w1=11.912146395377022\n",
      "SubSGD iter. 91/499: loss=10.048284533824038, w0=64.4000000000001, w1=12.801939064443843\n",
      "SubSGD iter. 92/499: loss=9.36964462765916, w0=65.10000000000011, w1=12.279880962137025\n",
      "SubSGD iter. 93/499: loss=8.74879447396409, w0=65.80000000000011, w1=11.863160915059268\n",
      "SubSGD iter. 94/499: loss=8.159409559571184, w0=66.50000000000011, w1=13.153606496303613\n",
      "SubSGD iter. 95/499: loss=7.5028222194079515, w0=67.20000000000012, w1=13.08360198337851\n",
      "SubSGD iter. 96/499: loss=6.94393200513418, w0=67.90000000000012, w1=13.772425138642225\n",
      "SubSGD iter. 97/499: loss=6.390705144710556, w0=67.20000000000012, w1=12.744280749649072\n",
      "SubSGD iter. 98/499: loss=6.966162340300253, w0=67.90000000000012, w1=12.322282785322818\n",
      "SubSGD iter. 99/499: loss=6.532653755013211, w0=67.20000000000012, w1=13.289140853139298\n",
      "SubSGD iter. 100/499: loss=6.934464289043615, w0=67.90000000000012, w1=13.868376858225822\n",
      "SubSGD iter. 101/499: loss=6.385470804092292, w0=68.60000000000012, w1=13.841449466362922\n",
      "SubSGD iter. 102/499: loss=5.92267731172977, w0=69.30000000000013, w1=13.78679869672654\n",
      "SubSGD iter. 103/499: loss=5.505103468571886, w0=68.60000000000012, w1=13.600503888735357\n",
      "SubSGD iter. 104/499: loss=5.9418972135015, w0=69.30000000000013, w1=13.595687460741962\n",
      "SubSGD iter. 105/499: loss=5.527240383740526, w0=70.00000000000013, w1=13.68781636895832\n",
      "SubSGD iter. 106/499: loss=5.128472863894172, w0=70.70000000000013, w1=13.426682553132615\n",
      "SubSGD iter. 107/499: loss=4.870400006303313, w0=71.40000000000013, w1=14.565444250686863\n",
      "SubSGD iter. 108/499: loss=4.491790264871437, w0=70.70000000000013, w1=14.73241087256606\n",
      "SubSGD iter. 109/499: loss=4.749167947002364, w0=71.40000000000013, w1=15.673100234855928\n",
      "SubSGD iter. 110/499: loss=4.574481222083519, w0=70.70000000000013, w1=15.335516586009453\n",
      "SubSGD iter. 111/499: loss=4.793518987362781, w0=71.40000000000013, w1=16.115488346020076\n",
      "SubSGD iter. 112/499: loss=4.6594943728932945, w0=72.10000000000014, w1=15.03545456702002\n",
      "SubSGD iter. 113/499: loss=4.320265579268799, w0=72.80000000000014, w1=14.577772616991867\n",
      "SubSGD iter. 114/499: loss=4.229568641700801, w0=73.50000000000014, w1=14.84319730271017\n",
      "SubSGD iter. 115/499: loss=4.2072643904876665, w0=74.20000000000014, w1=14.850480973034925\n",
      "SubSGD iter. 116/499: loss=4.273487587134075, w0=73.50000000000014, w1=14.664186165043741\n",
      "SubSGD iter. 117/499: loss=4.207794608457205, w0=72.80000000000014, w1=13.636041776050588\n",
      "SubSGD iter. 118/499: loss=4.3220248283134, w0=72.10000000000014, w1=13.7596866961266\n",
      "SubSGD iter. 119/499: loss=4.392369251279356, w0=72.80000000000014, w1=13.603493206317442\n",
      "SubSGD iter. 120/499: loss=4.328471878245516, w0=73.50000000000014, w1=13.66758225803507\n",
      "SubSGD iter. 121/499: loss=4.289527182238348, w0=74.20000000000014, w1=13.817343272083654\n",
      "SubSGD iter. 122/499: loss=4.318217466137472, w0=73.50000000000014, w1=13.181206558582465\n",
      "SubSGD iter. 123/499: loss=4.373052237451588, w0=74.20000000000014, w1=13.82857224078664\n",
      "SubSGD iter. 124/499: loss=4.317030624455485, w0=74.90000000000015, w1=12.644320608595056\n",
      "SubSGD iter. 125/499: loss=4.618063517135633, w0=75.60000000000015, w1=11.663685005550315\n",
      "SubSGD iter. 126/499: loss=5.060681655461021, w0=74.90000000000015, w1=12.13612344487739\n",
      "SubSGD iter. 127/499: loss=4.759206735476205, w0=74.20000000000014, w1=12.862853994524603\n",
      "SubSGD iter. 128/499: loss=4.457463755548052, w0=73.50000000000014, w1=11.572408413280257\n",
      "SubSGD iter. 129/499: loss=4.912883123096437, w0=72.80000000000014, w1=10.877902740382249\n",
      "SubSGD iter. 130/499: loss=5.333649447440608, w0=73.50000000000014, w1=10.401006603401203\n",
      "SubSGD iter. 131/499: loss=5.535486482550645, w0=72.80000000000014, w1=10.823446905064174\n",
      "SubSGD iter. 132/499: loss=5.363385703994552, w0=73.50000000000014, w1=11.960530083990893\n",
      "SubSGD iter. 133/499: loss=4.739252745241799, w0=72.80000000000014, w1=12.683087243584257\n",
      "SubSGD iter. 134/499: loss=4.54808762623092, w0=73.50000000000014, w1=13.349979363192952\n",
      "SubSGD iter. 135/499: loss=4.340356576637005, w0=74.20000000000014, w1=12.646585079406846\n",
      "SubSGD iter. 136/499: loss=4.511811752215876, w0=74.90000000000015, w1=12.64176865141345\n",
      "SubSGD iter. 137/499: loss=4.618718913899437, w0=75.60000000000015, w1=12.791137807837053\n",
      "SubSGD iter. 138/499: loss=4.7633630505492315, w0=74.90000000000015, w1=12.12547523440875\n",
      "SubSGD iter. 139/499: loss=4.762320888806134, w0=74.20000000000014, w1=10.835029653164405\n",
      "SubSGD iter. 140/499: loss=5.28381278030255, w0=74.90000000000015, w1=11.597139029749576\n",
      "SubSGD iter. 141/499: loss=4.959742947957032, w0=74.20000000000014, w1=12.677172808749631\n",
      "SubSGD iter. 142/499: loss=4.503551251814706, w0=73.50000000000014, w1=11.538411111195384\n",
      "SubSGD iter. 143/499: loss=4.928739198906105, w0=72.80000000000014, w1=12.05115392547582\n",
      "SubSGD iter. 144/499: loss=4.773147731703928, w0=72.10000000000014, w1=11.807743271325101\n",
      "SubSGD iter. 145/499: loss=5.012394648000235, w0=72.80000000000014, w1=12.453259256153267\n",
      "SubSGD iter. 146/499: loss=4.624404338969497, w0=73.50000000000014, w1=12.383254743228164\n",
      "SubSGD iter. 147/499: loss=4.577956709964201, w0=74.20000000000014, w1=11.494659464298696\n",
      "SubSGD iter. 148/499: loss=4.95539022093303, w0=74.90000000000015, w1=11.760084150016999\n",
      "SubSGD iter. 149/499: loss=4.892827917523566, w0=74.20000000000014, w1=12.271960149592745\n",
      "SubSGD iter. 150/499: loss=4.6240573107755525, w0=73.50000000000014, w1=12.912602951204656\n",
      "SubSGD iter. 151/499: loss=4.42974936431076, w0=74.20000000000014, w1=13.443037880281345\n",
      "SubSGD iter. 152/499: loss=4.359297065612824, w0=73.50000000000014, w1=13.965095982588164\n",
      "SubSGD iter. 153/499: loss=4.255710587114149, w0=72.80000000000014, w1=13.94132392359636\n",
      "SubSGD iter. 154/499: loss=4.27560627002386, w0=72.10000000000014, w1=14.90221805688095\n",
      "SubSGD iter. 155/499: loss=4.318165794744655, w0=71.40000000000013, w1=15.40830208390277\n",
      "SubSGD iter. 156/499: loss=4.53574103344276, w0=72.10000000000014, w1=15.353651314266388\n",
      "SubSGD iter. 157/499: loss=4.337414826209279, w0=72.80000000000014, w1=15.537461968593872\n",
      "SubSGD iter. 158/499: loss=4.281888871913605, w0=73.50000000000014, w1=15.07978001856572\n",
      "SubSGD iter. 159/499: loss=4.22554705876678, w0=72.80000000000014, w1=15.58586404558754\n",
      "SubSGD iter. 160/499: loss=4.289030912297143, w0=72.10000000000014, w1=14.44878086666082\n",
      "SubSGD iter. 161/499: loss=4.324799664096538, w0=72.80000000000014, w1=14.540909774877179\n",
      "SubSGD iter. 162/499: loss=4.230478535223941, w0=72.10000000000014, w1=15.030685086884853\n",
      "SubSGD iter. 163/499: loss=4.320190413080751, w0=71.40000000000013, w1=14.394548373383664\n",
      "SubSGD iter. 164/499: loss=4.500206076751049, w0=72.10000000000014, w1=15.174520133394287\n",
      "SubSGD iter. 165/499: loss=4.32558303658726, w0=71.40000000000013, w1=14.03575843584004\n",
      "SubSGD iter. 166/499: loss=4.527902377810894, w0=72.10000000000014, w1=13.585326954961321\n",
      "SubSGD iter. 167/499: loss=4.4210699124398465, w0=72.80000000000014, w1=14.395321202067315\n",
      "SubSGD iter. 168/499: loss=4.236719510933295, w0=72.10000000000014, w1=15.325257282494865\n",
      "SubSGD iter. 169/499: loss=4.335322558024826, w0=72.80000000000014, w1=16.53853379218131\n",
      "SubSGD iter. 170/499: loss=4.502103040921026, w0=72.10000000000014, w1=17.3271059992877\n",
      "SubSGD iter. 171/499: loss=4.851293554593595, w0=72.80000000000014, w1=17.322289571294306\n",
      "SubSGD iter. 172/499: loss=4.75216298313213, w0=72.10000000000014, w1=16.423791450114997\n",
      "SubSGD iter. 173/499: loss=4.569079510010272, w0=71.40000000000013, w1=16.16430152990783\n",
      "SubSGD iter. 174/499: loss=4.670930322044821, w0=72.10000000000014, w1=16.008108040098673\n",
      "SubSGD iter. 175/499: loss=4.4597877559125205, w0=71.40000000000013, w1=16.654082289226714\n",
      "SubSGD iter. 176/499: loss=4.8016364604571065, w0=70.70000000000013, w1=16.138679645012967\n",
      "SubSGD iter. 177/499: loss=4.909544457670859, w0=70.00000000000013, w1=14.465405103734929\n",
      "SubSGD iter. 178/499: loss=5.066201613329585, w0=69.30000000000013, w1=14.279110295743745\n",
      "SubSGD iter. 179/499: loss=5.458015978358892, w0=70.00000000000013, w1=13.364411385732986\n",
      "SubSGD iter. 180/499: loss=5.193876447941568, w0=70.70000000000013, w1=13.71611760281377\n",
      "SubSGD iter. 181/499: loss=4.81511273915527, w0=71.40000000000013, w1=14.267471964141748\n",
      "SubSGD iter. 182/499: loss=4.507785503503194, w0=72.10000000000014, w1=14.392316235058095\n",
      "SubSGD iter. 183/499: loss=4.328094546836549, w0=71.40000000000013, w1=15.359174302874575\n",
      "SubSGD iter. 184/499: loss=4.529006695977335, w0=70.70000000000013, w1=14.417950409000216\n",
      "SubSGD iter. 185/499: loss=4.735826362090599, w0=70.00000000000013, w1=13.84404838195824\n",
      "SubSGD iter. 186/499: loss=5.106361210339056, w0=69.30000000000013, w1=13.680378904649858\n",
      "SubSGD iter. 187/499: loss=5.517430351904145, w0=70.00000000000013, w1=13.79302063811761\n",
      "SubSGD iter. 188/499: loss=5.11240117218856, w0=70.70000000000013, w1=14.881366129800538\n",
      "SubSGD iter. 189/499: loss=4.75958357290655, w0=70.00000000000013, w1=15.522008931412449\n",
      "SubSGD iter. 190/499: loss=5.1232235904255035, w0=70.70000000000013, w1=15.741962044093066\n",
      "SubSGD iter. 191/499: loss=4.841903903135748, w0=70.00000000000013, w1=16.382604845704975\n",
      "SubSGD iter. 192/499: loss=5.266010205418209, w0=70.70000000000013, w1=15.662165594082701\n",
      "SubSGD iter. 193/499: loss=4.830316207138414, w0=70.00000000000013, w1=14.69273315763526\n",
      "SubSGD iter. 194/499: loss=5.066218771714488, w0=70.70000000000013, w1=15.45484253422043\n",
      "SubSGD iter. 195/499: loss=4.804545746914695, w0=71.40000000000013, w1=15.720267219938734\n",
      "SubSGD iter. 196/499: loss=4.5825678757267285, w0=72.10000000000014, w1=16.267325426200447\n",
      "SubSGD iter. 197/499: loss=4.525238771658952, w0=71.40000000000013, w1=14.964377552294511\n",
      "SubSGD iter. 198/499: loss=4.491263068664818, w0=72.10000000000014, w1=13.98374194924977\n",
      "SubSGD iter. 199/499: loss=4.361934659601824, w0=71.40000000000013, w1=14.01066934111267\n",
      "SubSGD iter. 200/499: loss=4.530357500712882, w0=70.70000000000013, w1=14.858568953861422\n",
      "SubSGD iter. 201/499: loss=4.757891441995739, w0=71.40000000000013, w1=14.702375464052263\n",
      "SubSGD iter. 202/499: loss=4.487610852361869, w0=72.10000000000014, w1=14.647724694415881\n",
      "SubSGD iter. 203/499: loss=4.318038308122427, w0=72.80000000000014, w1=14.346518637125099\n",
      "SubSGD iter. 204/499: loss=4.240215577999201, w0=72.10000000000014, w1=14.999313354048235\n",
      "SubSGD iter. 205/499: loss=4.319695999957522, w0=72.80000000000014, w1=15.091442262264593\n",
      "SubSGD iter. 206/499: loss=4.236284944247299, w0=72.10000000000014, w1=15.118369654127493\n",
      "SubSGD iter. 207/499: loss=4.322979559654755, w0=72.80000000000014, w1=15.33832276680811\n",
      "SubSGD iter. 208/499: loss=4.257680570728431, w0=72.10000000000014, w1=15.000739117961635\n",
      "SubSGD iter. 209/499: loss=4.319718469752005, w0=71.40000000000013, w1=15.641381919573545\n",
      "SubSGD iter. 210/499: loss=4.569378147866508, w0=70.70000000000013, w1=14.861410159562922\n",
      "SubSGD iter. 211/499: loss=4.7581023318008775, w0=71.40000000000013, w1=15.751202828629744\n",
      "SubSGD iter. 212/499: loss=4.587871703229946, w0=70.70000000000013, w1=14.573962169288116\n",
      "SubSGD iter. 213/499: loss=4.740255918345801, w0=71.40000000000013, w1=15.848960232686446\n",
      "SubSGD iter. 214/499: loss=4.60519978888404, w0=72.10000000000014, w1=15.831877056938128\n",
      "SubSGD iter. 215/499: loss=4.417961073859231, w0=72.80000000000014, w1=15.675683567128969\n",
      "SubSGD iter. 216/499: loss=4.30323894995875, w0=73.50000000000014, w1=14.972289283342862\n",
      "SubSGD iter. 217/499: loss=4.216506194605359, w0=74.20000000000014, w1=15.537946079688941\n",
      "SubSGD iter. 218/499: loss=4.340607671574932, w0=73.50000000000014, w1=15.600044828617747\n",
      "SubSGD iter. 219/499: loss=4.285527910339901, w0=74.20000000000014, w1=15.193096211019004\n",
      "SubSGD iter. 220/499: loss=4.296733231550437, w0=74.90000000000015, w1=15.176013035270685\n",
      "SubSGD iter. 221/499: loss=4.418932682626938, w0=74.20000000000014, w1=14.645578106193996\n",
      "SubSGD iter. 222/499: loss=4.265487144213706, w0=74.90000000000015, w1=15.422987703068797\n",
      "SubSGD iter. 223/499: loss=4.4421178310281375, w0=75.60000000000015, w1=15.876638775730925\n",
      "SubSGD iter. 224/499: loss=4.685775240325958, w0=74.90000000000015, w1=15.346203846654236\n",
      "SubSGD iter. 225/499: loss=4.433666625912452, w0=75.60000000000015, w1=14.365568243609495\n",
      "SubSGD iter. 226/499: loss=4.563864526191736, w0=76.30000000000015, w1=15.306257605899363\n",
      "SubSGD iter. 227/499: loss=4.870613702111583, w0=75.60000000000015, w1=15.19361587243161\n",
      "SubSGD iter. 228/499: loss=4.599681720783787, w0=76.30000000000015, w1=14.524317972053712\n",
      "SubSGD iter. 229/499: loss=4.817839039379529, w0=75.60000000000015, w1=13.263148594559073\n",
      "SubSGD iter. 230/499: loss=4.671413976099929, w0=74.90000000000015, w1=12.126065415632354\n",
      "SubSGD iter. 231/499: loss=4.762148285641673, w0=74.20000000000014, w1=13.503113901384866\n",
      "SubSGD iter. 232/499: loss=4.351958594876743, w0=73.50000000000014, w1=13.156124181071593\n",
      "SubSGD iter. 233/499: loss=4.377911343214579, w0=72.80000000000014, w1=14.0977501385233\n",
      "SubSGD iter. 234/499: loss=4.258475041280455, w0=72.10000000000014, w1=14.54228631301173\n",
      "SubSGD iter. 235/499: loss=4.320228896892986, w0=71.40000000000013, w1=14.892486907288124\n",
      "SubSGD iter. 236/499: loss=4.48707488435591, w0=72.10000000000014, w1=13.911851304243383\n",
      "SubSGD iter. 237/499: loss=4.370990999495114, w0=71.40000000000013, w1=14.877167364499076\n",
      "SubSGD iter. 238/499: loss=4.486758477864376, w0=72.10000000000014, w1=13.896531761454336\n",
      "SubSGD iter. 239/499: loss=4.372920860974241, w0=72.80000000000014, w1=14.47576776654086\n",
      "SubSGD iter. 240/499: loss=4.232647749022221, w0=72.10000000000014, w1=14.905290515062342\n",
      "SubSGD iter. 241/499: loss=4.318214216158077, w0=71.40000000000013, w1=13.728049855720716\n",
      "SubSGD iter. 242/499: loss=4.562304034686465, w0=72.10000000000014, w1=14.57552927035497\n",
      "SubSGD iter. 243/499: loss=4.319023251726333, w0=72.80000000000014, w1=15.385523517460964\n",
      "SubSGD iter. 244/499: loss=4.262599997184657, w0=73.50000000000014, w1=15.82541967389327\n",
      "SubSGD iter. 245/499: loss=4.317908122864787, w0=74.20000000000014, w1=15.063141702763703\n",
      "SubSGD iter. 246/499: loss=4.287916121409685, w0=73.50000000000014, w1=14.093709266316262\n",
      "SubSGD iter. 247/499: loss=4.241333573587295, w0=74.20000000000014, w1=14.941188680950516\n",
      "SubSGD iter. 248/499: loss=4.279641893452836, w0=73.50000000000014, w1=13.804105502023797\n",
      "SubSGD iter. 249/499: loss=4.273706879951688, w0=74.20000000000014, w1=15.079103565422127\n",
      "SubSGD iter. 250/499: loss=4.288999096544602, w0=73.50000000000014, w1=14.384597892524118\n",
      "SubSGD iter. 251/499: loss=4.218065849551636, w0=74.20000000000014, w1=14.209735052485772\n",
      "SubSGD iter. 252/499: loss=4.284997061983391, w0=73.50000000000014, w1=14.78665633726997\n",
      "SubSGD iter. 253/499: loss=4.205662365745962, w0=72.80000000000014, w1=15.600622069206624\n",
      "SubSGD iter. 254/499: loss=4.291209008102908, w0=73.50000000000014, w1=16.249901175836925\n",
      "SubSGD iter. 255/499: loss=4.399375105884752, w0=72.80000000000014, w1=15.309295683929253\n",
      "SubSGD iter. 256/499: loss=4.254939994108294, w0=72.10000000000014, w1=15.812556389834716\n",
      "SubSGD iter. 257/499: loss=4.4137919182137555, w0=71.40000000000013, w1=15.936201309910729\n",
      "SubSGD iter. 258/499: loss=4.622496769290932, w0=70.70000000000013, w1=16.4259766219184\n",
      "SubSGD iter. 259/499: loss=4.981130655861341, w0=71.40000000000013, w1=17.114799777182117\n",
      "SubSGD iter. 260/499: loss=4.950580069979991, w0=72.10000000000014, w1=16.707851159583374\n",
      "SubSGD iter. 261/499: loss=4.6511418164611005, w0=72.80000000000014, w1=15.787097700483445\n",
      "SubSGD iter. 262/499: loss=4.322626054283435, w0=72.10000000000014, w1=16.30859390441705\n",
      "SubSGD iter. 263/499: loss=4.536693677867284, w0=72.80000000000014, w1=15.672547221929271\n",
      "SubSGD iter. 264/499: loss=4.3027362947835055, w0=73.50000000000014, w1=15.600035198560626\n",
      "SubSGD iter. 265/499: loss=4.285526649278431, w0=74.20000000000014, w1=15.443841708751467\n",
      "SubSGD iter. 266/499: loss=4.327054002770584, w0=73.50000000000014, w1=14.474409272304026\n",
      "SubSGD iter. 267/499: loss=4.2138613170258745, w0=74.20000000000014, w1=15.021467478565741\n",
      "SubSGD iter. 268/499: loss=4.28508862254322, w0=73.50000000000014, w1=14.25175651422557\n",
      "SubSGD iter. 269/499: loss=4.225545098037884, w0=74.20000000000014, w1=13.778720034014238\n",
      "SubSGD iter. 270/499: loss=4.322299734959005, w0=73.50000000000014, w1=14.053626644215738\n",
      "SubSGD iter. 271/499: loss=4.245814202519446, w0=74.20000000000014, w1=13.138927734204978\n",
      "SubSGD iter. 272/499: loss=4.405048641846296, w0=74.90000000000015, w1=14.079617096494847\n",
      "SubSGD iter. 273/499: loss=4.4045860387441085, w0=75.60000000000015, w1=14.76844025175856\n",
      "SubSGD iter. 274/499: loss=4.573378725542381, w0=74.90000000000015, w1=13.85327763886397\n",
      "SubSGD iter. 275/499: loss=4.423215714844982, w0=75.60000000000015, w1=13.61583230635273\n",
      "SubSGD iter. 276/499: loss=4.614568404899621, w0=74.90000000000015, w1=14.088270745679804\n",
      "SubSGD iter. 277/499: loss=4.4039703655436355, w0=74.20000000000014, w1=14.578046057687478\n",
      "SubSGD iter. 278/499: loss=4.265455715742226, w0=74.90000000000015, w1=13.597410454642738\n",
      "SubSGD iter. 279/499: loss=4.446962445450015, w0=74.20000000000014, w1=13.872317064844237\n",
      "SubSGD iter. 280/499: loss=4.312407031502641, w0=73.50000000000014, w1=14.289037111921994\n",
      "SubSGD iter. 281/499: loss=4.222713531880287, w0=72.80000000000014, w1=15.077609319028385\n",
      "SubSGD iter. 282/499: loss=4.235314770496681, w0=73.50000000000014, w1=15.261225424270327\n",
      "SubSGD iter. 283/499: loss=4.2456218915503685, w0=74.20000000000014, w1=14.372630145340858\n",
      "SubSGD iter. 284/499: loss=4.274240014180355, w0=73.50000000000014, w1=14.966551070216836\n",
      "SubSGD iter. 285/499: loss=4.2160847284162575, w0=72.80000000000014, w1=15.469811776122299\n",
      "SubSGD iter. 286/499: loss=4.272650005817456, w0=73.50000000000014, w1=15.012129826094146\n",
      "SubSGD iter. 287/499: loss=4.219503593438086, w0=74.20000000000014, w1=14.308735542308039\n",
      "SubSGD iter. 288/499: loss=4.278459400333328, w0=73.50000000000014, w1=15.388769321308095\n",
      "SubSGD iter. 289/499: loss=4.260424689058482, w0=72.80000000000014, w1=14.873366677094348\n",
      "SubSGD iter. 290/499: loss=4.229729849428236, w0=73.50000000000014, w1=15.116020534694213\n",
      "SubSGD iter. 291/499: loss=4.229358500260598, w0=72.80000000000014, w1=15.963920147442964\n",
      "SubSGD iter. 292/499: loss=4.357702456180805, w0=73.50000000000014, w1=15.168590457046276\n",
      "SubSGD iter. 293/499: loss=4.234887317308042, w0=72.80000000000014, w1=14.27009233586697\n",
      "SubSGD iter. 294/499: loss=4.245690525373666, w0=72.10000000000014, w1=13.477138856722453\n",
      "SubSGD iter. 295/499: loss=4.439491301771268, w0=72.80000000000014, w1=15.150413398000492\n",
      "SubSGD iter. 296/499: loss=4.240420886031933, w0=73.50000000000014, w1=14.743464780401748\n",
      "SubSGD iter. 297/499: loss=4.206057380762099, w0=72.80000000000014, w1=14.077802206973445\n",
      "SubSGD iter. 298/499: loss=4.260134874747085, w0=73.50000000000014, w1=14.424791927286716\n",
      "SubSGD iter. 299/499: loss=4.216118401447181, w0=72.80000000000014, w1=15.060838609774496\n",
      "SubSGD iter. 300/499: loss=4.234392374237095, w0=73.50000000000014, w1=15.084610668766299\n",
      "SubSGD iter. 301/499: loss=4.226055101769018, w0=72.80000000000014, w1=14.274616421660305\n",
      "SubSGD iter. 302/499: loss=4.2453664336509425, w0=73.50000000000014, w1=14.85385242674683\n",
      "SubSGD iter. 303/499: loss=4.208026278807528, w0=72.80000000000014, w1=15.114986242572535\n",
      "SubSGD iter. 304/499: loss=4.2379362017772255, w0=72.10000000000014, w1=13.947444939321018\n",
      "SubSGD iter. 305/499: loss=4.366507132960432, w0=71.40000000000013, w1=14.294733575876212\n",
      "SubSGD iter. 306/499: loss=4.505961471713755, w0=70.70000000000013, w1=14.724256324397693\n",
      "SubSGD iter. 307/499: loss=4.748645779251506, w0=71.40000000000013, w1=14.30225836007144\n",
      "SubSGD iter. 308/499: loss=4.5054580001760325, w0=70.70000000000013, w1=14.503109483463431\n",
      "SubSGD iter. 309/499: loss=4.737652308651683, w0=71.40000000000013, w1=14.486026307715113\n",
      "SubSGD iter. 310/499: loss=4.495655082704983, w0=72.10000000000014, w1=14.41602179479001\n",
      "SubSGD iter. 311/499: loss=4.326711255431186, w0=71.40000000000013, w1=15.43544271972916\n",
      "SubSGD iter. 312/499: loss=4.539468850635398, w0=70.70000000000013, w1=14.132494845823224\n",
      "SubSGD iter. 313/499: loss=4.748145691768764, w0=71.40000000000013, w1=14.156266904815027\n",
      "SubSGD iter. 314/499: loss=4.516255200942492, w0=72.10000000000014, w1=13.486969004437128\n",
      "SubSGD iter. 315/499: loss=4.4378175041799865, w0=72.80000000000014, w1=13.636338160860731\n",
      "SubSGD iter. 316/499: loss=4.32196612196514, w0=72.10000000000014, w1=14.126113472868406\n",
      "SubSGD iter. 317/499: loss=4.347080808002641, w0=72.80000000000014, w1=13.675681991989688\n",
      "SubSGD iter. 318/499: loss=4.314903484596116, w0=72.10000000000014, w1=13.010019418561384\n",
      "SubSGD iter. 319/499: loss=4.5605322086261895, w0=72.80000000000014, w1=13.698842573825099\n",
      "SubSGD iter. 320/499: loss=4.310836384678883, w0=73.50000000000014, w1=14.355994109746698\n",
      "SubSGD iter. 321/499: loss=4.219451736359945, w0=72.80000000000014, w1=13.545999862640704\n",
      "SubSGD iter. 322/499: loss=4.3398598581317955, w0=72.10000000000014, w1=14.182046545128484\n",
      "SubSGD iter. 323/499: loss=4.342255845470126, w0=72.80000000000014, w1=13.512748644750586\n",
      "SubSGD iter. 324/499: loss=4.346446084924519, w0=72.10000000000014, w1=14.546360261053767\n",
      "SubSGD iter. 325/499: loss=4.3200596841845025, w0=72.80000000000014, w1=14.638489169270125\n",
      "SubSGD iter. 326/499: loss=4.22849860075577, w0=73.50000000000014, w1=14.822105274512067\n",
      "SubSGD iter. 327/499: loss=4.206180131118949, w0=72.80000000000014, w1=15.344163376818885\n",
      "SubSGD iter. 328/499: loss=4.258252214504497, w0=73.50000000000014, w1=15.729762824608619\n",
      "SubSGD iter. 329/499: loss=4.3030253764420765, w0=72.80000000000014, w1=16.251259028542226\n",
      "SubSGD iter. 330/499: loss=4.425015414453985, w0=72.10000000000014, w1=16.278186420405127\n",
      "SubSGD iter. 331/499: loss=4.528253461691642, w0=72.80000000000014, w1=15.374725061038783\n",
      "SubSGD iter. 332/499: loss=4.261379194802191, w0=73.50000000000014, w1=15.49956933195513\n",
      "SubSGD iter. 333/499: loss=4.2732841841781815, w0=72.80000000000014, w1=16.460463465239716\n",
      "SubSGD iter. 334/499: loss=4.479911887787074, w0=73.50000000000014, w1=17.30794287987397\n",
      "SubSGD iter. 335/499: loss=4.7070724413013245, w0=74.20000000000014, w1=16.39324396986321\n",
      "SubSGD iter. 336/499: loss=4.4822677356947525, w0=74.90000000000015, w1=16.24802240914861\n",
      "SubSGD iter. 337/499: loss=4.585309147050781, w0=75.60000000000015, w1=15.26738680610387\n",
      "SubSGD iter. 338/499: loss=4.607142455554497, w0=76.30000000000015, w1=14.809704856075717\n",
      "SubSGD iter. 339/499: loss=4.830790090151714, w0=75.60000000000015, w1=14.142812736467022\n",
      "SubSGD iter. 340/499: loss=4.568550294982808, w0=74.90000000000015, w1=12.666106814635349\n",
      "SubSGD iter. 341/499: loss=4.612728700234023, w0=74.20000000000014, w1=13.627000947919937\n",
      "SubSGD iter. 342/499: loss=4.338335627487769, w0=74.90000000000015, w1=12.831671257523249\n",
      "SubSGD iter. 343/499: loss=4.575918674149868, w0=74.20000000000014, w1=13.067340668935135\n",
      "SubSGD iter. 344/499: loss=4.417883019927218, w0=73.50000000000014, w1=13.665123308992607\n",
      "SubSGD iter. 345/499: loss=4.289821423318593, w0=72.80000000000014, w1=14.39185385863982\n",
      "SubSGD iter. 346/499: loss=4.2369679008525, w0=72.10000000000014, w1=15.01009690721067\n",
      "SubSGD iter. 347/499: loss=4.319865946903913, w0=72.80000000000014, w1=13.83757657796638\n",
      "SubSGD iter. 348/499: loss=4.288579369198128, w0=73.50000000000014, w1=13.929705486182739\n",
      "SubSGD iter. 349/499: loss=4.259666707592486, w0=72.80000000000014, w1=14.652262645776103\n",
      "SubSGD iter. 350/499: loss=4.22845899994793, w0=72.10000000000014, w1=14.078360618734127\n",
      "SubSGD iter. 351/499: loss=4.351374256554407, w0=72.80000000000014, w1=15.019049981023995\n",
      "SubSGD iter. 352/499: loss=4.2330219630994295, w0=73.50000000000014, w1=13.736777058751349\n",
      "SubSGD iter. 353/499: loss=4.281247241272254, w0=74.20000000000014, w1=12.201714000794215\n",
      "SubSGD iter. 354/499: loss=4.650749249539327, w0=74.90000000000015, w1=12.549541415023146\n",
      "SubSGD iter. 355/499: loss=4.64311151150668, w0=75.60000000000015, w1=12.73335206935063\n",
      "SubSGD iter. 356/499: loss=4.776067152962809, w0=74.90000000000015, w1=13.183783550229348\n",
      "SubSGD iter. 357/499: loss=4.508219910609443, w0=75.60000000000015, w1=12.56811703356464\n",
      "SubSGD iter. 358/499: loss=4.813136981847899, w0=74.90000000000015, w1=11.43103385463792\n",
      "SubSGD iter. 359/499: loss=5.030552518248218, w0=75.60000000000015, w1=12.109738509523895\n",
      "SubSGD iter. 360/499: loss=4.929293220737006, w0=76.30000000000015, w1=12.309783988968318\n",
      "SubSGD iter. 361/499: loss=5.092799897324162, w0=75.60000000000015, w1=13.09585940576722\n",
      "SubSGD iter. 362/499: loss=4.70126382380052, w0=74.90000000000015, w1=13.270722245805565\n",
      "SubSGD iter. 363/499: loss=4.492292797106883, w0=74.20000000000014, w1=11.98027666456122\n",
      "SubSGD iter. 364/499: loss=4.73706713091849, w0=74.90000000000015, w1=12.328104078790151\n",
      "SubSGD iter. 365/499: loss=4.7031448479012115, w0=74.20000000000014, w1=12.584710351913856\n",
      "SubSGD iter. 366/499: loss=4.5285215970543184, w0=74.90000000000015, w1=13.354421316254026\n",
      "SubSGD iter. 367/499: loss=4.479112928616069, w0=74.20000000000014, w1=12.688758742825723\n",
      "SubSGD iter. 368/499: loss=4.50042236270983, w0=74.90000000000015, w1=13.367463397711697\n",
      "SubSGD iter. 369/499: loss=4.477059226973534, w0=74.20000000000014, w1=13.961384322587675\n",
      "SubSGD iter. 370/499: loss=4.303602071509158, w0=74.90000000000015, w1=13.166054632190987\n",
      "SubSGD iter. 371/499: loss=4.511627350215224, w0=75.60000000000015, w1=12.484226721842841\n",
      "SubSGD iter. 372/499: loss=4.831957442568299, w0=76.30000000000015, w1=10.94916366388571\n",
      "SubSGD iter. 373/499: loss=5.529899050892651, w0=75.60000000000015, w1=11.205769937009414\n",
      "SubSGD iter. 374/499: loss=5.24126937905873, w0=76.30000000000015, w1=11.736204866086103\n",
      "SubSGD iter. 375/499: loss=5.249624554351652, w0=75.60000000000015, w1=11.992811139209808\n",
      "SubSGD iter. 376/499: loss=4.961985843163729, w0=74.90000000000015, w1=12.249417412333512\n",
      "SubSGD iter. 377/499: loss=4.726073019059381, w0=75.60000000000015, w1=12.95053218777189\n",
      "SubSGD iter. 378/499: loss=4.729004195362673, w0=76.30000000000015, w1=11.7780118585276\n",
      "SubSGD iter. 379/499: loss=5.2368838483411135, w0=75.60000000000015, w1=12.228443339406319\n",
      "SubSGD iter. 380/499: loss=4.8964466668245805, w0=74.90000000000015, w1=13.01451875620522\n",
      "SubSGD iter. 381/499: loss=4.540752046060347, w0=75.60000000000015, w1=12.366176913717975\n",
      "SubSGD iter. 382/499: loss=4.860061830459022, w0=74.90000000000015, w1=11.671671240819967\n",
      "SubSGD iter. 383/499: loss=4.929073886643235, w0=74.20000000000014, w1=12.101193989341448\n",
      "SubSGD iter. 384/499: loss=4.68956924378673, w0=73.50000000000014, w1=12.907487957313323\n",
      "SubSGD iter. 385/499: loss=4.431008602886723, w0=72.80000000000014, w1=13.810949316679668\n",
      "SubSGD iter. 386/499: loss=4.292227083714968, w0=73.50000000000014, w1=12.626697684488086\n",
      "SubSGD iter. 387/499: loss=4.503713713199982, w0=72.80000000000014, w1=11.32374981058215\n",
      "SubSGD iter. 388/499: loss=5.099236213642643, w0=72.10000000000014, w1=11.768285985070579\n",
      "SubSGD iter. 389/499: loss=5.029334828191775, w0=71.40000000000013, w1=12.043192595272078\n",
      "SubSGD iter. 390/499: loss=5.090464279064068, w0=70.70000000000013, w1=12.709109282300219\n",
      "SubSGD iter. 391/499: loss=5.070864532064604, w0=71.40000000000013, w1=14.382383823578257\n",
      "SubSGD iter. 392/499: loss=4.500811258690739, w0=72.10000000000014, w1=14.207520983539911\n",
      "SubSGD iter. 393/499: loss=4.340196914805487, w0=72.80000000000014, w1=12.67245792558278\n",
      "SubSGD iter. 394/499: loss=4.551617199010951, w0=73.50000000000014, w1=12.003160025204881\n",
      "SubSGD iter. 395/499: loss=4.721955245473574, w0=72.80000000000014, w1=12.506420731110344\n",
      "SubSGD iter. 396/499: loss=4.606751532370924, w0=73.50000000000014, w1=12.989624768031872\n",
      "SubSGD iter. 397/499: loss=4.411951130180061, w0=74.20000000000014, w1=12.478501621142621\n",
      "SubSGD iter. 398/499: loss=4.557721671823176, w0=73.50000000000014, w1=12.939684733088646\n",
      "SubSGD iter. 399/499: loss=4.423276308186525, w0=74.20000000000014, w1=12.321184927224927\n",
      "SubSGD iter. 400/499: loss=4.606438355560898, w0=74.90000000000015, w1=12.470945941273511\n",
      "SubSGD iter. 401/499: loss=4.664179096746623, w0=74.20000000000014, w1=13.257021358072413\n",
      "SubSGD iter. 402/499: loss=4.38618322632817, w0=73.50000000000014, w1=14.186957438499963\n",
      "SubSGD iter. 403/499: loss=4.231788453223938, w0=74.20000000000014, w1=13.466518186877689\n",
      "SubSGD iter. 404/499: loss=4.356159011342725, w0=73.50000000000014, w1=13.22310753272697\n",
      "SubSGD iter. 405/499: loss=4.364934934183498, w0=72.80000000000014, w1=12.885523883880495\n",
      "SubSGD iter. 406/499: loss=4.485249320058132, w0=72.10000000000014, w1=13.904944808819645\n",
      "SubSGD iter. 407/499: loss=4.3718610371755835, w0=71.40000000000013, w1=14.334467557341126\n",
      "SubSGD iter. 408/499: loss=4.503302933179096, w0=72.10000000000014, w1=14.033261500050344\n",
      "SubSGD iter. 409/499: loss=4.355884231876207, w0=72.80000000000014, w1=13.112508040950415\n",
      "SubSGD iter. 410/499: loss=4.429732679343026, w0=73.50000000000014, w1=13.302878124805884\n",
      "SubSGD iter. 411/499: loss=4.349481305693944, w0=72.80000000000014, w1=13.824936227112703\n",
      "SubSGD iter. 412/499: loss=4.290254251939288, w0=73.50000000000014, w1=14.474215333743004\n",
      "SubSGD iter. 413/499: loss=4.213869180968169, w0=72.80000000000014, w1=14.98695814802344\n",
      "SubSGD iter. 414/499: loss=4.231969547485732, w0=72.10000000000014, w1=15.476733460031115\n",
      "SubSGD iter. 415/499: loss=4.3525270139767995, w0=71.40000000000013, w1=15.600378380107127\n",
      "SubSGD iter. 416/499: loss=4.563328345925957, w0=72.10000000000014, w1=15.83717241655825\n",
      "SubSGD iter. 417/499: loss=4.419140834004025, w0=72.80000000000014, w1=16.59928179314342\n",
      "SubSGD iter. 418/499: loss=4.5203502146035355, w0=73.50000000000014, w1=16.126245312932088\n",
      "SubSGD iter. 419/499: loss=4.373511112554902, w0=72.80000000000014, w1=16.47063069303612\n",
      "SubSGD iter. 420/499: loss=4.482784384683898, w0=73.50000000000014, w1=16.654246798278063\n",
      "SubSGD iter. 421/499: loss=4.503445434945715, w0=74.20000000000014, w1=17.13745083519959\n",
      "SubSGD iter. 422/499: loss=4.685849488715834, w0=73.50000000000014, w1=17.86193124764527\n",
      "SubSGD iter. 423/499: loss=4.918087823560865, w0=72.80000000000014, w1=17.396911046930605\n",
      "SubSGD iter. 424/499: loss=4.777421223897691, w0=72.10000000000014, w1=16.881508402716857\n",
      "SubSGD iter. 425/499: loss=4.704115800065486, w0=71.40000000000013, w1=17.67008060982325\n",
      "SubSGD iter. 426/499: loss=5.154989177739603, w0=72.10000000000014, w1=16.75538169981249\n",
      "SubSGD iter. 427/499: loss=4.665374268686317, w0=72.80000000000014, w1=16.08608379943459\n",
      "SubSGD iter. 428/499: loss=4.384967776747619, w0=73.50000000000014, w1=15.182622440068245\n",
      "SubSGD iter. 429/499: loss=4.236499196653159, w0=72.80000000000014, w1=15.971194647174636\n",
      "SubSGD iter. 430/499: loss=4.359297774423026, w0=73.50000000000014, w1=16.65346629193875\n",
      "SubSGD iter. 431/499: loss=4.503231857565174, w0=72.80000000000014, w1=17.614360425223335\n",
      "SubSGD iter. 432/499: loss=4.856769843712583, w0=73.50000000000014, w1=17.156678475195182\n",
      "SubSGD iter. 433/499: loss=4.654522942446252, w0=72.80000000000014, w1=17.822595162223323\n",
      "SubSGD iter. 434/499: loss=4.9396403241144, w0=72.10000000000014, w1=18.079201435347027\n",
      "SubSGD iter. 435/499: loss=5.140872540102248, w0=71.40000000000013, w1=18.20284635542304\n",
      "SubSGD iter. 436/499: loss=5.3716599052439085, w0=70.70000000000013, w1=17.56670964192185\n",
      "SubSGD iter. 437/499: loss=5.340183007422771, w0=71.40000000000013, w1=16.839979092274636\n",
      "SubSGD iter. 438/499: loss=4.8591197187043385, w0=70.70000000000013, w1=16.174316518846332\n",
      "SubSGD iter. 439/499: loss=4.917664993156899, w0=70.00000000000013, w1=15.146172129853179\n",
      "SubSGD iter. 440/499: loss=5.084856208970569, w0=70.70000000000013, w1=15.629376166774707\n",
      "SubSGD iter. 441/499: loss=4.825857436779151, w0=71.40000000000013, w1=15.011133118203857\n",
      "SubSGD iter. 442/499: loss=4.4943458401874015, w0=70.70000000000013, w1=15.638868799786728\n",
      "SubSGD iter. 443/499: loss=4.827114071496046, w0=70.00000000000013, w1=15.064966772744752\n",
      "SubSGD iter. 444/499: loss=5.07919166134572, w0=70.70000000000013, w1=15.301760809195875\n",
      "SubSGD iter. 445/499: loss=4.790787562104369, w0=70.00000000000013, w1=15.81450362347631\n",
      "SubSGD iter. 446/499: loss=5.1635237802589655, w0=70.70000000000013, w1=15.998314277803795\n",
      "SubSGD iter. 447/499: loss=4.881082292572295, w0=71.40000000000013, w1=15.271583728156582\n",
      "SubSGD iter. 448/499: loss=4.518647826153955, w0=70.70000000000013, w1=15.333682477085386\n",
      "SubSGD iter. 449/499: loss=4.793352004243851, w0=71.40000000000013, w1=15.517298582327328\n",
      "SubSGD iter. 450/499: loss=4.551070469450671, w0=72.10000000000014, w1=16.29470817920213\n",
      "SubSGD iter. 451/499: loss=4.532839412169945, w0=71.40000000000013, w1=15.524997214861958\n",
      "SubSGD iter. 452/499: loss=4.552206351929445, w0=72.10000000000014, w1=16.05543214393865\n",
      "SubSGD iter. 453/499: loss=4.471146290099401, w0=72.80000000000014, w1=16.62108894028473\n",
      "SubSGD iter. 454/499: loss=4.526982750008763, w0=72.10000000000014, w1=15.92658326738672\n",
      "SubSGD iter. 455/499: loss=4.440220519709644, w0=71.40000000000013, w1=15.29044655388553\n",
      "SubSGD iter. 456/499: loss=4.5208446213124285, w0=72.10000000000014, w1=16.630311140996167\n",
      "SubSGD iter. 457/499: loss=4.627923384590451, w0=72.80000000000014, w1=16.329105083705386\n",
      "SubSGD iter. 458/499: loss=4.4448262482231975, w0=72.10000000000014, w1=16.841847897985822\n",
      "SubSGD iter. 459/499: loss=4.69182818590815, w0=72.80000000000014, w1=17.078641934436945\n",
      "SubSGD iter. 460/499: loss=4.67182857301201, w0=73.50000000000014, w1=16.224957068433795\n",
      "SubSGD iter. 461/499: loss=4.394000280147407, w0=74.20000000000014, w1=16.678608141095925\n",
      "SubSGD iter. 462/499: loss=4.550463595279622, w0=73.50000000000014, w1=15.98978498583221\n",
      "SubSGD iter. 463/499: loss=4.3457860192208555, w0=72.80000000000014, w1=15.459350056755522\n",
      "SubSGD iter. 464/499: loss=4.271258666924837, w0=72.10000000000014, w1=14.823213343254332\n",
      "SubSGD iter. 465/499: loss=4.317368841125453, w0=71.40000000000013, w1=15.441456391825183\n",
      "SubSGD iter. 466/499: loss=4.5402948397674825, w0=72.10000000000014, w1=16.654732901511625\n",
      "SubSGD iter. 467/499: loss=4.635236188907769, w0=71.40000000000013, w1=16.542091168043875\n",
      "SubSGD iter. 468/499: loss=4.768609005552861, w0=70.70000000000013, w1=15.749137688899358\n",
      "SubSGD iter. 469/499: loss=4.842945919297987, w0=71.40000000000013, w1=14.986859717769791\n",
      "SubSGD iter. 470/499: loss=4.492745402974012, w0=72.10000000000014, w1=14.20078430097089\n",
      "SubSGD iter. 471/499: loss=4.34074139637934, w0=72.80000000000014, w1=14.350153457394493\n",
      "SubSGD iter. 472/499: loss=4.239955190549291, w0=73.50000000000014, w1=14.540523541249962\n",
      "SubSGD iter. 473/499: loss=4.211180474221129, w0=72.80000000000014, w1=15.329095748356353\n",
      "SubSGD iter. 474/499: loss=4.256809406679739, w0=72.10000000000014, w1=15.529946871748345\n",
      "SubSGD iter. 475/499: loss=4.360374143112392, w0=71.40000000000013, w1=16.106868156532542\n",
      "SubSGD iter. 476/499: loss=4.657542648427784, w0=70.70000000000013, w1=16.833598706179757\n",
      "SubSGD iter. 477/499: loss=5.09460082577724, w0=71.40000000000013, w1=15.852963103135016\n",
      "SubSGD iter. 478/499: loss=4.605984302154916, w0=72.10000000000014, w1=17.06623961282146\n",
      "SubSGD iter. 479/499: loss=4.761869743050723, w0=72.80000000000014, w1=16.560155585799638\n",
      "SubSGD iter. 480/499: loss=4.508473511785385, w0=72.10000000000014, w1=16.34020247311902\n",
      "SubSGD iter. 481/499: loss=4.545467279548458, w0=71.40000000000013, w1=16.541053596511013\n",
      "SubSGD iter. 482/499: loss=4.768303013911295, w0=72.10000000000014, w1=16.13410497891227\n",
      "SubSGD iter. 483/499: loss=4.490029016091385, w0=72.80000000000014, w1=15.338775288515581\n",
      "SubSGD iter. 484/499: loss=4.257723295325207, w0=73.50000000000014, w1=14.424076378504822\n",
      "SubSGD iter. 485/499: loss=4.216153070624515, w0=72.80000000000014, w1=13.729570705606813\n",
      "SubSGD iter. 486/499: loss=4.305467204887481, w0=73.50000000000014, w1=14.81791619728974\n",
      "SubSGD iter. 487/499: loss=4.206066640789446, w0=74.20000000000014, w1=14.661722707480582\n",
      "SubSGD iter. 488/499: loss=4.265797727239072, w0=73.50000000000014, w1=15.259505347538054\n",
      "SubSGD iter. 489/499: loss=4.245422258732511, w0=72.80000000000014, w1=16.278926272477204\n",
      "SubSGD iter. 490/499: loss=4.432056377150158, w0=73.50000000000014, w1=16.224275502840822\n",
      "SubSGD iter. 491/499: loss=4.393858810556589, w0=74.20000000000014, w1=16.871641185044997\n",
      "SubSGD iter. 492/499: loss=4.603639663840392, w0=73.50000000000014, w1=17.36141649705267\n",
      "SubSGD iter. 493/499: loss=4.726836258745393, w0=72.80000000000014, w1=16.42019260317831\n",
      "SubSGD iter. 494/499: loss=4.4689912895840855, w0=73.50000000000014, w1=15.997752301515341\n",
      "SubSGD iter. 495/499: loss=4.347279842383374, w0=72.80000000000014, w1=16.786324508621732\n",
      "SubSGD iter. 496/499: loss=4.5772383267743315, w0=72.10000000000014, w1=16.01661354428156\n",
      "SubSGD iter. 497/499: loss=4.4618292115809615, w0=71.40000000000013, w1=15.04718110783412\n",
      "SubSGD iter. 498/499: loss=4.496722620656576, w0=70.70000000000013, w1=15.61120335262097\n",
      "SubSGD iter. 499/499: loss=4.82345171972042, w0=71.40000000000013, w1=15.876628038339273\n",
      "SubSGD: execution time=0.093 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f484040c932c42c38fcf8f4500c1760d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses, subsgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "state": {
    "d2b2c3aea192430e81437f33ba0b0e69": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "e4a6a7a70ccd42ddb112989c04f2ed3f": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
