{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_cost` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    e = np.abs(y - np.ravel(np.dot(tx, w)))\n",
    "    return np.sum(e)/e.shape[0]\n",
    "    \n",
    "    # ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from costs import *\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "        \n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    # ***************************************************\n",
    "    \n",
    "    for w0_index in range(losses.shape[0]):\n",
    "        for w1_index in range(losses.shape[1]):\n",
    "            losses[w0_index, w1_index] = compute_loss(y, tx, [[grid_w0[w0_index]],[grid_w1[w1_index]]])\n",
    "    \n",
    "    # ***************************************************\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=42.42448314678248, w0*=66.66666666666669, w1*=16.666666666666686, execution time=0.031 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=10)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "      l=loss_star, w0=w0_star, w1=w1_star, t=execution_time))\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0,6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    \n",
    "    e = y - np.ravel(np.dot(tx, w));   \n",
    "    return - np.dot(np.transpose(tx), e)/e.shape[0]\n",
    "    \n",
    "    # ***************************************************\n",
    "\n",
    "#y1, tx1 = build_model_data([3,2],[1,3])\n",
    "#w1 = [100, 20]\n",
    "#print(compute_gradient(y1, tx1, w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        w = w - gamma * compute_gradient(y, tx, w)\n",
    "        # ***************************************************\n",
    "        # ***************************************************\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        # ***************************************************\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2264.6350560300057, w0=7.329392200210475, w1=1.3479712434989204\n",
      "GD iter. 1/49: loss=1837.2777140793814, w0=13.92584518039996, w1=2.5611453626479217\n",
      "GD iter. 2/49: loss=1491.1182670993776, w0=19.862652862570464, w1=3.653002069882027\n",
      "GD iter. 3/49: loss=1210.7291150455742, w0=25.205779776523926, w1=4.635673106392729\n",
      "GD iter. 4/49: loss=983.6139018819919, w0=30.014593999082066, w1=5.520077039252341\n",
      "GD iter. 5/49: loss=799.6505792194912, w0=34.34252679938437, w1=6.316040578826014\n",
      "GD iter. 6/49: loss=650.6402878628655, w0=38.237666319656455, w1=7.032407764442313\n",
      "GD iter. 7/49: loss=529.9419518639996, w0=41.7432918879013, w1=7.677138231496978\n",
      "GD iter. 8/49: loss=432.1762997049176, w0=44.89835489932168, w1=8.257395651846178\n",
      "GD iter. 9/49: loss=352.98612145606035, w0=47.73791160960005, w1=8.779627330160464\n",
      "GD iter. 10/49: loss=288.8420770744868, w0=50.29351264885055, w1=9.249635840643316\n",
      "GD iter. 11/49: loss=236.8854011254118, w0=52.59355358417602, w1=9.67264350007789\n",
      "GD iter. 12/49: loss=194.80049360666106, w0=54.66359042596894, w1=10.053350393569005\n",
      "GD iter. 13/49: loss=160.711718516473, w0=56.52662358358257, w1=10.395986597711003\n",
      "GD iter. 14/49: loss=133.0998106934208, w0=58.20335342543483, w1=10.704359181438804\n",
      "GD iter. 15/49: loss=110.73416535674839, w0=59.71241028310187, w1=10.981894506793827\n",
      "GD iter. 16/49: loss=92.6179926340438, w0=61.0705614550022, w1=11.231676299613346\n",
      "GD iter. 17/49: loss=77.94389272865318, w0=62.29289750971249, w1=11.456479913150911\n",
      "GD iter. 18/49: loss=66.05787180528668, w0=63.392999958951755, w1=11.658803165334724\n",
      "GD iter. 19/49: loss=56.43019485735972, w0=64.38309216326711, w1=11.840894092300157\n",
      "GD iter. 20/49: loss=48.63177652953899, w0=65.27417514715091, w1=12.004775926569046\n",
      "GD iter. 21/49: loss=42.31505768400419, w0=66.07614983264634, w1=12.152269577411046\n",
      "GD iter. 22/49: loss=37.19851541912101, w0=66.79792704959222, w1=12.285013863168846\n",
      "GD iter. 23/49: loss=33.054116184565615, w0=67.44752654484351, w1=12.404483720350866\n",
      "GD iter. 24/49: loss=29.69715280457577, w0=68.03216609056967, w1=12.512006591814686\n",
      "GD iter. 25/49: loss=26.97801246678398, w0=68.55834168172322, w1=12.608777176132122\n",
      "GD iter. 26/49: loss=24.775508793172605, w0=69.03189971376142, w1=12.695870702017814\n",
      "GD iter. 27/49: loss=22.991480817547423, w0=69.45810194259579, w1=12.774254875314934\n",
      "GD iter. 28/49: loss=21.546418157290994, w0=69.84168394854673, w1=12.844800631282347\n",
      "GD iter. 29/49: loss=20.3759174024833, w0=70.18690775390257, w1=12.908291811653017\n",
      "GD iter. 30/49: loss=19.42781179108905, w0=70.49760917872284, w1=12.965433873986619\n",
      "GD iter. 31/49: loss=18.659846245859725, w0=70.77724046106107, w1=13.016861730086863\n",
      "GD iter. 32/49: loss=18.03779415422397, w0=71.02890861516548, w1=13.063146800577082\n",
      "GD iter. 33/49: loss=17.533931959998984, w0=71.25540995385946, w1=13.104803364018277\n",
      "GD iter. 34/49: loss=17.12580358267677, w0=71.45926115868403, w1=13.142294271115354\n",
      "GD iter. 35/49: loss=16.79521959704577, w0=71.64272724302614, w1=13.176036087502723\n",
      "GD iter. 36/49: loss=16.52744656868467, w0=71.80784671893404, w1=13.206403722251356\n",
      "GD iter. 37/49: loss=16.310550415712164, w0=71.95645424725116, w1=13.233734593525124\n",
      "GD iter. 38/49: loss=16.13486453180444, w0=72.09020102273657, w1=13.258332377671517\n",
      "GD iter. 39/49: loss=15.99255896583918, w0=72.21057312067343, w1=13.28047038340327\n",
      "GD iter. 40/49: loss=15.877291457407319, w0=72.3189080088166, w1=13.300394588561847\n",
      "GD iter. 41/49: loss=15.78392477557751, w0=72.41640940814547, w1=13.318326373204567\n",
      "GD iter. 42/49: loss=15.708297763295374, w0=72.50416066754144, w1=13.334464979383014\n",
      "GD iter. 43/49: loss=15.647039883346832, w0=72.58313680099782, w1=13.348989724943618\n",
      "GD iter. 44/49: loss=15.597421000588522, w0=72.65421532110855, w1=13.36206199594816\n",
      "GD iter. 45/49: loss=15.557229705554297, w0=72.71818598920821, w1=13.373827039852248\n",
      "GD iter. 46/49: loss=15.524674756576562, w0=72.77575959049791, w1=13.384415579365928\n",
      "GD iter. 47/49: loss=15.498305247904606, w0=72.82757583165863, w1=13.39394526492824\n",
      "GD iter. 48/49: loss=15.476945945880315, w0=72.8742104487033, w1=13.40252198193432\n",
      "GD iter. 49/49: loss=15.459644911240636, w0=72.91618160404349, w1=13.410241027239794\n",
      "GD: execution time=0.011 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4a7e21f6ba44198a95c775b5da5a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from just few examples n and their corresponding y_n labels.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ***************************************************\n",
    "    \n",
    "    e = y - np.ravel(np.dot(tx, w))\n",
    "    return -np.dot(np.transpose(tx), e)/e.shape[0]\n",
    "    \n",
    "    # ***************************************************\n",
    "    \n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        \n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            w = w - gamma * compute_stoch_gradient(minibatch_y, minibatch_tx, w)\n",
    "        \n",
    "        loss = compute_loss(y, tx, w)\n",
    "        \n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # TODO: implement stochastic gradient descent.\n",
    "        # ***************************************************\n",
    "        \n",
    "        print(\"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=2161.704709994212, w0=8.453355146119618, w1=4.080854310448986\n",
      "SGD iter. 1/49: loss=1830.12749991558, w0=14.02729883733159, w1=2.6653422388923484\n",
      "SGD iter. 2/49: loss=1364.5970000187524, w0=21.57162617975219, w1=18.29908367663981\n",
      "SGD iter. 3/49: loss=1201.529463783711, w0=25.004853871144043, w1=19.839983724262417\n",
      "SGD iter. 4/49: loss=940.7815011657491, w0=30.60133466292102, w1=18.78388182142178\n",
      "SGD iter. 5/49: loss=766.3033016258719, w0=35.10766308602658, w1=20.086107632696386\n",
      "SGD iter. 6/49: loss=658.8774671220386, w0=38.225359566761085, w1=21.04139791011894\n",
      "SGD iter. 7/49: loss=622.0147675473405, w0=40.44433432256834, w1=25.062559608350277\n",
      "SGD iter. 8/49: loss=570.7405278923463, w0=42.699342930138194, w1=26.696406857430393\n",
      "SGD iter. 9/49: loss=496.0605228075471, w0=45.71463067867738, w1=27.6477031173312\n",
      "SGD iter. 10/49: loss=433.4721960798567, w0=48.083492587262114, w1=27.643287730135008\n",
      "SGD iter. 11/49: loss=340.2549711297076, w0=51.94299128340806, w1=27.40364592977245\n",
      "SGD iter. 12/49: loss=207.47602134532005, w0=55.8860572691855, w1=22.487848335015855\n",
      "SGD iter. 13/49: loss=205.90913664548037, w0=56.29360109961611, w1=23.073230390655496\n",
      "SGD iter. 14/49: loss=102.79809530387031, w0=60.22852321569756, w1=15.509433948453317\n",
      "SGD iter. 15/49: loss=79.52012639269485, w0=62.00286512539025, w1=12.596246728513005\n",
      "SGD iter. 16/49: loss=69.28591118631205, w0=63.17731506179431, w1=11.144265871648512\n",
      "SGD iter. 17/49: loss=62.917141985521546, w0=64.24343196472789, w1=9.853262612267297\n",
      "SGD iter. 18/49: loss=59.16662652866125, w0=64.85021364081193, w1=9.44669043788935\n",
      "SGD iter. 19/49: loss=57.33728270952893, w0=65.15705513909558, w1=9.27326650819346\n",
      "SGD iter. 20/49: loss=56.45418878045124, w0=65.32536853553546, w1=9.162449705705214\n",
      "SGD iter. 21/49: loss=49.87828375112509, w0=66.18787401370432, w1=9.179843396153636\n",
      "SGD iter. 22/49: loss=36.184985071526704, w0=67.3673411390767, w1=10.935339490749323\n",
      "SGD iter. 23/49: loss=25.159082450544208, w0=69.00003142770574, w1=12.426672597892106\n",
      "SGD iter. 24/49: loss=23.218027312892985, w0=69.459602763232, w1=12.498756338859314\n",
      "SGD iter. 25/49: loss=22.844535262846794, w0=70.23662746026315, w1=11.119575813422898\n",
      "SGD iter. 26/49: loss=18.992713340814937, w0=71.3326130476897, w1=11.644796052888564\n",
      "SGD iter. 27/49: loss=16.529968575504725, w0=72.73486018757384, w1=12.074148028466327\n",
      "SGD iter. 28/49: loss=16.12082345163318, w0=72.27980530537619, w1=12.81530405829365\n",
      "SGD iter. 29/49: loss=16.216604674427042, w0=72.17773660159749, w1=12.83507008229358\n",
      "SGD iter. 30/49: loss=15.61386902848013, w0=72.73740212054236, w1=13.862136206120958\n",
      "SGD iter. 31/49: loss=15.687987377980102, w0=72.63372201300983, w1=13.88999891136853\n",
      "SGD iter. 32/49: loss=15.525977318569986, w0=73.09457751575162, w1=13.970059939562695\n",
      "SGD iter. 33/49: loss=15.662981973705367, w0=72.98452982816264, w1=14.156812643566061\n",
      "SGD iter. 34/49: loss=16.759233634284527, w0=74.08949690480075, w1=14.933587293809849\n",
      "SGD iter. 35/49: loss=16.8232820165009, w0=73.93972690411553, w1=15.047425522222301\n",
      "SGD iter. 36/49: loss=17.775415300191682, w0=74.45386184109127, w1=15.332708504239472\n",
      "SGD iter. 37/49: loss=16.6305896102774, w0=73.77655439695022, w1=14.981867371852141\n",
      "SGD iter. 38/49: loss=16.287182596511418, w0=74.04003358514034, w1=14.595914460173034\n",
      "SGD iter. 39/49: loss=15.710598023116294, w0=73.01706804428535, w1=14.2365295823987\n",
      "SGD iter. 40/49: loss=15.506046448296221, w0=73.35280807348677, w1=13.966384387674719\n",
      "SGD iter. 41/49: loss=15.775988029530915, w0=73.79466250454382, w1=14.207351954683062\n",
      "SGD iter. 42/49: loss=16.312555332559718, w0=73.33993945510022, w1=14.840306908548486\n",
      "SGD iter. 43/49: loss=16.020686256669876, w0=73.60019109615371, w1=14.56405371784775\n",
      "SGD iter. 44/49: loss=17.483519173761575, w0=74.5477179776893, w1=15.099360008220163\n",
      "SGD iter. 45/49: loss=17.647537534087903, w0=74.47268942539188, w1=15.249968544232871\n",
      "SGD iter. 46/49: loss=17.161501635098606, w0=74.15711951261541, w1=15.154859470442169\n",
      "SGD iter. 47/49: loss=17.381139433854372, w0=73.40397180688255, w1=15.474303162561602\n",
      "SGD iter. 48/49: loss=17.618629829282508, w0=73.75646012361268, w1=15.54163930252304\n",
      "SGD iter. 49/49: loss=17.666646524015263, w0=73.43205134881738, w1=15.611011943258262\n",
      "SGD: execution time=0.027 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30cee160c74548e1b50266cb6fada408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "# ***************************************************\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "# TODO: reload the data by subsampling first, then by subsampling and adding outliers\n",
    "# ***************************************************\n",
    "\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202,), (202, 2))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=318.28212470159644, w0=51.84746409844842, w1=7.724426406192425\n",
      "GD iter. 1/49: loss=88.6423556165128, w0=67.40170332798297, w1=10.041754328050116\n",
      "GD iter. 2/49: loss=67.97477639885521, w0=72.06797509684336, w1=10.736952704607413\n",
      "GD iter. 3/49: loss=66.11469426926604, w0=73.46785662750146, w1=10.9455122175746\n",
      "GD iter. 4/49: loss=65.94728687760302, w0=73.88782108669889, w1=11.00808007146475\n",
      "GD iter. 5/49: loss=65.93222021235334, w0=74.01381042445813, w1=11.026850427631798\n",
      "GD iter. 6/49: loss=65.93086421248087, w0=74.0516072257859, w1=11.032481534481912\n",
      "GD iter. 7/49: loss=65.93074217249234, w0=74.06294626618423, w1=11.034170866536945\n",
      "GD iter. 8/49: loss=65.93073118889338, w0=74.06634797830372, w1=11.034677666153454\n",
      "GD iter. 9/49: loss=65.93073020036947, w0=74.06736849193958, w1=11.034829706038408\n",
      "GD iter. 10/49: loss=65.93073011140231, w0=74.06767464603033, w1=11.034875318003895\n",
      "GD iter. 11/49: loss=65.93073010339529, w0=74.06776649225755, w1=11.034889001593541\n",
      "GD iter. 12/49: loss=65.93073010267466, w0=74.06779404612573, w1=11.034893106670431\n",
      "GD iter. 13/49: loss=65.93073010260979, w0=74.06780231228618, w1=11.034894338193501\n",
      "GD iter. 14/49: loss=65.93073010260393, w0=74.06780479213431, w1=11.034894707650421\n",
      "GD iter. 15/49: loss=65.93073010260342, w0=74.06780553608874, w1=11.034894818487496\n",
      "GD iter. 16/49: loss=65.93073010260339, w0=74.06780575927507, w1=11.03489485173862\n",
      "GD iter. 17/49: loss=65.93073010260336, w0=74.06780582623098, w1=11.034894861713957\n",
      "GD iter. 18/49: loss=65.93073010260338, w0=74.06780584631775, w1=11.034894864706557\n",
      "GD iter. 19/49: loss=65.93073010260338, w0=74.06780585234378, w1=11.034894865604338\n",
      "GD iter. 20/49: loss=65.93073010260338, w0=74.06780585415159, w1=11.034894865873675\n",
      "GD iter. 21/49: loss=65.93073010260338, w0=74.06780585469393, w1=11.034894865954474\n",
      "GD iter. 22/49: loss=65.93073010260336, w0=74.06780585485663, w1=11.034894865978712\n",
      "GD iter. 23/49: loss=65.93073010260338, w0=74.06780585490544, w1=11.034894865985985\n",
      "GD iter. 24/49: loss=65.93073010260338, w0=74.0678058549201, w1=11.034894865988166\n",
      "GD iter. 25/49: loss=65.93073010260336, w0=74.06780585492449, w1=11.034894865988823\n",
      "GD iter. 26/49: loss=65.93073010260336, w0=74.06780585492581, w1=11.034894865989017\n",
      "GD iter. 27/49: loss=65.93073010260338, w0=74.06780585492619, w1=11.034894865989076\n",
      "GD iter. 28/49: loss=65.93073010260339, w0=74.06780585492632, w1=11.034894865989099\n",
      "GD iter. 29/49: loss=65.93073010260338, w0=74.06780585492635, w1=11.0348948659891\n",
      "GD iter. 30/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 31/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 32/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 33/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 34/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 35/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 36/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 37/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 38/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 39/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 40/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 41/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 42/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 43/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 44/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 45/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 46/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 47/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 48/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 49/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD: execution time=0.006 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# ***************************************************\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "# TODO: fit the model to the subsampled data / subsampled data with outliers and visualize the cloud of points \n",
    "#       and the model fit\n",
    "# ***************************************************\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f6ae61840f4ab69caeacf806493a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    e = y - np.ravel(np.dot(tx, w))\n",
    "    \n",
    "    def subgradient_abs(x):\n",
    "        if x>0: \n",
    "            return 1 \n",
    "        elif x<0:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0.1\n",
    "    \n",
    "    subgradient_abs = np.vectorize(subgradient_abs)    \n",
    "    return np.dot(subgradient_abs(e), -tx)/e.shape[0]\n",
    "\n",
    "    # TODO: compute subgradient gradient vector for MAE\n",
    "    # ***************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        g = compute_subgradient_mae(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        # TODO: compute subgradient and loss\n",
    "        # ***************************************************\n",
    "\n",
    "        # ***************************************************\n",
    "        w = w - gamma * g\n",
    "        # TODO: update w by subgradient\n",
    "        # ***************************************************\n",
    "        \n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=2869.8351145358524, w0=0.7, w1=6.109524327590712e-16\n",
      "SubGD iter. 1/499: loss=2818.2326504374037, w0=1.4, w1=1.2219048655181425e-15\n",
      "SubGD iter. 2/499: loss=2767.120186338956, w0=2.0999999999999996, w1=1.832857298277214e-15\n",
      "SubGD iter. 3/499: loss=2716.4977222405073, w0=2.8, w1=2.443809731036285e-15\n",
      "SubGD iter. 4/499: loss=2666.365258142059, w0=3.5, w1=3.054762163795356e-15\n",
      "SubGD iter. 5/499: loss=2616.72279404361, w0=4.2, w1=3.665714596554428e-15\n",
      "SubGD iter. 6/499: loss=2567.570329945162, w0=4.9, w1=4.276667029313499e-15\n",
      "SubGD iter. 7/499: loss=2518.9078658467133, w0=5.6000000000000005, w1=4.887619462072571e-15\n",
      "SubGD iter. 8/499: loss=2470.735401748265, w0=6.300000000000001, w1=5.498571894831642e-15\n",
      "SubGD iter. 9/499: loss=2423.052937649816, w0=7.000000000000001, w1=6.109524327590714e-15\n",
      "SubGD iter. 10/499: loss=2375.8604735513677, w0=7.700000000000001, w1=6.720476760349785e-15\n",
      "SubGD iter. 11/499: loss=2329.1580094529195, w0=8.4, w1=7.331429193108857e-15\n",
      "SubGD iter. 12/499: loss=2282.9455453544715, w0=9.1, w1=7.942381625867928e-15\n",
      "SubGD iter. 13/499: loss=2237.223081256023, w0=9.799999999999999, w1=8.553334058627e-15\n",
      "SubGD iter. 14/499: loss=2191.9906171575744, w0=10.499999999999998, w1=9.164286491386072e-15\n",
      "SubGD iter. 15/499: loss=2147.248153059126, w0=11.199999999999998, w1=9.775238924145143e-15\n",
      "SubGD iter. 16/499: loss=2102.995688960677, w0=11.899999999999997, w1=1.0386191356904215e-14\n",
      "SubGD iter. 17/499: loss=2059.2332248622292, w0=12.599999999999996, w1=1.0997143789663286e-14\n",
      "SubGD iter. 18/499: loss=2015.9607607637806, w0=13.299999999999995, w1=1.1608096222422358e-14\n",
      "SubGD iter. 19/499: loss=1973.1782966653323, w0=13.999999999999995, w1=1.2219048655181429e-14\n",
      "SubGD iter. 20/499: loss=1930.8858325668837, w0=14.699999999999994, w1=1.28300010879405e-14\n",
      "SubGD iter. 21/499: loss=1889.0833684684355, w0=15.399999999999993, w1=1.3440953520699572e-14\n",
      "SubGD iter. 22/499: loss=1847.7709043699872, w0=16.099999999999994, w1=1.4051905953458644e-14\n",
      "SubGD iter. 23/499: loss=1806.9484402715382, w0=16.799999999999994, w1=1.4662858386217714e-14\n",
      "SubGD iter. 24/499: loss=1766.6159761730898, w0=17.499999999999993, w1=1.5273810818976784e-14\n",
      "SubGD iter. 25/499: loss=1726.7735120746415, w0=18.199999999999992, w1=1.5884763251735854e-14\n",
      "SubGD iter. 26/499: loss=1687.4210479761932, w0=18.89999999999999, w1=1.6495715684494924e-14\n",
      "SubGD iter. 27/499: loss=1648.5585838777447, w0=19.59999999999999, w1=1.7106668117253994e-14\n",
      "SubGD iter. 28/499: loss=1610.1861197792962, w0=20.29999999999999, w1=1.7717620550013064e-14\n",
      "SubGD iter. 29/499: loss=1572.3036556808477, w0=20.99999999999999, w1=1.8328572982772134e-14\n",
      "SubGD iter. 30/499: loss=1534.9111915823992, w0=21.69999999999999, w1=1.8939525415531204e-14\n",
      "SubGD iter. 31/499: loss=1498.0087274839507, w0=22.399999999999988, w1=1.9550477848290273e-14\n",
      "SubGD iter. 32/499: loss=1461.5962633855022, w0=23.099999999999987, w1=2.0161430281049343e-14\n",
      "SubGD iter. 33/499: loss=1425.6737992870542, w0=23.799999999999986, w1=2.0772382713808413e-14\n",
      "SubGD iter. 34/499: loss=1390.2413351886053, w0=24.499999999999986, w1=2.1383335146567483e-14\n",
      "SubGD iter. 35/499: loss=1355.2988710901573, w0=25.199999999999985, w1=2.1994287579326553e-14\n",
      "SubGD iter. 36/499: loss=1320.8464069917088, w0=25.899999999999984, w1=2.2605240012085623e-14\n",
      "SubGD iter. 37/499: loss=1286.8839428932602, w0=26.599999999999984, w1=2.3216192444844693e-14\n",
      "SubGD iter. 38/499: loss=1253.4114787948117, w0=27.299999999999983, w1=2.3827144877603763e-14\n",
      "SubGD iter. 39/499: loss=1220.4290146963633, w0=27.999999999999982, w1=2.4438097310362833e-14\n",
      "SubGD iter. 40/499: loss=1187.9365505979147, w0=28.69999999999998, w1=2.5049049743121903e-14\n",
      "SubGD iter. 41/499: loss=1155.9340864994663, w0=29.39999999999998, w1=2.5660002175880973e-14\n",
      "SubGD iter. 42/499: loss=1124.4216224010179, w0=30.09999999999998, w1=2.6270954608640043e-14\n",
      "SubGD iter. 43/499: loss=1093.3991583025695, w0=30.79999999999998, w1=2.6881907041399113e-14\n",
      "SubGD iter. 44/499: loss=1062.866694204121, w0=31.49999999999998, w1=2.7492859474158183e-14\n",
      "SubGD iter. 45/499: loss=1032.8242301056725, w0=32.19999999999998, w1=2.8103811906917253e-14\n",
      "SubGD iter. 46/499: loss=1003.2717660072241, w0=32.899999999999984, w1=2.871476433967632e-14\n",
      "SubGD iter. 47/499: loss=974.2093019087754, w0=33.59999999999999, w1=2.9325716772435396e-14\n",
      "SubGD iter. 48/499: loss=945.6368378103266, w0=34.29999999999999, w1=2.993666920519447e-14\n",
      "SubGD iter. 49/499: loss=917.554373711878, w0=34.99999999999999, w1=3.054762163795354e-14\n",
      "SubGD iter. 50/499: loss=889.9619096134294, w0=35.699999999999996, w1=3.1158574070712615e-14\n",
      "SubGD iter. 51/499: loss=862.859445514981, w0=36.4, w1=3.176952650347169e-14\n",
      "SubGD iter. 52/499: loss=836.2469814165324, w0=37.1, w1=3.238047893623076e-14\n",
      "SubGD iter. 53/499: loss=810.1245173180838, w0=37.800000000000004, w1=3.2991431368989835e-14\n",
      "SubGD iter. 54/499: loss=784.4920532196353, w0=38.50000000000001, w1=3.360238380174891e-14\n",
      "SubGD iter. 55/499: loss=759.3495891211868, w0=39.20000000000001, w1=3.421333623450798e-14\n",
      "SubGD iter. 56/499: loss=734.6971250227382, w0=39.90000000000001, w1=3.4824288667267054e-14\n",
      "SubGD iter. 57/499: loss=710.5346609242895, w0=40.600000000000016, w1=3.543524110002613e-14\n",
      "SubGD iter. 58/499: loss=686.8621968258409, w0=41.30000000000002, w1=3.60461935327852e-14\n",
      "SubGD iter. 59/499: loss=663.6797327273924, w0=42.00000000000002, w1=3.6657145965544273e-14\n",
      "SubGD iter. 60/499: loss=640.9872686289439, w0=42.700000000000024, w1=3.7268098398303347e-14\n",
      "SubGD iter. 61/499: loss=618.7848045304953, w0=43.40000000000003, w1=3.787905083106242e-14\n",
      "SubGD iter. 62/499: loss=597.0723404320469, w0=44.10000000000003, w1=3.849000326382149e-14\n",
      "SubGD iter. 63/499: loss=575.8498763335982, w0=44.80000000000003, w1=3.9100955696580566e-14\n",
      "SubGD iter. 64/499: loss=555.1174122351497, w0=45.500000000000036, w1=3.971190812933964e-14\n",
      "SubGD iter. 65/499: loss=534.8749481367013, w0=46.20000000000004, w1=4.032286056209871e-14\n",
      "SubGD iter. 66/499: loss=515.1224840382527, w0=46.90000000000004, w1=4.0933812994857785e-14\n",
      "SubGD iter. 67/499: loss=495.8600199398042, w0=47.59306930693074, w1=0.011147845678271063\n",
      "SubGD iter. 68/499: loss=477.1480669293974, w0=48.279207920792125, w1=0.03308574108989941\n",
      "SubGD iter. 69/499: loss=458.97652381717785, w0=48.96534653465351, w1=0.05502363650152776\n",
      "SubGD iter. 70/499: loss=441.2762481736449, w0=49.63069306930698, w1=0.10538326388307814\n",
      "SubGD iter. 71/499: loss=424.24408268143, w0=50.28910891089114, w1=0.16746568532793435\n",
      "SubGD iter. 72/499: loss=407.6944527790818, w0=50.947524752475296, w1=0.22954810677279056\n",
      "SubGD iter. 73/499: loss=391.58218852423494, w0=51.59207920792084, w1=0.31242512932747524\n",
      "SubGD iter. 74/499: loss=375.99555288487295, w0=52.22277227722777, w1=0.4119501328839991\n",
      "SubGD iter. 75/499: loss=360.95695350929594, w0=52.84653465346539, w1=0.5208167847923756\n",
      "SubGD iter. 76/499: loss=346.3748247543329, w0=53.4564356435644, w1=0.6457900912635992\n",
      "SubGD iter. 77/499: loss=332.3117701076255, w0=54.0594059405941, w1=0.7796904498577214\n",
      "SubGD iter. 78/499: loss=318.6833724768497, w0=54.655445544554496, w1=0.9197570104995693\n",
      "SubGD iter. 79/499: loss=305.5086034302328, w0=55.24455445544559, w1=1.0670920297849913\n",
      "SubGD iter. 80/499: loss=292.7666734173505, w0=55.819801980198065, w1=1.2261255948210765\n",
      "SubGD iter. 81/499: loss=280.53153011615814, w0=56.36732673267331, w1=1.410709342622213\n",
      "SubGD iter. 82/499: loss=268.89668417535404, w0=56.900990099009945, w1=1.605853732220269\n",
      "SubGD iter. 83/499: loss=257.733920052546, w0=57.42772277227727, w1=1.808762802293962\n",
      "SubGD iter. 84/499: loss=246.93766902970765, w0=57.933663366336674, w1=2.0285064197514697\n",
      "SubGD iter. 85/499: loss=236.64352344592254, w0=58.43267326732677, w1=2.2494370848672776\n",
      "SubGD iter. 86/499: loss=226.75154983045, w0=58.91089108910895, w1=2.4837982986028337\n",
      "SubGD iter. 87/499: loss=217.3573889641131, w0=59.382178217821824, w1=2.7260245553531504\n",
      "SubGD iter. 88/499: loss=208.28322256993167, w0=59.83960396039608, w1=2.978742333469136\n",
      "SubGD iter. 89/499: loss=199.60239149197503, w0=60.262376237623805, w1=3.251528669355438\n",
      "SubGD iter. 90/499: loss=191.51606823720067, w0=60.67821782178222, w1=3.5270865794242794\n",
      "SubGD iter. 91/499: loss=183.75485658516766, w0=61.087128712871326, w1=3.806459183951815\n",
      "SubGD iter. 92/499: loss=176.30486084041354, w0=61.49603960396043, w1=4.085831788479351\n",
      "SubGD iter. 93/499: loss=169.10012226467114, w0=61.891089108910926, w1=4.373839384328607\n",
      "SubGD iter. 94/499: loss=162.25177552382956, w0=62.27920792079211, w1=4.666037469532047\n",
      "SubGD iter. 95/499: loss=155.69742299714352, w0=62.65346534653469, w1=4.959829093241769\n",
      "SubGD iter. 96/499: loss=149.52752679496214, w0=63.02079207920796, w1=5.25705719205664\n",
      "SubGD iter. 97/499: loss=143.6406908762162, w0=63.38118811881192, w1=5.560434316352406\n",
      "SubGD iter. 98/499: loss=138.01748857628561, w0=63.74158415841588, w1=5.863811440648173\n",
      "SubGD iter. 99/499: loss=132.61620926126318, w0=64.08811881188123, w1=6.172402175278548\n",
      "SubGD iter. 100/499: loss=127.5497244247717, w0=64.42772277227726, w1=6.486369310516498\n",
      "SubGD iter. 101/499: loss=122.74087338718583, w0=64.7673267326733, w1=6.800336445754448\n",
      "SubGD iter. 102/499: loss=118.14592856152613, w0=65.10693069306933, w1=7.114303580992399\n",
      "SubGD iter. 103/499: loss=113.76488994779264, w0=65.44653465346536, w1=7.428270716230349\n",
      "SubGD iter. 104/499: loss=109.59775754598533, w0=65.76534653465349, w1=7.747893210218626\n",
      "SubGD iter. 105/499: loss=105.79833542751531, w0=66.070297029703, w1=8.073669686866905\n",
      "SubGD iter. 106/499: loss=102.29523108809987, w0=66.37524752475251, w1=8.399446163515185\n",
      "SubGD iter. 107/499: loss=98.99125186585272, w0=66.6663366336634, w1=8.73297028041739\n",
      "SubGD iter. 108/499: loss=95.97103181808463, w0=66.9574257425743, w1=9.066494397319596\n",
      "SubGD iter. 109/499: loss=93.14678297619844, w0=67.23465346534658, w1=9.39863031947029\n",
      "SubGD iter. 110/499: loss=90.61539672531055, w0=67.51188118811886, w1=9.730766241620982\n",
      "SubGD iter. 111/499: loss=88.27117995547904, w0=67.78910891089114, w1=10.062902163771675\n",
      "SubGD iter. 112/499: loss=86.11413266670397, w0=68.06633663366343, w1=10.363999289979422\n",
      "SubGD iter. 113/499: loss=84.16459694644126, w0=68.32970297029709, w1=10.660466909273612\n",
      "SubGD iter. 114/499: loss=82.46374060728382, w0=68.59306930693076, w1=10.943174379960814\n",
      "SubGD iter. 115/499: loss=80.92130656136145, w0=68.85643564356442, w1=11.225881850648015\n",
      "SubGD iter. 116/499: loss=79.52815785669323, w0=69.11287128712878, w1=11.504395843582206\n",
      "SubGD iter. 117/499: loss=78.3166339721615, w0=69.35544554455453, w1=11.78820189306775\n",
      "SubGD iter. 118/499: loss=77.31763568851028, w0=69.58415841584166, w1=12.060911465190971\n",
      "SubGD iter. 119/499: loss=76.5086323125277, w0=69.80594059405948, w1=12.324245668386048\n",
      "SubGD iter. 120/499: loss=75.84369059931619, w0=70.0277227722773, w1=12.587579871581125\n",
      "SubGD iter. 121/499: loss=75.2972811232521, w0=70.25643564356443, w1=12.824765405096484\n",
      "SubGD iter. 122/499: loss=74.79581982001422, w0=70.47821782178225, w1=13.065616959310148\n",
      "SubGD iter. 123/499: loss=74.43521733660019, w0=70.69306930693077, w1=13.302953389983912\n",
      "SubGD iter. 124/499: loss=74.19719822092475, w0=70.89405940594067, w1=13.525403099312918\n",
      "SubGD iter. 125/499: loss=74.0683789939549, w0=71.08811881188126, w1=13.742945617944212\n",
      "SubGD iter. 126/499: loss=74.03676697743113, w0=71.27524752475254, w1=13.953548196006844\n",
      "SubGD iter. 127/499: loss=74.0891897467268, w0=71.46237623762383, w1=14.164150774069476\n",
      "SubGD iter. 128/499: loss=74.22098311708994, w0=71.62178217821788, w1=14.349779559473173\n",
      "SubGD iter. 129/499: loss=74.41647628166011, w0=71.75346534653471, w1=14.51689010761231\n",
      "SubGD iter. 130/499: loss=74.67096152833797, w0=71.87128712871292, w1=14.670791185324186\n",
      "SubGD iter. 131/499: loss=74.95294838238374, w0=71.95445544554461, w1=14.780276456654521\n",
      "SubGD iter. 132/499: loss=75.17779670886809, w0=72.0376237623763, w1=14.889761727984856\n",
      "SubGD iter. 133/499: loss=75.42154902891534, w0=72.10693069306937, w1=14.985916181776727\n",
      "SubGD iter. 134/499: loss=75.65853052170131, w0=72.17623762376245, w1=15.082070635568597\n",
      "SubGD iter. 135/499: loss=75.90956114411335, w0=72.24554455445552, w1=15.178225089360467\n",
      "SubGD iter. 136/499: loss=76.17464089615152, w0=72.30099009900998, w1=15.25972348971591\n",
      "SubGD iter. 137/499: loss=76.41613751021124, w0=72.34950495049513, w1=15.335091856448138\n",
      "SubGD iter. 138/499: loss=76.65285618006445, w0=72.39801980198028, w1=15.410460223180365\n",
      "SubGD iter. 139/499: loss=76.89760893143615, w0=72.43267326732682, w1=15.469961786755725\n",
      "SubGD iter. 140/499: loss=77.10246868795754, w0=72.46039603960405, w1=15.51864528583281\n",
      "SubGD iter. 141/499: loss=77.27462217352495, w0=72.48811881188128, w1=15.561592159086487\n",
      "SubGD iter. 142/499: loss=77.42392987125324, w0=72.5019801980199, w1=15.597828332032526\n",
      "SubGD iter. 143/499: loss=77.56681600428621, w0=72.52277227722782, w1=15.624722856626713\n",
      "SubGD iter. 144/499: loss=77.65755497253161, w0=72.55049504950505, w1=15.642690329098\n",
      "SubGD iter. 145/499: loss=77.697735657651, w0=72.56435643564366, w1=15.664356578291091\n",
      "SubGD iter. 146/499: loss=77.77686805360916, w0=72.58514851485158, w1=15.677095775361284\n",
      "SubGD iter. 147/499: loss=77.80488113813013, w0=72.6059405940595, w1=15.689834972431477\n",
      "SubGD iter. 148/499: loss=77.83348882035091, w0=72.62673267326743, w1=15.70257416950167\n",
      "SubGD iter. 149/499: loss=77.86269110027146, w0=72.64059405940604, w1=15.72424041869476\n",
      "SubGD iter. 150/499: loss=77.9441777135797, w0=72.66138613861396, w1=15.736979615764954\n",
      "SubGD iter. 151/499: loss=77.97453880885682, w0=72.66831683168327, w1=15.74811029423128\n",
      "SubGD iter. 152/499: loss=78.01721470220238, w0=72.67524752475258, w1=15.759240972697606\n",
      "SubGD iter. 153/499: loss=78.06006252205749, w0=72.68217821782189, w1=15.770371651163932\n",
      "SubGD iter. 154/499: loss=78.10308226842213, w0=72.68217821782189, w1=15.774323911906686\n",
      "SubGD iter. 155/499: loss=78.12180591760087, w0=72.68217821782189, w1=15.77827617264944\n",
      "SubGD iter. 156/499: loss=78.14054518714461, w0=72.68217821782189, w1=15.782228433392193\n",
      "SubGD iter. 157/499: loss=78.15930007705333, w0=72.68217821782189, w1=15.786180694134947\n",
      "SubGD iter. 158/499: loss=78.17807058732701, w0=72.68217821782189, w1=15.7901329548777\n",
      "SubGD iter. 159/499: loss=78.19685671796569, w0=72.68217821782189, w1=15.794085215620454\n",
      "SubGD iter. 160/499: loss=78.21565846896934, w0=72.68217821782189, w1=15.798037476363207\n",
      "SubGD iter. 161/499: loss=78.23447584033798, w0=72.68217821782189, w1=15.801989737105961\n",
      "SubGD iter. 162/499: loss=78.25330883207158, w0=72.68217821782189, w1=15.805941997848715\n",
      "SubGD iter. 163/499: loss=78.27215744417015, w0=72.68217821782189, w1=15.809894258591468\n",
      "SubGD iter. 164/499: loss=78.29102167663372, w0=72.68217821782189, w1=15.813846519334222\n",
      "SubGD iter. 165/499: loss=78.30990152946228, w0=72.68217821782189, w1=15.817798780076975\n",
      "SubGD iter. 166/499: loss=78.32879700265579, w0=72.68217821782189, w1=15.821751040819729\n",
      "SubGD iter. 167/499: loss=78.3477080962143, w0=72.68217821782189, w1=15.825703301562482\n",
      "SubGD iter. 168/499: loss=78.36663481013778, w0=72.68217821782189, w1=15.829655562305236\n",
      "SubGD iter. 169/499: loss=78.38557714442624, w0=72.68217821782189, w1=15.83360782304799\n",
      "SubGD iter. 170/499: loss=78.40453509907967, w0=72.68217821782189, w1=15.837560083790743\n",
      "SubGD iter. 171/499: loss=78.42350867409807, w0=72.68217821782189, w1=15.841512344533497\n",
      "SubGD iter. 172/499: loss=78.44249786948147, w0=72.68217821782189, w1=15.84546460527625\n",
      "SubGD iter. 173/499: loss=78.46150268522986, w0=72.68217821782189, w1=15.849416866019004\n",
      "SubGD iter. 174/499: loss=78.4805231213432, w0=72.68217821782189, w1=15.853369126761757\n",
      "SubGD iter. 175/499: loss=78.49955917782155, w0=72.68217821782189, w1=15.857321387504511\n",
      "SubGD iter. 176/499: loss=78.51861085466484, w0=72.68217821782189, w1=15.861273648247264\n",
      "SubGD iter. 177/499: loss=78.53767815187312, w0=72.68217821782189, w1=15.865225908990018\n",
      "SubGD iter. 178/499: loss=78.55676106944641, w0=72.68217821782189, w1=15.869178169732772\n",
      "SubGD iter. 179/499: loss=78.57585960738464, w0=72.68217821782189, w1=15.873130430475525\n",
      "SubGD iter. 180/499: loss=78.59497376568788, w0=72.68217821782189, w1=15.877082691218279\n",
      "SubGD iter. 181/499: loss=78.61410354435608, w0=72.68217821782189, w1=15.881034951961032\n",
      "SubGD iter. 182/499: loss=78.63324894338926, w0=72.68217821782189, w1=15.884987212703786\n",
      "SubGD iter. 183/499: loss=78.65240996278742, w0=72.68217821782189, w1=15.88893947344654\n",
      "SubGD iter. 184/499: loss=78.67158660255055, w0=72.68217821782189, w1=15.892891734189293\n",
      "SubGD iter. 185/499: loss=78.69077886267868, w0=72.68217821782189, w1=15.896843994932047\n",
      "SubGD iter. 186/499: loss=78.70998674317178, w0=72.68217821782189, w1=15.9007962556748\n",
      "SubGD iter. 187/499: loss=78.72921024402984, w0=72.68217821782189, w1=15.904748516417554\n",
      "SubGD iter. 188/499: loss=78.7484493652529, w0=72.68217821782189, w1=15.908700777160307\n",
      "SubGD iter. 189/499: loss=78.76770410684094, w0=72.68217821782189, w1=15.91265303790306\n",
      "SubGD iter. 190/499: loss=78.78697446879394, w0=72.67524752475258, w1=15.910526938117323\n",
      "SubGD iter. 191/499: loss=78.78623350545423, w0=72.67524752475258, w1=15.914479198860077\n",
      "SubGD iter. 192/499: loss=78.80551108487151, w0=72.67524752475258, w1=15.91843145960283\n",
      "SubGD iter. 193/499: loss=78.82480428465377, w0=72.66831683168327, w1=15.916305359817093\n",
      "SubGD iter. 194/499: loss=78.82409907031933, w0=72.66831683168327, w1=15.920257620559847\n",
      "SubGD iter. 195/499: loss=78.84339948756585, w0=72.66831683168327, w1=15.9242098813026\n",
      "SubGD iter. 196/499: loss=78.86271552517734, w0=72.66831683168327, w1=15.928162142045354\n",
      "SubGD iter. 197/499: loss=78.88204718315382, w0=72.66138613861396, w1=15.926036042259616\n",
      "SubGD iter. 198/499: loss=78.88136931492394, w0=72.66138613861396, w1=15.92998830300237\n",
      "SubGD iter. 199/499: loss=78.90070819036468, w0=72.66138613861396, w1=15.933940563745123\n",
      "SubGD iter. 200/499: loss=78.9200626861704, w0=72.65445544554466, w1=15.931814463959386\n",
      "SubGD iter. 201/499: loss=78.91942056694583, w0=72.65445544554466, w1=15.93576672470214\n",
      "SubGD iter. 202/499: loss=78.93878228021579, w0=72.65445544554466, w1=15.939718985444893\n",
      "SubGD iter. 203/499: loss=78.95815961385074, w0=72.65445544554466, w1=15.943671246187646\n",
      "SubGD iter. 204/499: loss=78.97755256785065, w0=72.64752475247535, w1=15.941545146401909\n",
      "SubGD iter. 205/499: loss=78.97693779473066, w0=72.64752475247535, w1=15.945497407144662\n",
      "SubGD iter. 206/499: loss=78.99633796619483, w0=72.64752475247535, w1=15.949449667887416\n",
      "SubGD iter. 207/499: loss=79.01575375802399, w0=72.64059405940604, w1=15.947323568101679\n",
      "SubGD iter. 208/499: loss=79.01517473390928, w0=72.64059405940604, w1=15.951275828844432\n",
      "SubGD iter. 209/499: loss=79.03459774320271, w0=72.64059405940604, w1=15.955228089587186\n",
      "SubGD iter. 210/499: loss=79.0540363728611, w0=72.64059405940604, w1=15.95918035032994\n",
      "SubGD iter. 211/499: loss=79.07349062288448, w0=72.63366336633673, w1=15.957054250544202\n",
      "SubGD iter. 212/499: loss=79.07293894487434, w0=72.63366336633673, w1=15.961006511286955\n",
      "SubGD iter. 213/499: loss=79.09240041236197, w0=72.63366336633673, w1=15.964958772029709\n",
      "SubGD iter. 214/499: loss=79.11187750021458, w0=72.62673267326743, w1=15.962832672243971\n",
      "SubGD iter. 215/499: loss=79.11136157120973, w0=72.63366336633673, w1=15.967301372051375\n",
      "SubGD iter. 216/499: loss=79.12342941191513, w0=72.62673267326743, w1=15.965175272265638\n",
      "SubGD iter. 217/499: loss=79.12290850230883, w0=72.63366336633673, w1=15.969643972073042\n",
      "SubGD iter. 218/499: loss=79.13498681139052, w0=72.62673267326743, w1=15.967517872287305\n",
      "SubGD iter. 219/499: loss=79.13446092118284, w0=72.63366336633673, w1=15.971986572094709\n",
      "SubGD iter. 220/499: loss=79.14654969864078, w0=72.62673267326743, w1=15.969860472308971\n",
      "SubGD iter. 221/499: loss=79.1460188278317, w0=72.63366336633673, w1=15.974329172116375\n",
      "SubGD iter. 222/499: loss=79.1581180736659, w0=72.62673267326743, w1=15.972203072330638\n",
      "SubGD iter. 223/499: loss=79.15758222225541, w0=72.62673267326743, w1=15.970593411609551\n",
      "SubGD iter. 224/499: loss=79.14963612667158, w0=72.63366336633673, w1=15.975062111416955\n",
      "SubGD iter. 225/499: loss=79.16173864779152, w0=72.62673267326743, w1=15.972936011631218\n",
      "SubGD iter. 226/499: loss=79.16120123807895, w0=72.62673267326743, w1=15.97132635091013\n",
      "SubGD iter. 227/499: loss=79.1532539627115, w0=72.63366336633673, w1=15.975795050717535\n",
      "SubGD iter. 228/499: loss=79.16535975911715, w0=72.62673267326743, w1=15.973668950931797\n",
      "SubGD iter. 229/499: loss=79.16482079110246, w0=72.62673267326743, w1=15.97205929021071\n",
      "SubGD iter. 230/499: loss=79.15687233595143, w0=72.62673267326743, w1=15.970449629489623\n",
      "SubGD iter. 231/499: loss=79.148926471808, w0=72.63366336633673, w1=15.974918329297028\n",
      "SubGD iter. 232/499: loss=79.16102835040881, w0=72.62673267326743, w1=15.97279222951129\n",
      "SubGD iter. 233/499: loss=79.16049124639137, w0=72.62673267326743, w1=15.971182568790203\n",
      "SubGD iter. 234/499: loss=79.15254420246437, w0=72.63366336633673, w1=15.975651268597607\n",
      "SubGD iter. 235/499: loss=79.16464935635088, w0=72.62673267326743, w1=15.97352516881187\n",
      "SubGD iter. 236/499: loss=79.16411069403134, w0=72.62673267326743, w1=15.971915508090783\n",
      "SubGD iter. 237/499: loss=79.15616247032072, w0=72.63366336633673, w1=15.976384207898187\n",
      "SubGD iter. 238/499: loss=79.16827089949295, w0=72.62673267326743, w1=15.97425810811245\n",
      "SubGD iter. 239/499: loss=79.16773067887131, w0=72.62673267326743, w1=15.972648447391363\n",
      "SubGD iter. 240/499: loss=79.15978127537711, w0=72.62673267326743, w1=15.971038786670276\n",
      "SubGD iter. 241/499: loss=79.15183446289053, w0=72.63366336633673, w1=15.97550748647768\n",
      "SubGD iter. 242/499: loss=79.16393897425792, w0=72.62673267326743, w1=15.973381386691942\n",
      "SubGD iter. 243/499: loss=79.16340061763351, w0=72.62673267326743, w1=15.971771725970855\n",
      "SubGD iter. 244/499: loss=79.15545262536332, w0=72.63366336633673, w1=15.97624042577826\n",
      "SubGD iter. 245/499: loss=79.16756041201641, w0=72.62673267326743, w1=15.974114325992522\n",
      "SubGD iter. 246/499: loss=79.16702049708994, w0=72.62673267326743, w1=15.972504665271435\n",
      "SubGD iter. 247/499: loss=79.15907132503614, w0=72.62673267326743, w1=15.970895004550348\n",
      "SubGD iter. 248/499: loss=79.15112474399001, w0=72.63366336633673, w1=15.975363704357752\n",
      "SubGD iter. 249/499: loss=79.16322861283825, w0=72.62673267326743, w1=15.973237604572015\n",
      "SubGD iter. 250/499: loss=79.16269056190899, w0=72.62673267326743, w1=15.971627943850928\n",
      "SubGD iter. 251/499: loss=79.15474280107924, w0=72.63366336633673, w1=15.976096643658332\n",
      "SubGD iter. 252/499: loss=79.16684994521319, w0=72.62673267326743, w1=15.973970543872595\n",
      "SubGD iter. 253/499: loss=79.16631033598182, w0=72.62673267326743, w1=15.972360883151508\n",
      "SubGD iter. 254/499: loss=79.15836139536849, w0=72.62673267326743, w1=15.97075122243042\n",
      "SubGD iter. 255/499: loss=79.15041504576276, w0=72.63366336633673, w1=15.975219922237825\n",
      "SubGD iter. 256/499: loss=79.16251827209189, w0=72.62673267326743, w1=15.973093822452087\n",
      "SubGD iter. 257/499: loss=79.16198052685773, w0=72.62673267326743, w1=15.971484161731\n",
      "SubGD iter. 258/499: loss=79.15403299746843, w0=72.63366336633673, w1=15.975952861538405\n",
      "SubGD iter. 259/499: loss=79.16613949908324, w0=72.62673267326743, w1=15.973826761752667\n",
      "SubGD iter. 260/499: loss=79.16560019554703, w0=72.62673267326743, w1=15.97221710103158\n",
      "SubGD iter. 261/499: loss=79.15765148637412, w0=72.62673267326743, w1=15.970607440310493\n",
      "SubGD iter. 262/499: loss=79.14970536820883, w0=72.63366336633673, w1=15.975076140117897\n",
      "SubGD iter. 263/499: loss=79.1618079520188, w0=72.62673267326743, w1=15.97295004033216\n",
      "SubGD iter. 264/499: loss=79.1612705124798, w0=72.62673267326743, w1=15.971340379611073\n",
      "SubGD iter. 265/499: loss=79.15332321453093, w0=72.63366336633673, w1=15.975809079418477\n",
      "SubGD iter. 266/499: loss=79.16542907362661, w0=72.62673267326743, w1=15.97368297963274\n",
      "SubGD iter. 267/499: loss=79.16489007578552, w0=72.62673267326743, w1=15.972073318911653\n",
      "SubGD iter. 268/499: loss=79.15694159805304, w0=72.62673267326743, w1=15.970463658190566\n",
      "SubGD iter. 269/499: loss=79.1489957113282, w0=72.63366336633673, w1=15.97493235799797\n",
      "SubGD iter. 270/499: loss=79.16109765261903, w0=72.62673267326743, w1=15.972806258212232\n",
      "SubGD iter. 271/499: loss=79.16056051877517, w0=72.62673267326743, w1=15.971196597491145\n",
      "SubGD iter. 272/499: loss=79.15261345226672, w0=72.63366336633673, w1=15.97566529729855\n",
      "SubGD iter. 273/499: loss=79.16471866884326, w0=72.62673267326743, w1=15.973539197512812\n",
      "SubGD iter. 274/499: loss=79.16417997669731, w0=72.62673267326743, w1=15.971929536791725\n",
      "SubGD iter. 275/499: loss=79.15623173040525, w0=72.63366336633673, w1=15.97639823659913\n",
      "SubGD iter. 276/499: loss=79.16834022226753, w0=72.62673267326743, w1=15.974272136813392\n",
      "SubGD iter. 277/499: loss=79.16779997181949, w0=72.62673267326743, w1=15.972662476092305\n",
      "SubGD iter. 278/499: loss=79.15985054574385, w0=72.62673267326743, w1=15.971052815371218\n",
      "SubGD iter. 279/499: loss=79.15190371067581, w0=72.63366336633673, w1=15.975521515178622\n",
      "SubGD iter. 280/499: loss=79.16400828473323, w0=72.62673267326743, w1=15.973395415392885\n",
      "SubGD iter. 281/499: loss=79.16346989828241, w0=72.62673267326743, w1=15.971785754671798\n",
      "SubGD iter. 282/499: loss=79.1555218834308, w0=72.63366336633673, w1=15.976254454479202\n",
      "SubGD iter. 283/499: loss=79.16762973277392, w0=72.62673267326743, w1=15.974128354693464\n",
      "SubGD iter. 284/499: loss=79.16708978802102, w0=72.62673267326743, w1=15.972518693972377\n",
      "SubGD iter. 285/499: loss=79.15914059338579, w0=72.62673267326743, w1=15.97090903325129\n",
      "SubGD iter. 286/499: loss=79.1511939897582, w0=72.63366336633673, w1=15.975377733058695\n",
      "SubGD iter. 287/499: loss=79.16329792129649, w0=72.62673267326743, w1=15.973251633272957\n",
      "SubGD iter. 288/499: loss=79.1627598405408, w0=72.62673267326743, w1=15.97164197255187\n",
      "SubGD iter. 289/499: loss=79.15481205712962, w0=72.63366336633673, w1=15.976110672359274\n",
      "SubGD iter. 290/499: loss=79.16691926395362, w0=72.62673267326743, w1=15.973984572573537\n",
      "SubGD iter. 291/499: loss=79.16637962489584, w0=72.62673267326743, w1=15.97237491185245\n",
      "SubGD iter. 292/499: loss=79.15843066170103, w0=72.62673267326743, w1=15.970765251131363\n",
      "SubGD iter. 293/499: loss=79.1504842895139, w0=72.63366336633673, w1=15.975233950938767\n",
      "SubGD iter. 294/499: loss=79.16258757853304, w0=72.62673267326743, w1=15.97310785115303\n",
      "SubGD iter. 295/499: loss=79.16204980347248, w0=72.62673267326743, w1=15.971498190431943\n",
      "SubGD iter. 296/499: loss=79.15410225150173, w0=72.63366336633673, w1=15.975966890239347\n",
      "SubGD iter. 297/499: loss=79.1662088158066, w0=72.62673267326743, w1=15.97384079045361\n",
      "SubGD iter. 298/499: loss=79.16566948244396, w0=72.62673267326743, w1=15.972231129732522\n",
      "SubGD iter. 299/499: loss=79.15772075068959, w0=72.62673267326743, w1=15.970621469011435\n",
      "SubGD iter. 300/499: loss=79.14977460994288, w0=72.63366336633673, w1=15.97509016881884\n",
      "SubGD iter. 301/499: loss=79.1618772564429, w0=72.62673267326743, w1=15.972964069033102\n",
      "SubGD iter. 302/499: loss=79.16133978707748, w0=72.62673267326743, w1=15.971354408312015\n",
      "SubGD iter. 303/499: loss=79.15339246654715, w0=72.63366336633673, w1=15.97582310811942\n",
      "SubGD iter. 304/499: loss=79.16549838833289, w0=72.62673267326743, w1=15.973697008333682\n",
      "SubGD iter. 305/499: loss=79.16495936066538, w0=72.62673267326743, w1=15.972087347612595\n",
      "SubGD iter. 306/499: loss=79.15701086035145, w0=72.62673267326743, w1=15.970477686891508\n",
      "SubGD iter. 307/499: loss=79.14906495104516, w0=72.63366336633673, w1=15.974946386698912\n",
      "SubGD iter. 308/499: loss=79.16116695502606, w0=72.62673267326743, w1=15.972820286913175\n",
      "SubGD iter. 309/499: loss=79.16062979135577, w0=72.62673267326743, w1=15.971210626192088\n",
      "SubGD iter. 310/499: loss=79.15268270226588, w0=72.63366336633673, w1=15.975679325999492\n",
      "SubGD iter. 311/499: loss=79.16478798153247, w0=72.62673267326743, w1=15.973553226213754\n",
      "SubGD iter. 312/499: loss=79.1642492595601, w0=72.62673267326743, w1=15.971943565492667\n",
      "SubGD iter. 313/499: loss=79.1563009906866, w0=72.63366336633673, w1=15.976412265300072\n",
      "SubGD iter. 314/499: loss=79.16840954523893, w0=72.62673267326743, w1=15.974286165514334\n",
      "SubGD iter. 315/499: loss=79.16786926496447, w0=72.62673267326743, w1=15.972676504793247\n",
      "SubGD iter. 316/499: loss=79.15991981630737, w0=72.62673267326743, w1=15.97106684407216\n",
      "SubGD iter. 317/499: loss=79.15197295865788, w0=72.63366336633673, w1=15.975535543879564\n",
      "SubGD iter. 318/499: loss=79.16407759540537, w0=72.62673267326743, w1=15.973409444093827\n",
      "SubGD iter. 319/499: loss=79.16353917912812, w0=72.62673267326743, w1=15.97179978337274\n",
      "SubGD iter. 320/499: loss=79.15559114169504, w0=72.63366336633673, w1=15.976268483180144\n",
      "SubGD iter. 321/499: loss=79.16769905372824, w0=72.62673267326743, w1=15.974142383394407\n",
      "SubGD iter. 322/499: loss=79.1671590791489, w0=72.62673267326743, w1=15.97253272267332\n",
      "SubGD iter. 323/499: loss=79.15920986193224, w0=72.62673267326743, w1=15.970923061952233\n",
      "SubGD iter. 324/499: loss=79.15126323572319, w0=72.63366336633673, w1=15.975391761759637\n",
      "SubGD iter. 325/499: loss=79.16336722995155, w0=72.62673267326743, w1=15.9732656619739\n",
      "SubGD iter. 326/499: loss=79.16282911936945, w0=72.62673267326743, w1=15.971656001252812\n",
      "SubGD iter. 327/499: loss=79.15488131337679, w0=72.63366336633673, w1=15.976124701060217\n",
      "SubGD iter. 328/499: loss=79.16698858289087, w0=72.62673267326743, w1=15.973998601274479\n",
      "SubGD iter. 329/499: loss=79.16644891400666, w0=72.62673267326743, w1=15.972388940553392\n",
      "SubGD iter. 330/499: loss=79.1584999282304, w0=72.62673267326743, w1=15.970779279832305\n",
      "SubGD iter. 331/499: loss=79.15055353346182, w0=72.63366336633673, w1=15.97524797963971\n",
      "SubGD iter. 332/499: loss=79.16265688517102, w0=72.62673267326743, w1=15.973121879853972\n",
      "SubGD iter. 333/499: loss=79.16211908028406, w0=72.62673267326743, w1=15.971512219132885\n",
      "SubGD iter. 334/499: loss=79.15417150573185, w0=72.63366336633673, w1=15.975980918940289\n",
      "SubGD iter. 335/499: loss=79.16627813272679, w0=72.62673267326743, w1=15.973854819154552\n",
      "SubGD iter. 336/499: loss=79.16573876953771, w0=72.62673267326743, w1=15.972245158433465\n",
      "SubGD iter. 337/499: loss=79.15779001520188, w0=72.62673267326743, w1=15.970635497712378\n",
      "SubGD iter. 338/499: loss=79.14984385187373, w0=72.63366336633673, w1=15.975104197519782\n",
      "SubGD iter. 339/499: loss=79.1619465610638, w0=72.62673267326743, w1=15.972978097734044\n",
      "SubGD iter. 340/499: loss=79.16140906187198, w0=72.62673267326743, w1=15.971368437012957\n",
      "SubGD iter. 341/499: loss=79.1534617187602, w0=72.63366336633673, w1=15.975837136820362\n",
      "SubGD iter. 342/499: loss=79.16556770323598, w0=72.62673267326743, w1=15.973711037034624\n",
      "SubGD iter. 343/499: loss=79.16502864574204, w0=72.62673267326743, w1=15.972101376313537\n",
      "SubGD iter. 344/499: loss=79.15708012284666, w0=72.62673267326743, w1=15.97049171559245\n",
      "SubGD iter. 345/499: loss=79.14913419095892, w0=72.63366336633673, w1=15.974960415399854\n",
      "SubGD iter. 346/499: loss=79.1612362576299, w0=72.62673267326743, w1=15.972834315614117\n",
      "SubGD iter. 347/499: loss=79.16069906413318, w0=72.62673267326743, w1=15.97122465489303\n",
      "SubGD iter. 348/499: loss=79.15275195246183, w0=72.63366336633673, w1=15.975693354700434\n",
      "SubGD iter. 349/499: loss=79.16485729441851, w0=72.62673267326743, w1=15.973567254914697\n",
      "SubGD iter. 350/499: loss=79.16431854261971, w0=72.62673267326743, w1=15.97195759419361\n",
      "SubGD iter. 351/499: loss=79.15637025116476, w0=72.63366336633673, w1=15.976426294001014\n",
      "SubGD iter. 352/499: loss=79.16847886840712, w0=72.62673267326743, w1=15.974300194215276\n",
      "SubGD iter. 353/499: loss=79.16793855830623, w0=72.62673267326743, w1=15.97269053349419\n",
      "SubGD iter. 354/499: loss=79.15998908706769, w0=72.62673267326743, w1=15.971080872773102\n",
      "SubGD iter. 355/499: loss=79.15204220683677, w0=72.63366336633673, w1=15.975549572580507\n",
      "SubGD iter. 356/499: loss=79.16414690627431, w0=72.62673267326743, w1=15.973423472794769\n",
      "SubGD iter. 357/499: loss=79.16360846017065, w0=72.62673267326743, w1=15.971813812073682\n",
      "SubGD iter. 358/499: loss=79.15566040015614, w0=72.63366336633673, w1=15.976282511881086\n",
      "SubGD iter. 359/499: loss=79.16776837487937, w0=72.62673267326743, w1=15.974156412095349\n",
      "SubGD iter. 360/499: loss=79.16722837047362, w0=72.62673267326743, w1=15.972546751374262\n",
      "SubGD iter. 361/499: loss=79.15927913067549, w0=72.62673267326743, w1=15.970937090653175\n",
      "SubGD iter. 362/499: loss=79.15133248188499, w0=72.63366336633673, w1=15.975405790460579\n",
      "SubGD iter. 363/499: loss=79.1634365388034, w0=72.62673267326743, w1=15.973279690674842\n",
      "SubGD iter. 364/499: loss=79.16289839839487, w0=72.62673267326743, w1=15.971670029953755\n",
      "SubGD iter. 365/499: loss=79.1549505698208, w0=72.63366336633673, w1=15.976138729761159\n",
      "SubGD iter. 366/499: loss=79.16705790202491, w0=72.62673267326743, w1=15.974012629975421\n",
      "SubGD iter. 367/499: loss=79.1665182033143, w0=72.62673267326743, w1=15.972402969254334\n",
      "SubGD iter. 368/499: loss=79.15856919495658, w0=72.62673267326743, w1=15.970793308533247\n",
      "SubGD iter. 369/499: loss=79.15062277760654, w0=72.63366336633673, w1=15.975262008340652\n",
      "SubGD iter. 370/499: loss=79.1627261920058, w0=72.62673267326743, w1=15.973135908554914\n",
      "SubGD iter. 371/499: loss=79.16218835729242, w0=72.62673267326743, w1=15.971526247833827\n",
      "SubGD iter. 372/499: loss=79.15424076015876, w0=72.63366336633673, w1=15.975994947641231\n",
      "SubGD iter. 373/499: loss=79.16634744984376, w0=72.62673267326743, w1=15.973868847855494\n",
      "SubGD iter. 374/499: loss=79.16580805682827, w0=72.62673267326743, w1=15.972259187134407\n",
      "SubGD iter. 375/499: loss=79.15785927991101, w0=72.62673267326743, w1=15.97064952641332\n",
      "SubGD iter. 376/499: loss=79.14991309400138, w0=72.63366336633673, w1=15.975118226220724\n",
      "SubGD iter. 377/499: loss=79.16201586588151, w0=72.62673267326743, w1=15.972992126434987\n",
      "SubGD iter. 378/499: loss=79.16147833686327, w0=72.62673267326743, w1=15.9713824657139\n",
      "SubGD iter. 379/499: loss=79.15353097117004, w0=72.63366336633673, w1=15.975851165521304\n",
      "SubGD iter. 380/499: loss=79.16563701833589, w0=72.62673267326743, w1=15.973725065735566\n",
      "SubGD iter. 381/499: loss=79.16509793101554, w0=72.62673267326743, w1=15.97211540501448\n",
      "SubGD iter. 382/499: loss=79.1571493855387, w0=72.62673267326743, w1=15.970505744293392\n",
      "SubGD iter. 383/499: loss=79.1492034310695, w0=72.63366336633673, w1=15.974974444100797\n",
      "SubGD iter. 384/499: loss=79.16130556043052, w0=72.62673267326743, w1=15.972848344315059\n",
      "SubGD iter. 385/499: loss=79.1607683371074, w0=72.62673267326743, w1=15.971238683593972\n",
      "SubGD iter. 386/499: loss=79.1528212028546, w0=72.63366336633673, w1=15.975707383401376\n",
      "SubGD iter. 387/499: loss=79.1649266075013, w0=72.62673267326743, w1=15.973581283615639\n",
      "SubGD iter. 388/499: loss=79.1643878258761, w0=72.62673267326743, w1=15.971971622894552\n",
      "SubGD iter. 389/499: loss=79.1564395118397, w0=72.63366336633673, w1=15.976440322701956\n",
      "SubGD iter. 390/499: loss=79.16854819177212, w0=72.62673267326743, w1=15.974314222916218\n",
      "SubGD iter. 391/499: loss=79.16800785184482, w0=72.62673267326743, w1=15.972704562195132\n",
      "SubGD iter. 392/499: loss=79.16005835802483, w0=72.62673267326743, w1=15.971094901474045\n",
      "SubGD iter. 393/499: loss=79.15211145521245, w0=72.63366336633673, w1=15.975563601281449\n",
      "SubGD iter. 394/499: loss=79.16421621734004, w0=72.62673267326743, w1=15.973437501495711\n",
      "SubGD iter. 395/499: loss=79.16367774140997, w0=72.62673267326743, w1=15.971827840774624\n",
      "SubGD iter. 396/499: loss=79.155729658814, w0=72.63366336633673, w1=15.976296540582029\n",
      "SubGD iter. 397/499: loss=79.1678376962273, w0=72.62673267326743, w1=15.974170440796291\n",
      "SubGD iter. 398/499: loss=79.16729766199512, w0=72.62673267326743, w1=15.972560780075204\n",
      "SubGD iter. 399/499: loss=79.15934839961555, w0=72.62673267326743, w1=15.970951119354117\n",
      "SubGD iter. 400/499: loss=79.15140172824361, w0=72.63366336633673, w1=15.975419819161521\n",
      "SubGD iter. 401/499: loss=79.16350584785206, w0=72.62673267326743, w1=15.973293719375784\n",
      "SubGD iter. 402/499: loss=79.16296767761713, w0=72.62673267326743, w1=15.971684058654697\n",
      "SubGD iter. 403/499: loss=79.1550198264616, w0=72.63366336633673, w1=15.976152758462101\n",
      "SubGD iter. 404/499: loss=79.16712722135576, w0=72.62673267326743, w1=15.974026658676364\n",
      "SubGD iter. 405/499: loss=79.16658749281872, w0=72.62673267326743, w1=15.972416997955277\n",
      "SubGD iter. 406/499: loss=79.15863846187959, w0=72.62673267326743, w1=15.97080733723419\n",
      "SubGD iter. 407/499: loss=79.15069202194807, w0=72.63366336633673, w1=15.975276037041594\n",
      "SubGD iter. 408/499: loss=79.16279549903739, w0=72.62673267326743, w1=15.973149937255856\n",
      "SubGD iter. 409/499: loss=79.16225763449759, w0=72.62673267326743, w1=15.97154027653477\n",
      "SubGD iter. 410/499: loss=79.15431001478248, w0=72.63366336633673, w1=15.976008976342174\n",
      "SubGD iter. 411/499: loss=79.16641676715751, w0=72.62673267326743, w1=15.973882876556436\n",
      "SubGD iter. 412/499: loss=79.16587734431562, w0=72.62673267326743, w1=15.972273215835349\n",
      "SubGD iter. 413/499: loss=79.15792854481691, w0=72.62673267326743, w1=15.970663555114262\n",
      "SubGD iter. 414/499: loss=79.14998233632583, w0=72.63366336633673, w1=15.975132254921666\n",
      "SubGD iter. 415/499: loss=79.16208517089602, w0=72.62673267326743, w1=15.973006155135929\n",
      "SubGD iter. 416/499: loss=79.16154761205135, w0=72.62673267326743, w1=15.971396494414842\n",
      "SubGD iter. 417/499: loss=79.15360022377668, w0=72.63366336633673, w1=15.975865194222246\n",
      "SubGD iter. 418/499: loss=79.16570633363258, w0=72.62673267326743, w1=15.973739094436509\n",
      "SubGD iter. 419/499: loss=79.16516721648581, w0=72.62673267326743, w1=15.972129433715422\n",
      "SubGD iter. 420/499: loss=79.15721864842752, w0=72.62673267326743, w1=15.970519772994335\n",
      "SubGD iter. 421/499: loss=79.14927267137689, w0=72.63366336633673, w1=15.974988472801739\n",
      "SubGD iter. 422/499: loss=79.16137486342794, w0=72.62673267326743, w1=15.972862373016001\n",
      "SubGD iter. 423/499: loss=79.16083761027839, w0=72.62673267326743, w1=15.971252712294914\n",
      "SubGD iter. 424/499: loss=79.15289045344416, w0=72.63366336633673, w1=15.975721412102319\n",
      "SubGD iter. 425/499: loss=79.16499592078094, w0=72.62673267326743, w1=15.973595312316581\n",
      "SubGD iter. 426/499: loss=79.1644571093293, w0=72.62673267326743, w1=15.971985651595494\n",
      "SubGD iter. 427/499: loss=79.15650877271145, w0=72.63366336633673, w1=15.976454351402898\n",
      "SubGD iter. 428/499: loss=79.16861751533393, w0=72.62673267326743, w1=15.97432825161716\n",
      "SubGD iter. 429/499: loss=79.16807714558021, w0=72.62673267326743, w1=15.972718590896074\n",
      "SubGD iter. 430/499: loss=79.16012762917876, w0=72.62673267326743, w1=15.971108930174987\n",
      "SubGD iter. 431/499: loss=79.15218070378494, w0=72.63366336633673, w1=15.975577629982391\n",
      "SubGD iter. 432/499: loss=79.16428552860258, w0=72.62673267326743, w1=15.973451530196654\n",
      "SubGD iter. 433/499: loss=79.16374702284608, w0=72.62673267326743, w1=15.971841869475567\n",
      "SubGD iter. 434/499: loss=79.15579891766868, w0=72.63366336633673, w1=15.97631056928297\n",
      "SubGD iter. 435/499: loss=79.16790701777202, w0=72.62673267326743, w1=15.974184469497233\n",
      "SubGD iter. 436/499: loss=79.16736695371343, w0=72.62673267326743, w1=15.972574808776146\n",
      "SubGD iter. 437/499: loss=79.15941766875243, w0=72.62673267326743, w1=15.97096514805506\n",
      "SubGD iter. 438/499: loss=79.15147097479903, w0=72.63366336633673, w1=15.975433847862464\n",
      "SubGD iter. 439/499: loss=79.16357515709754, w0=72.62673267326743, w1=15.973307748076726\n",
      "SubGD iter. 440/499: loss=79.16303695703617, w0=72.62673267326743, w1=15.97169808735564\n",
      "SubGD iter. 441/499: loss=79.15508908329919, w0=72.63366336633673, w1=15.976166787163043\n",
      "SubGD iter. 442/499: loss=79.16719654088341, w0=72.62673267326743, w1=15.974040687377306\n",
      "SubGD iter. 443/499: loss=79.16665678251996, w0=72.62673267326743, w1=15.972431026656219\n",
      "SubGD iter. 444/499: loss=79.15870772899937, w0=72.62673267326743, w1=15.970821365935132\n",
      "SubGD iter. 445/499: loss=79.15076126648641, w0=72.63366336633673, w1=15.975290065742536\n",
      "SubGD iter. 446/499: loss=79.16286480626579, w0=72.62673267326743, w1=15.973163965956799\n",
      "SubGD iter. 447/499: loss=79.16232691189956, w0=72.62673267326743, w1=15.971554305235712\n",
      "SubGD iter. 448/499: loss=79.154379269603, w0=72.63366336633673, w1=15.976023005043116\n",
      "SubGD iter. 449/499: loss=79.1664860846681, w0=72.62673267326743, w1=15.973896905257378\n",
      "SubGD iter. 450/499: loss=79.16594663199977, w0=72.62673267326743, w1=15.972287244536291\n",
      "SubGD iter. 451/499: loss=79.15799780991962, w0=72.62673267326743, w1=15.970677583815204\n",
      "SubGD iter. 452/499: loss=79.15005157884708, w0=72.63366336633673, w1=15.975146283622609\n",
      "SubGD iter. 453/499: loss=79.16215447610735, w0=72.62673267326743, w1=15.973020183836871\n",
      "SubGD iter. 454/499: loss=79.16161688743625, w0=72.62673267326743, w1=15.971410523115784\n",
      "SubGD iter. 455/499: loss=79.1536694765801, w0=72.63366336633673, w1=15.975879222923188\n",
      "SubGD iter. 456/499: loss=79.16577564912608, w0=72.62673267326743, w1=15.97375312313745\n",
      "SubGD iter. 457/499: loss=79.16523650215288, w0=72.62673267326743, w1=15.972143462416364\n",
      "SubGD iter. 458/499: loss=79.15728791151318, w0=72.62673267326743, w1=15.970533801695277\n",
      "SubGD iter. 459/499: loss=79.14934191188108, w0=72.63366336633673, w1=15.975002501502681\n",
      "SubGD iter. 460/499: loss=79.1614441666222, w0=72.62673267326743, w1=15.972876401716944\n",
      "SubGD iter. 461/499: loss=79.16090688364623, w0=72.62673267326743, w1=15.971266740995857\n",
      "SubGD iter. 462/499: loss=79.15295970423053, w0=72.63366336633673, w1=15.97573544080326\n",
      "SubGD iter. 463/499: loss=79.16506523425736, w0=72.62673267326743, w1=15.973609341017523\n",
      "SubGD iter. 464/499: loss=79.16452639297931, w0=72.62673267326743, w1=15.971999680296436\n",
      "SubGD iter. 465/499: loss=79.15657803378001, w0=72.63366336633673, w1=15.97646838010384\n",
      "SubGD iter. 466/499: loss=79.16868683909254, w0=72.62673267326743, w1=15.974342280318103\n",
      "SubGD iter. 467/499: loss=79.16814643951241, w0=72.62673267326743, w1=15.972732619597016\n",
      "SubGD iter. 468/499: loss=79.1601969005295, w0=72.62673267326743, w1=15.97112295887593\n",
      "SubGD iter. 469/499: loss=79.15224995255426, w0=72.63366336633673, w1=15.975591658683333\n",
      "SubGD iter. 470/499: loss=79.16435484006193, w0=72.62673267326743, w1=15.973465558897596\n",
      "SubGD iter. 471/499: loss=79.16381630447903, w0=72.62673267326743, w1=15.971855898176509\n",
      "SubGD iter. 472/499: loss=79.15586817672015, w0=72.63366336633673, w1=15.976324597983913\n",
      "SubGD iter. 473/499: loss=79.16797633951357, w0=72.62673267326743, w1=15.974198498198175\n",
      "SubGD iter. 474/499: loss=79.16743624562855, w0=72.62673267326743, w1=15.972588837477089\n",
      "SubGD iter. 475/499: loss=79.15948693808609, w0=72.62673267326743, w1=15.970979176756002\n",
      "SubGD iter. 476/499: loss=79.15154022155124, w0=72.63366336633673, w1=15.975447876563406\n",
      "SubGD iter. 477/499: loss=79.16364446653981, w0=72.62673267326743, w1=15.973321776777668\n",
      "SubGD iter. 478/499: loss=79.16310623665203, w0=72.62673267326743, w1=15.971712116056581\n",
      "SubGD iter. 479/499: loss=79.15515834033359, w0=72.63366336633673, w1=15.976180815863986\n",
      "SubGD iter. 480/499: loss=79.16726586060786, w0=72.62673267326743, w1=15.974054716078248\n",
      "SubGD iter. 481/499: loss=79.16672607241802, w0=72.62673267326743, w1=15.972445055357161\n",
      "SubGD iter. 482/499: loss=79.15877699631596, w0=72.62673267326743, w1=15.970835394636074\n",
      "SubGD iter. 483/499: loss=79.15083051122156, w0=72.63366336633673, w1=15.975304094443478\n",
      "SubGD iter. 484/499: loss=79.16293411369098, w0=72.62673267326743, w1=15.97317799465774\n",
      "SubGD iter. 485/499: loss=79.16239618949835, w0=72.62673267326743, w1=15.971568333936654\n",
      "SubGD iter. 486/499: loss=79.15444852462032, w0=72.63366336633673, w1=15.976037033744058\n",
      "SubGD iter. 487/499: loss=79.16655540237548, w0=72.62673267326743, w1=15.97391093395832\n",
      "SubGD iter. 488/499: loss=79.16601591988075, w0=72.62673267326743, w1=15.972301273237234\n",
      "SubGD iter. 489/499: loss=79.15806707521914, w0=72.62673267326743, w1=15.970691612516147\n",
      "SubGD iter. 490/499: loss=79.15012082156517, w0=72.63366336633673, w1=15.97516031232355\n",
      "SubGD iter. 491/499: loss=79.16222378151546, w0=72.62673267326743, w1=15.973034212537813\n",
      "SubGD iter. 492/499: loss=79.16168616301795, w0=72.62673267326743, w1=15.971424551816726\n",
      "SubGD iter. 493/499: loss=79.1537387295804, w0=72.63366336633673, w1=15.97589325162413\n",
      "SubGD iter. 494/499: loss=79.16584496481637, w0=72.62673267326743, w1=15.973767151838393\n",
      "SubGD iter. 495/499: loss=79.16530578801678, w0=72.62673267326743, w1=15.972157491117306\n",
      "SubGD iter. 496/499: loss=79.15735717479562, w0=72.62673267326743, w1=15.97054783039622\n",
      "SubGD iter. 497/499: loss=79.14941115258208, w0=72.63366336633673, w1=15.975016530203623\n",
      "SubGD iter. 498/499: loss=79.16151347001323, w0=72.62673267326743, w1=15.972890430417886\n",
      "SubGD iter. 499/499: loss=79.16097615721087, w0=72.62673267326743, w1=15.971280769696799\n",
      "SubGD: execution time=0.086 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(\n",
    "    y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4240e2c7b574cb7959e4c385bfaa944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses, subgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic SubGradient Descent algorithm (SubSGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "\n",
    "        # ***************************************************\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "            g = compute_subgradient_mae(minibatch_y, minibatch_tx, w)\n",
    "            \n",
    "        loss = compute_loss(y, tx, w)\n",
    "        w = w - gamma * g\n",
    "        \n",
    "        losses.append(loss)\n",
    "        ws.append(w)\n",
    "        # TODO: implement stochastic subgradient descent.\n",
    "        # ***************************************************\n",
    "        \n",
    "        print(\"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=2869.8351145358524, w0=0.7, w1=0.7613569506190693\n",
      "SubSGD iter. 1/499: loss=2810.120988734961, w0=1.4, w1=1.5565266757364165\n",
      "SubSGD iter. 2/499: loss=2751.155465762236, w0=2.0999999999999996, w1=2.1711876568668034\n",
      "SubSGD iter. 3/499: loss=2694.8959226333145, w0=2.8, w1=3.00452523942638\n",
      "SubSGD iter. 4/499: loss=2637.724223959953, w0=3.5, w1=3.2930225039837957\n",
      "SubSGD iter. 5/499: loss=2585.8066355266847, w0=4.2, w1=2.9399067524164813\n",
      "SubSGD iter. 6/499: loss=2539.450293872886, w0=4.9, w1=2.0382744880037236\n",
      "SubSGD iter. 7/499: loss=2498.493002607788, w0=5.6000000000000005, w1=2.630592449271902\n",
      "SubSGD iter. 8/499: loss=2445.167098952368, w0=6.300000000000001, w1=2.82693704278913\n",
      "SubSGD iter. 9/499: loss=2395.853771111815, w0=7.000000000000001, w1=1.9730907723638609\n",
      "SubSGD iter. 10/499: loss=2356.0341679152734, w0=7.700000000000001, w1=3.056966169055163\n",
      "SubSGD iter. 11/499: loss=2300.0972302478845, w0=8.4, w1=3.4049332154455776\n",
      "SubSGD iter. 12/499: loss=2251.169250397137, w0=9.1, w1=2.8577709132117075\n",
      "SubSGD iter. 13/499: loss=2209.7713069738493, w0=9.799999999999999, w1=3.5827911032925117\n",
      "SubSGD iter. 14/499: loss=2158.873090050856, w0=10.499999999999998, w1=3.4557766580644866\n",
      "SubSGD iter. 15/499: loss=2115.085217112257, w0=11.199999999999998, w1=3.3749623525757757\n",
      "SubSGD iter. 16/499: loss=2071.4485196639844, w0=11.899999999999997, w1=3.193098509000607\n",
      "SubSGD iter. 17/499: loss=2029.0956575627517, w0=12.599999999999996, w1=3.3069188358149195\n",
      "SubSGD iter. 18/499: loss=1984.9371151735377, w0=13.299999999999995, w1=3.3133467166932657\n",
      "SubSGD iter. 19/499: loss=1942.1049972245626, w0=13.999999999999995, w1=2.7778270338670605\n",
      "SubSGD iter. 20/499: loss=1904.0909648072998, w0=14.699999999999994, w1=3.2013682869210762\n",
      "SubSGD iter. 21/499: loss=1858.8809854492017, w0=15.399999999999993, w1=3.4001291527396007\n",
      "SubSGD iter. 22/499: loss=1816.0312757663755, w0=16.099999999999994, w1=4.1344048962314\n",
      "SubSGD iter. 23/499: loss=1769.8723688311852, w0=16.799999999999994, w1=3.0084724827302294\n",
      "SubSGD iter. 24/499: loss=1737.943251958613, w0=17.499999999999993, w1=2.6382043922014327\n",
      "SubSGD iter. 25/499: loss=1701.1412651792236, w0=18.199999999999992, w1=2.1485208588048446\n",
      "SubSGD iter. 26/499: loss=1666.0204171222567, w0=18.89999999999999, w1=2.3448654523220727\n",
      "SubSGD iter. 27/499: loss=1625.4324371312273, w0=19.59999999999999, w1=3.539604289426498\n",
      "SubSGD iter. 28/499: loss=1577.3913578411339, w0=20.29999999999999, w1=4.902355684818245\n",
      "SubSGD iter. 29/499: loss=1530.223221733429, w0=20.99999999999999, w1=4.658328860109094\n",
      "SubSGD iter. 30/499: loss=1494.357036244355, w0=21.69999999999999, w1=3.5323964466079234\n",
      "SubSGD iter. 31/499: loss=1465.2680163986429, w0=22.399999999999988, w1=3.100603868324503\n",
      "SubSGD iter. 32/499: loss=1432.1882978515966, w0=23.099999999999987, w1=2.123806389382584\n",
      "SubSGD iter. 33/499: loss=1404.4930958542923, w0=23.799999999999986, w1=3.1362447272490046\n",
      "SubSGD iter. 34/499: loss=1360.5512198439985, w0=24.499999999999986, w1=3.992466779753543\n",
      "SubSGD iter. 35/499: loss=1319.2123154133405, w0=25.199999999999985, w1=3.7125377873170797\n",
      "SubSGD iter. 36/499: loss=1286.7704112337813, w0=25.899999999999984, w1=3.177018104490874\n",
      "SubSGD iter. 37/499: loss=1256.8726041409907, w0=26.599999999999984, w1=2.4964189268347914\n",
      "SubSGD iter. 38/499: loss=1228.9798121248543, w0=27.299999999999983, w1=2.548579272364502\n",
      "SubSGD iter. 39/499: loss=1195.5533385219449, w0=27.999999999999982, w1=1.5110650785509616\n",
      "SubSGD iter. 40/499: loss=1172.4037651562458, w0=28.69999999999998, w1=1.8462931338542166\n",
      "SubSGD iter. 41/499: loss=1137.2648350436464, w0=29.39999999999998, w1=2.580568877346016\n",
      "SubSGD iter. 42/499: loss=1099.2749840104248, w0=30.09999999999998, w1=1.8464545543450268\n",
      "SubSGD iter. 43/499: loss=1074.7284236311762, w0=30.79999999999998, w1=2.387411606316169\n",
      "SubSGD iter. 44/499: loss=1039.3717252155666, w0=31.49999999999998, w1=2.6317243300253033\n",
      "SubSGD iter. 45/499: loss=1007.2464152822014, w0=32.19999999999998, w1=2.920221594582719\n",
      "SubSGD iter. 46/499: loss=975.3112748063464, w0=32.899999999999984, w1=2.084182737309903\n",
      "SubSGD iter. 47/499: loss=953.3824733623017, w0=33.59999999999999, w1=2.249635529242529\n",
      "SubSGD iter. 48/499: loss=923.342776265557, w0=34.29999999999999, w1=2.974655719323333\n",
      "SubSGD iter. 49/499: loss=889.1536489108838, w0=34.99999999999999, w1=3.708931462815132\n",
      "SubSGD iter. 50/499: loss=855.9123271540356, w0=35.699999999999996, w1=3.453499581713561\n",
      "SubSGD iter. 51/499: loss=830.7137703914826, w0=36.4, w1=3.1171366240374843\n",
      "SubSGD iter. 52/499: loss=806.7079768538127, w0=37.1, w1=2.996632072558442\n",
      "SubSGD iter. 53/499: loss=781.5468993344936, w0=37.800000000000004, w1=3.0339193841763117\n",
      "SubSGD iter. 54/499: loss=755.6154051982039, w0=38.50000000000001, w1=3.665084724171164\n",
      "SubSGD iter. 55/499: loss=725.622187532692, w0=39.20000000000001, w1=3.9146831868583423\n",
      "SubSGD iter. 56/499: loss=699.1613798488371, w0=39.90000000000001, w1=3.330370649312451\n",
      "SubSGD iter. 57/499: loss=679.3300552752519, w0=40.600000000000016, w1=3.674838896182919\n",
      "SubSGD iter. 58/499: loss=653.0629564134647, w0=41.30000000000002, w1=4.2887533095605015\n",
      "SubSGD iter. 59/499: loss=625.5504933253628, w0=42.00000000000002, w1=3.5494205101165575\n",
      "SubSGD iter. 60/499: loss=608.1189794434406, w0=42.700000000000024, w1=3.0022582078826874\n",
      "SubSGD iter. 61/499: loss=590.1619780193568, w0=43.40000000000003, w1=2.657081120157689\n",
      "SubSGD iter. 62/499: loss=571.2817696602517, w0=44.10000000000003, w1=1.7682492959911955\n",
      "SubSGD iter. 63/499: loss=557.9007840418632, w0=44.80000000000003, w1=2.411178994754991\n",
      "SubSGD iter. 64/499: loss=531.4171975973213, w0=45.500000000000036, w1=2.378749986987631\n",
      "SubSGD iter. 65/499: loss=511.45491786811687, w0=46.20000000000004, w1=2.137770072320983\n",
      "SubSGD iter. 66/499: loss=493.81744648358875, w0=46.90000000000004, w1=2.9789602405456264\n",
      "SubSGD iter. 67/499: loss=467.42460893279775, w0=47.600000000000044, w1=1.6250033465123987\n",
      "SubSGD iter. 68/499: loss=460.47613269379946, w0=48.30000000000005, w1=1.6622906581302686\n",
      "SubSGD iter. 69/499: loss=441.84349420977776, w0=49.00000000000005, w1=2.171476502076521\n",
      "SubSGD iter. 70/499: loss=419.40826783961404, w0=49.70000000000005, w1=2.1577213061010303\n",
      "SubSGD iter. 71/499: loss=402.22781640048197, w0=50.400000000000055, w1=2.505688352491445\n",
      "SubSGD iter. 72/499: loss=382.3869289707911, w0=51.10000000000006, w1=2.6840404738712595\n",
      "SubSGD iter. 73/499: loss=364.5591675365741, w0=51.80000000000006, w1=3.7404417052501495\n",
      "SubSGD iter. 74/499: loss=340.462842356056, w0=52.500000000000064, w1=3.9367862987673776\n",
      "SubSGD iter. 75/499: loss=323.7074274165334, w0=53.20000000000007, w1=3.095367789041433\n",
      "SubSGD iter. 76/499: loss=315.1814358048443, w0=53.90000000000007, w1=2.5223577060268094\n",
      "SubSGD iter. 77/499: loss=305.53257105347274, w0=54.60000000000007, w1=2.700709827406624\n",
      "SubSGD iter. 78/499: loss=290.1577826338208, w0=55.300000000000075, w1=2.9892070919640394\n",
      "SubSGD iter. 79/499: loss=274.41254428525446, w0=56.00000000000008, w1=2.4461906305989407\n",
      "SubSGD iter. 80/499: loss=266.0364545297623, w0=55.300000000000075, w1=3.2876091403248853\n",
      "SubSGD iter. 81/499: loss=272.05621646424663, w0=56.00000000000008, w1=2.779108022585052\n",
      "SubSGD iter. 82/499: loss=263.2325425101196, w0=56.70000000000008, w1=1.9948752945457278\n",
      "SubSGD iter. 83/499: loss=257.612047135849, w0=57.400000000000084, w1=1.7941257574964764\n",
      "SubSGD iter. 84/499: loss=247.5345129695968, w0=58.10000000000009, w1=1.3631150727478627\n",
      "SubSGD iter. 85/499: loss=240.1878041973873, w0=58.80000000000009, w1=1.4769353995621755\n",
      "SubSGD iter. 86/499: loss=228.1609724953941, w0=58.10000000000009, w1=2.378567663974933\n",
      "SubSGD iter. 87/499: loss=230.8821423270777, w0=58.80000000000009, w1=2.4923879907892457\n",
      "SubSGD iter. 88/499: loss=218.97088977088245, w0=59.50000000000009, w1=2.9199050760066636\n",
      "SubSGD iter. 89/499: loss=204.96774346177523, w0=60.200000000000095, w1=3.429090919952916\n",
      "SubSGD iter. 90/499: loss=191.01287655033698, w0=60.9000000000001, w1=3.224257400077693\n",
      "SubSGD iter. 91/499: loss=183.12931443106825, w0=61.6000000000001, w1=3.881440103272678\n",
      "SubSGD iter. 92/499: loss=169.23977904178548, w0=62.300000000000104, w1=3.857474263469587\n",
      "SubSGD iter. 93/499: loss=160.9290406749577, w0=63.00000000000011, w1=4.713696315974125\n",
      "SubSGD iter. 94/499: loss=147.1576688781292, w0=62.300000000000104, w1=5.145488894257545\n",
      "SubSGD iter. 95/499: loss=152.51390877215553, w0=63.00000000000011, w1=5.669669445709263\n",
      "SubSGD iter. 96/499: loss=141.57171522898193, w0=63.70000000000011, w1=6.726070677088153\n",
      "SubSGD iter. 97/499: loss=128.95941217075398, w0=64.4000000000001, w1=5.688556483274612\n",
      "SubSGD iter. 98/499: loss=126.95563217811976, w0=65.10000000000011, w1=5.504395704956291\n",
      "SubSGD iter. 99/499: loss=121.4347115135206, w0=64.4000000000001, w1=5.927635129728806\n",
      "SubSGD iter. 100/499: loss=125.70601613368947, w0=65.10000000000011, w1=6.570564828492602\n",
      "SubSGD iter. 101/499: loss=116.10662237027502, w0=65.80000000000011, w1=6.84869179729429\n",
      "SubSGD iter. 102/499: loss=108.87118499615477, w0=66.50000000000011, w1=7.357877641240543\n",
      "SubSGD iter. 103/499: loss=101.32680066708022, w0=67.20000000000012, w1=7.885933456751191\n",
      "SubSGD iter. 104/499: loss=94.47208771151786, w0=66.50000000000011, w1=8.86273093569311\n",
      "SubSGD iter. 105/499: loss=96.92572090157088, w0=65.80000000000011, w1=9.566235854083972\n",
      "SubSGD iter. 106/499: loss=101.18751657660489, w0=65.10000000000011, w1=9.591533619472814\n",
      "SubSGD iter. 107/499: loss=107.18314687240077, w0=64.4000000000001, w1=10.143505051410493\n",
      "SubSGD iter. 108/499: loss=113.06125302764389, w0=65.10000000000011, w1=9.796421065790502\n",
      "SubSGD iter. 109/499: loss=106.9084097053174, w0=65.80000000000011, w1=9.961873857723129\n",
      "SubSGD iter. 110/499: loss=100.68472397206988, w0=66.50000000000011, w1=9.78001001414796\n",
      "SubSGD iter. 111/499: loss=95.3539408272216, w0=67.20000000000012, w1=8.803212535206042\n",
      "SubSGD iter. 112/499: loss=92.00431174584769, w0=67.90000000000012, w1=7.776280337613508\n",
      "SubSGD iter. 113/499: loss=90.26092895690489, w0=68.60000000000012, w1=8.868904851939302\n",
      "SubSGD iter. 114/499: loss=83.22493690666796, w0=69.30000000000013, w1=9.515052085326523\n",
      "SubSGD iter. 115/499: loss=78.45167747670394, w0=68.60000000000012, w1=10.195651262982606\n",
      "SubSGD iter. 116/499: loss=81.23134544877989, w0=69.30000000000013, w1=10.210958929694742\n",
      "SubSGD iter. 117/499: loss=77.63615165129649, w0=70.00000000000013, w1=9.471626130250797\n",
      "SubSGD iter. 118/499: loss=75.4261569093579, w0=69.30000000000013, w1=10.825583024284025\n",
      "SubSGD iter. 119/499: loss=77.31862216127685, w0=70.00000000000013, w1=11.666773192508668\n",
      "SubSGD iter. 120/499: loss=74.40388744905204, w0=70.70000000000013, w1=12.50796336073331\n",
      "SubSGD iter. 121/499: loss=72.68675363594507, w0=70.00000000000013, w1=13.409595625146068\n",
      "SubSGD iter. 122/499: loss=77.0238541870598, w0=69.30000000000013, w1=13.53010017662511\n",
      "SubSGD iter. 123/499: loss=80.40974120885089, w0=68.60000000000012, w1=14.210699354281193\n",
      "SubSGD iter. 124/499: loss=85.9220476101143, w0=69.30000000000013, w1=13.828573369730858\n",
      "SubSGD iter. 125/499: loss=81.19903622887222, w0=70.00000000000013, w1=13.044340641691534\n",
      "SubSGD iter. 126/499: loss=76.22318850203364, w0=70.70000000000013, w1=12.803360727024886\n",
      "SubSGD iter. 127/499: loss=73.16552399166562, w0=71.40000000000013, w1=13.327541278476604\n",
      "SubSGD iter. 128/499: loss=72.11743792873862, w0=72.10000000000014, w1=13.34284894518874\n",
      "SubSGD iter. 129/499: loss=70.53018605979148, w0=72.80000000000014, w1=13.453148978649532\n",
      "SubSGD iter. 130/499: loss=69.65837242219554, w0=72.10000000000014, w1=12.847745827205546\n",
      "SubSGD iter. 131/499: loss=69.51007434773602, w0=72.80000000000014, w1=12.980069353559385\n",
      "SubSGD iter. 132/499: loss=68.62624783904315, w0=72.10000000000014, w1=13.180818890608636\n",
      "SubSGD iter. 133/499: loss=70.169355003664, w0=71.40000000000013, w1=13.803605297428929\n",
      "SubSGD iter. 134/499: loss=73.32220286897459, w0=72.10000000000014, w1=14.659827349933467\n",
      "SubSGD iter. 135/499: loss=74.43692780052187, w0=72.80000000000014, w1=13.920494550489522\n",
      "SubSGD iter. 136/499: loss=70.89773871509045, w0=73.50000000000014, w1=13.570556619009205\n",
      "SubSGD iter. 137/499: loss=69.30672210991212, w0=72.80000000000014, w1=13.074930358540373\n",
      "SubSGD iter. 138/499: loss=68.81526835093044, w0=72.10000000000014, w1=13.622092660774243\n",
      "SubSGD iter. 139/499: loss=71.2136562586149, w0=71.40000000000013, w1=14.149049696543091\n",
      "SubSGD iter. 140/499: loss=74.33830429672409, w0=72.10000000000014, w1=13.766923711992757\n",
      "SubSGD iter. 141/499: loss=71.59885085164237, w0=71.40000000000013, w1=14.314086014226627\n",
      "SubSGD iter. 142/499: loss=74.86587143573237, w0=72.10000000000014, w1=15.326524352093047\n",
      "SubSGD iter. 143/499: loss=77.07590186694276, w0=72.80000000000014, w1=15.670992598963515\n",
      "SubSGD iter. 144/499: loss=77.48109704034123, w0=72.10000000000014, w1=14.945972408882712\n",
      "SubSGD iter. 145/499: loss=75.5151238172077, w0=72.80000000000014, w1=15.059792735697025\n",
      "SubSGD iter. 146/499: loss=74.83429737628568, w0=72.10000000000014, w1=14.325516992205227\n",
      "SubSGD iter. 147/499: loss=73.2809570327159, w0=71.40000000000013, w1=13.829890731736395\n",
      "SubSGD iter. 148/499: loss=73.39532508716493, w0=72.10000000000014, w1=14.079489194423573\n",
      "SubSGD iter. 149/499: loss=72.50163735631202, w0=72.80000000000014, w1=14.116776506041443\n",
      "SubSGD iter. 150/499: loss=71.48339316714184, w0=72.10000000000014, w1=13.158641415139636\n",
      "SubSGD iter. 151/499: loss=70.12200974645874, w0=71.40000000000013, w1=12.610246911381825\n",
      "SubSGD iter. 152/499: loss=70.73019117585424, w0=70.70000000000013, w1=13.099930444778414\n",
      "SubSGD iter. 153/499: loss=73.73397421167392, w0=70.00000000000013, w1=12.494527293334428\n",
      "SubSGD iter. 154/499: loss=75.26951575076848, w0=70.70000000000013, w1=13.126801721816118\n",
      "SubSGD iter. 155/499: loss=73.78982538756912, w0=70.00000000000013, w1=12.791573666512864\n",
      "SubSGD iter. 156/499: loss=75.74721254339426, w0=70.70000000000013, w1=13.670821632409035\n",
      "SubSGD iter. 157/499: loss=75.0758431998056, w0=71.40000000000013, w1=13.295235794122465\n",
      "SubSGD iter. 158/499: loss=72.04389469809011, w0=70.70000000000013, w1=13.690672586065471\n",
      "SubSGD iter. 159/499: loss=75.12836589006811, w0=71.40000000000013, w1=13.968799554867159\n",
      "SubSGD iter. 160/499: loss=73.79322250410304, w0=72.10000000000014, w1=14.477985398813411\n",
      "SubSGD iter. 161/499: loss=73.79429625255653, w0=72.80000000000014, w1=15.596295619424893\n",
      "SubSGD iter. 162/499: loss=77.13758436221829, w0=73.50000000000014, w1=15.469281174196867\n",
      "SubSGD iter. 163/499: loss=75.92382281225787, w0=74.20000000000014, w1=14.11532428016364\n",
      "SubSGD iter. 164/499: loss=70.68399043645512, w0=73.50000000000014, w1=14.526399464705865\n",
      "SubSGD iter. 165/499: loss=72.18723402847776, w0=74.20000000000014, w1=15.189543781748911\n",
      "SubSGD iter. 166/499: loss=74.57002155521135, w0=74.90000000000015, w1=14.570402626334529\n",
      "SubSGD iter. 167/499: loss=72.5269112118823, w0=75.60000000000015, w1=11.197258948744851\n",
      "SubSGD iter. 168/499: loss=67.11772059938713, w0=76.30000000000015, w1=8.426959251855862\n",
      "SubSGD iter. 169/499: loss=71.82273953698648, w0=77.00000000000016, w1=8.95113980330758\n",
      "SubSGD iter. 170/499: loss=72.40062893543131, w0=77.70000000000016, w1=9.807361855812118\n",
      "SubSGD iter. 171/499: loss=73.28056590189463, w0=78.40000000000016, w1=10.072845558036349\n",
      "SubSGD iter. 172/499: loss=75.77745259337539, w0=79.10000000000016, w1=10.523025304242642\n",
      "SubSGD iter. 173/499: loss=78.72322428358214, w0=78.40000000000016, w1=10.072845558036349\n",
      "SubSGD iter. 174/499: loss=75.77745259337539, w0=77.70000000000016, w1=9.94052203168251\n",
      "SubSGD iter. 175/499: loss=73.12597320659167, w0=77.00000000000016, w1=10.05069614856325\n",
      "SubSGD iter. 176/499: loss=70.71393491249721, w0=76.30000000000015, w1=8.882365348045782\n",
      "SubSGD iter. 177/499: loss=70.73876711606286, w0=75.60000000000015, w1=9.03282202449691\n",
      "SubSGD iter. 178/499: loss=69.10868738302288, w0=74.90000000000015, w1=9.560010663244137\n",
      "SubSGD iter. 179/499: loss=67.36464535590424, w0=74.20000000000014, w1=9.573765859219627\n",
      "SubSGD iter. 180/499: loss=67.00691673581085, w0=74.90000000000015, w1=10.657641255910928\n",
      "SubSGD iter. 181/499: loss=66.34816379330942, w0=74.20000000000014, w1=11.089433834194349\n",
      "SubSGD iter. 182/499: loss=65.94095499812572, w0=73.50000000000014, w1=11.306690838374099\n",
      "SubSGD iter. 183/499: loss=66.12886837234997, w0=72.80000000000014, w1=11.586619830810562\n",
      "SubSGD iter. 184/499: loss=66.8865961638996, w0=73.50000000000014, w1=11.729950421405078\n",
      "SubSGD iter. 185/499: loss=66.33348295960492, w0=72.80000000000014, w1=12.077034407025069\n",
      "SubSGD iter. 186/499: loss=67.2774233569913, w0=73.50000000000014, w1=13.202164367155376\n",
      "SubSGD iter. 187/499: loss=68.44046039239038, w0=74.20000000000014, w1=13.65234411336167\n",
      "SubSGD iter. 188/499: loss=69.36498802988488, w0=74.90000000000015, w1=14.508566165866208\n",
      "SubSGD iter. 189/499: loss=72.31019979994588, w0=74.20000000000014, w1=14.94876142057318\n",
      "SubSGD iter. 190/499: loss=73.59864345214518, w0=74.90000000000015, w1=16.314130873236792\n",
      "SubSGD iter. 191/499: loss=80.2121700602612, w0=75.60000000000015, w1=15.914673100850212\n",
      "SubSGD iter. 192/499: loss=79.01065736241469, w0=76.30000000000015, w1=15.659241219748642\n",
      "SubSGD iter. 193/499: loss=79.11436505301882, w0=75.60000000000015, w1=15.013093986361422\n",
      "SubSGD iter. 194/499: loss=75.01757367236813, w0=74.90000000000015, w1=15.026849182336912\n",
      "SubSGD iter. 195/499: loss=74.24485328205489, w0=75.60000000000015, w1=15.7450125063681\n",
      "SubSGD iter. 196/499: loss=78.1971436448073, w0=74.90000000000015, w1=15.702685949764609\n",
      "SubSGD iter. 197/499: loss=77.17114045103799, w0=74.20000000000014, w1=15.414188685207193\n",
      "SubSGD iter. 198/499: loss=75.52857492612016, w0=73.50000000000014, w1=14.467276423464764\n",
      "SubSGD iter. 199/499: loss=71.98255342509717, w0=72.80000000000014, w1=14.178779158907348\n",
      "SubSGD iter. 200/499: loss=71.67640016912502, w0=72.10000000000014, w1=13.928498042445291\n",
      "SubSGD iter. 201/499: loss=72.05332971534303, w0=72.80000000000014, w1=14.98641334927285\n",
      "SubSGD iter. 202/499: loss=74.54164510736251, w0=72.10000000000014, w1=15.494914467012682\n",
      "SubSGD iter. 203/499: loss=77.8127474647016, w0=71.40000000000013, w1=14.889511315568697\n",
      "SubSGD iter. 204/499: loss=76.91835812907752, w0=72.10000000000014, w1=15.504172296699084\n",
      "SubSGD iter. 205/499: loss=77.85408042027116, w0=72.80000000000014, w1=15.480206456895992\n",
      "SubSGD iter. 206/499: loss=76.61479351562156, w0=73.50000000000014, w1=15.750344907185662\n",
      "SubSGD iter. 207/499: loss=77.20966639255795, w0=72.80000000000014, w1=14.870614777353184\n",
      "SubSGD iter. 208/499: loss=74.09076956471343, w0=72.10000000000014, w1=14.53538672204993\n",
      "SubSGD iter. 209/499: loss=73.99358166111844, w0=72.80000000000014, w1=14.50069586270846\n",
      "SubSGD iter. 210/499: loss=72.74028421992644, w0=72.10000000000014, w1=14.250414746246403\n",
      "SubSGD iter. 211/499: loss=73.0366440941093, w0=71.40000000000013, w1=14.370306559734734\n",
      "SubSGD iter. 212/499: loss=75.05180972578026, w0=70.70000000000013, w1=14.793545984507249\n",
      "SubSGD iter. 213/499: loss=78.66551735620992, w0=71.40000000000013, w1=14.989890578024477\n",
      "SubSGD iter. 214/499: loss=77.31031968350182, w0=70.70000000000013, w1=15.340227511392868\n",
      "SubSGD iter. 215/499: loss=80.86973283463077, w0=70.00000000000013, w1=15.161875390013053\n",
      "SubSGD iter. 216/499: loss=82.72023646212594, w0=70.70000000000013, w1=15.214035735542764\n",
      "SubSGD iter. 217/499: loss=80.33439744462792, w0=71.40000000000013, w1=14.81805949232093\n",
      "SubSGD iter. 218/499: loss=76.64549143735694, w0=72.10000000000014, w1=15.106556756878346\n",
      "SubSGD iter. 219/499: loss=76.15607532080423, w0=71.40000000000013, w1=15.501993548821352\n",
      "SubSGD iter. 220/499: loss=79.46680946347352, w0=70.70000000000013, w1=16.205498467212216\n",
      "SubSGD iter. 221/499: loss=84.96935904133178, w0=70.00000000000013, w1=16.230796232601058\n",
      "SubSGD iter. 222/499: loss=87.70294784506945, w0=69.30000000000013, w1=15.036057395496632\n",
      "SubSGD iter. 223/499: loss=85.30136723150538, w0=70.00000000000013, w1=15.649049892680193\n",
      "SubSGD iter. 224/499: loss=84.84946564445877, w0=70.70000000000013, w1=15.89864835536737\n",
      "SubSGD iter. 225/499: loss=83.4298372435607, w0=70.00000000000013, w1=15.979462660856083\n",
      "SubSGD iter. 226/499: loss=86.4286276783073, w0=70.70000000000013, w1=17.160267361555977\n",
      "SubSGD iter. 227/499: loss=90.36188234556465, w0=71.40000000000013, w1=16.764291118334146\n",
      "SubSGD iter. 228/499: loss=85.90231485058541, w0=70.70000000000013, w1=16.78958888372299\n",
      "SubSGD iter. 229/499: loss=88.16003985971216, w0=71.40000000000013, w1=16.26495595901125\n",
      "SubSGD iter. 230/499: loss=83.16609366076472, w0=70.70000000000013, w1=15.716561455253439\n",
      "SubSGD iter. 231/499: loss=82.56078926735806, w0=70.00000000000013, w1=16.329359626254853\n",
      "SubSGD iter. 232/499: loss=88.21993088813737, w0=69.30000000000013, w1=15.245484229563552\n",
      "SubSGD iter. 233/499: loss=86.16124783206098, w0=68.60000000000012, w1=14.821942976509536\n",
      "SubSGD iter. 234/499: loss=88.0500472318844, w0=69.30000000000013, w1=14.396156829218796\n",
      "SubSGD iter. 235/499: loss=82.9457574304652, w0=70.00000000000013, w1=13.611924101179472\n",
      "SubSGD iter. 236/499: loss=77.52479217880239, w0=70.70000000000013, w1=13.755254691773988\n",
      "SubSGD iter. 237/499: loss=75.30196703171327, w0=71.40000000000013, w1=14.634427004004795\n",
      "SubSGD iter. 238/499: loss=75.96763994869663, w0=70.70000000000013, w1=14.578117388959491\n",
      "SubSGD iter. 239/499: loss=77.87900116448341, w0=71.40000000000013, w1=14.477840289719282\n",
      "SubSGD iter. 240/499: loss=75.41626073778501, w0=70.70000000000013, w1=15.00479732548813\n",
      "SubSGD iter. 241/499: loss=79.48185100980929, w0=71.40000000000013, w1=15.838134908047707\n",
      "SubSGD iter. 242/499: loss=81.02488159321028, w0=70.70000000000013, w1=16.656793277853335\n",
      "SubSGD iter. 243/499: loss=87.40465911750186, w0=71.40000000000013, w1=17.0012615247238\n",
      "SubSGD iter. 244/499: loss=87.28808969562324, w0=72.10000000000014, w1=16.900984425483593\n",
      "SubSGD iter. 245/499: loss=85.0723634039495, w0=72.80000000000014, w1=16.297218355517078\n",
      "SubSGD iter. 246/499: loss=80.58042019971491, w0=73.50000000000014, w1=17.3096566933835\n",
      "SubSGD iter. 247/499: loss=85.77824984231057, w0=72.80000000000014, w1=16.191346472772018\n",
      "SubSGD iter. 248/499: loss=80.02889253204305, w0=73.50000000000014, w1=17.55671592543563\n",
      "SubSGD iter. 249/499: loss=87.35900681276775, w0=74.20000000000014, w1=17.722168717368255\n",
      "SubSGD iter. 250/499: loss=88.29928353026897, w0=74.90000000000015, w1=17.992307167657927\n",
      "SubSGD iter. 251/499: loss=90.47979661785726, w0=75.60000000000015, w1=17.592849395271347\n",
      "SubSGD iter. 252/499: loss=88.60792335576933, w0=74.90000000000015, w1=18.40096313631328\n",
      "SubSGD iter. 253/499: loss=93.40648453168924, w0=75.60000000000015, w1=17.8468560623167\n",
      "SubSGD iter. 254/499: loss=90.30594722183905, w0=74.90000000000015, w1=17.997312738767828\n",
      "SubSGD iter. 255/499: loss=90.51463496774527, w0=75.60000000000015, w1=17.424302655753205\n",
      "SubSGD iter. 256/499: loss=87.51680550365168, w0=74.90000000000015, w1=17.799888494039774\n",
      "SubSGD iter. 257/499: loss=89.15957304393402, w0=75.60000000000015, w1=17.424302655753205\n",
      "SubSGD iter. 258/499: loss=87.51680550365168, w0=74.90000000000015, w1=17.524579754993415\n",
      "SubSGD iter. 259/499: loss=87.33500862943639, w0=75.60000000000015, w1=16.83788511948728\n",
      "SubSGD iter. 260/499: loss=83.94188749279999, w0=74.90000000000015, w1=17.123059614036574\n",
      "SubSGD iter. 261/499: loss=84.80987864983489, w0=75.60000000000015, w1=16.77312168255626\n",
      "SubSGD iter. 262/499: loss=83.5681630508878, w0=74.90000000000015, w1=17.38591985355767\n",
      "SubSGD iter. 263/499: loss=86.4447628465111, w0=74.20000000000014, w1=17.998718024559082\n",
      "SubSGD iter. 264/499: loss=90.18688424051707, w0=73.50000000000014, w1=18.34389511228408\n",
      "SubSGD iter. 265/499: loss=92.80267414721764, w0=72.80000000000014, w1=18.956693283285492\n",
      "SubSGD iter. 266/499: loss=98.1118410276358, w0=73.50000000000014, w1=19.509899199361605\n",
      "SubSGD iter. 267/499: loss=102.00478107238897, w0=72.80000000000014, w1=19.245114090170837\n",
      "SubSGD iter. 268/499: loss=100.43824580005763, w0=73.50000000000014, w1=18.849137846949006\n",
      "SubSGD iter. 269/499: loss=96.62312852978818, w0=72.80000000000014, w1=19.407261550698045\n",
      "SubSGD iter. 270/499: loss=101.78265789710812, w0=72.10000000000014, w1=19.80269834264105\n",
      "SubSGD iter. 271/499: loss=106.30404894653933, w0=72.80000000000014, w1=19.85485868817076\n",
      "SubSGD iter. 272/499: loss=105.63027685779264, w0=73.50000000000014, w1=19.45888244494893\n",
      "SubSGD iter. 273/499: loss=101.57371521228237, w0=72.80000000000014, w1=19.994402127775135\n",
      "SubSGD iter. 274/499: loss=106.87078113249433, w0=72.10000000000014, w1=19.884102094314343\n",
      "SubSGD iter. 275/499: loss=107.02109432886616, w0=71.40000000000013, w1=19.33570759055653\n",
      "SubSGD iter. 276/499: loss=103.94107008656297, w0=70.70000000000013, w1=18.277792283728974\n",
      "SubSGD iter. 277/499: loss=97.83156974279254, w0=71.40000000000013, w1=18.243101424387504\n",
      "SubSGD iter. 278/499: loss=95.46844503666156, w0=72.10000000000014, w1=18.142824325147295\n",
      "SubSGD iter. 279/499: loss=93.12819064212879, w0=72.80000000000014, w1=18.570341410364712\n",
      "SubSGD iter. 280/499: loss=95.12587325706714, w0=73.50000000000014, w1=19.123547326440825\n",
      "SubSGD iter. 281/499: loss=98.80508116003348, w0=74.20000000000014, w1=18.747961488154257\n",
      "SubSGD iter. 282/499: loss=95.68516610757837, w0=73.50000000000014, w1=19.237645021550843\n",
      "SubSGD iter. 283/499: loss=99.7344869043317, w0=74.20000000000014, w1=19.390538778665427\n",
      "SubSGD iter. 284/499: loss=100.84786034632174, w0=73.50000000000014, w1=19.510430592153757\n",
      "SubSGD iter. 285/499: loss=102.00928476979433, w0=72.80000000000014, w1=19.59124489764247\n",
      "SubSGD iter. 286/499: loss=103.33995887758326, w0=73.50000000000014, w1=19.70506522445678\n",
      "SubSGD iter. 287/499: loss=103.67785886947341, w0=74.20000000000014, w1=18.910695879966003\n",
      "SubSGD iter. 288/499: loss=96.95358855447907, w0=74.90000000000015, w1=19.15500860367514\n",
      "SubSGD iter. 289/499: loss=99.24512720662968, w0=74.20000000000014, w1=18.53862165483789\n",
      "SubSGD iter. 290/499: loss=94.09242560944277, w0=73.50000000000014, w1=17.17325220217428\n",
      "SubSGD iter. 291/499: loss=84.93164724039681, w0=72.80000000000014, w1=16.88475493761686\n",
      "SubSGD iter. 292/499: loss=83.84482737430834, w0=72.10000000000014, w1=17.3105410849076\n",
      "SubSGD iter. 293/499: loss=87.55872777645737, w0=72.80000000000014, w1=17.291961626008725\n",
      "SubSGD iter. 294/499: loss=86.30983816516722, w0=72.10000000000014, w1=16.530604675389654\n",
      "SubSGD iter. 295/499: loss=82.96827319851506, w0=71.40000000000013, w1=16.488278118786162\n",
      "SubSGD iter. 296/499: loss=84.35901859333643, w0=72.10000000000014, w1=17.606588339397643\n",
      "SubSGD iter. 297/499: loss=89.46043759816455, w0=72.80000000000014, w1=16.57965614180511\n",
      "SubSGD iter. 298/499: loss=82.10658474839038, w0=73.50000000000014, w1=16.829254604492288\n",
      "SubSGD iter. 299/499: loss=82.87923423664097, w0=72.80000000000014, w1=17.665293461765103\n",
      "SubSGD iter. 300/499: loss=88.71548871493016, w0=73.50000000000014, w1=18.397258055272736\n",
      "SubSGD iter. 301/499: loss=93.19412771250698, w0=72.80000000000014, w1=18.679805874358962\n",
      "SubSGD iter. 302/499: loss=95.95672810844333, w0=73.50000000000014, w1=18.647376866591603\n",
      "SubSGD iter. 303/499: loss=95.06687295179616, w0=74.20000000000014, w1=18.79070745718612\n",
      "SubSGD iter. 304/499: loss=96.01578222348438, w0=74.90000000000015, w1=17.43675056315289\n",
      "SubSGD iter. 305/499: loss=86.76888183380517, w0=75.60000000000015, w1=17.253284784102473\n",
      "SubSGD iter. 306/499: loss=86.4387261385496, w0=74.90000000000015, w1=16.295149693200667\n",
      "SubSGD iter. 307/499: loss=80.11214407375222, w0=75.60000000000015, w1=16.918637815486843\n",
      "SubSGD iter. 308/499: loss=84.41375509958476, w0=74.90000000000015, w1=17.19011799746942\n",
      "SubSGD iter. 309/499: loss=85.22038954930612, w0=74.20000000000014, w1=17.310622548948462\n",
      "SubSGD iter. 310/499: loss=85.63184672393051, w0=73.50000000000014, w1=16.887081295894447\n",
      "SubSGD iter. 311/499: loss=83.21597485223175, w0=72.80000000000014, w1=15.92894620499264\n",
      "SubSGD iter. 312/499: loss=78.71026519989714, w0=72.10000000000014, w1=16.049450756471682\n",
      "SubSGD iter. 313/499: loss=80.43974543333111, w0=71.40000000000013, w1=16.32093093845426\n",
      "SubSGD iter. 314/499: loss=83.46041282209416, w0=70.70000000000013, w1=17.16095850991407\n",
      "SubSGD iter. 315/499: loss=90.36611612555069, w0=70.00000000000013, w1=17.34282235348924\n",
      "SubSGD iter. 316/499: loss=94.09922693306935, w0=69.30000000000013, w1=18.178861210762054\n",
      "SubSGD iter. 317/499: loss=102.81484400536215, w0=70.00000000000013, w1=17.566063039760643\n",
      "SubSGD iter. 318/499: loss=95.53233119633256, w0=70.70000000000013, w1=17.914030086151058\n",
      "SubSGD iter. 319/499: loss=95.2630389294774, w0=71.40000000000013, w1=17.174697286707115\n",
      "SubSGD iter. 320/499: loss=88.33791102512009, w0=70.70000000000013, w1=17.356561130282284\n",
      "SubSGD iter. 321/499: loss=91.58352041939233, w0=71.40000000000013, w1=17.7045281766727\n",
      "SubSGD iter. 322/499: loss=91.73132839188284, w0=70.70000000000013, w1=17.349500699142872\n",
      "SubSGD iter. 323/499: loss=91.53891165489087, w0=71.40000000000013, w1=16.530842329337244\n",
      "SubSGD iter. 324/499: loss=84.59204340233414, w0=72.10000000000014, w1=16.006209404625505\n",
      "SubSGD iter. 325/499: loss=80.22384416497319, w0=71.40000000000013, w1=16.553371706859377\n",
      "SubSGD iter. 326/499: loss=84.71611746400347, w0=72.10000000000014, w1=15.463576683786156\n",
      "SubSGD iter. 327/499: loss=77.67347136558745, w0=71.40000000000013, w1=15.477331879761646\n",
      "SubSGD iter. 328/499: loss=79.35694745306087, w0=70.70000000000013, w1=15.982349611585969\n",
      "SubSGD iter. 329/499: loss=83.84044247070555, w0=71.40000000000013, w1=16.634406509028423\n",
      "SubSGD iter. 330/499: loss=85.16658946265919, w0=70.70000000000013, w1=16.042088547760244\n",
      "SubSGD iter. 331/499: loss=84.13778252422549, w0=70.00000000000013, w1=15.863736426380429\n",
      "SubSGD iter. 332/499: loss=85.86310774697073, w0=70.70000000000013, w1=16.11333488906761\n",
      "SubSGD iter. 333/499: loss=84.49706477484389, w0=70.00000000000013, w1=15.507931737623624\n",
      "SubSGD iter. 334/499: loss=84.20828176679042, w0=70.70000000000013, w1=15.947856180507806\n",
      "SubSGD iter. 335/499: loss=83.67038267981977, w0=71.40000000000013, w1=14.736433292581626\n",
      "SubSGD iter. 336/499: loss=76.34001750416323, w0=72.10000000000014, w1=16.276495630968814\n",
      "SubSGD iter. 337/499: loss=81.6040493336623, w0=71.40000000000013, w1=15.671963525114654\n",
      "SubSGD iter. 338/499: loss=80.24052701711493, w0=70.70000000000013, w1=15.853827368689823\n",
      "SubSGD iter. 339/499: loss=83.2128434736338, w0=71.40000000000013, w1=15.428041221399083\n",
      "SubSGD iter. 340/499: loss=79.13919159241871, w0=72.10000000000014, w1=15.771753842582473\n",
      "SubSGD iter. 341/499: loss=79.08577652601093, w0=72.80000000000014, w1=15.882053876043265\n",
      "SubSGD iter. 342/499: loss=78.48187117987061, w0=72.10000000000014, w1=16.50310544961386\n",
      "SubSGD iter. 343/499: loss=82.81752353737724, w0=72.80000000000014, w1=16.70554233965486\n",
      "SubSGD iter. 344/499: loss=82.81251733079189, w0=72.10000000000014, w1=16.82543415314319\n",
      "SubSGD iter. 345/499: loss=84.6320326619818, w0=71.40000000000013, w1=17.248673577915703\n",
      "SubSGD iter. 346/499: loss=88.7948470827888, w0=70.70000000000013, w1=16.54773958048095\n",
      "SubSGD iter. 347/499: loss=86.79751666389143, w0=70.00000000000013, w1=16.93302372080908\n",
      "SubSGD iter. 348/499: loss=91.59821433331943, w0=70.70000000000013, w1=17.651187044840267\n",
      "SubSGD iter. 349/499: loss=93.48944933880462, w0=71.40000000000013, w1=16.856817700349488\n",
      "SubSGD iter. 350/499: loss=86.43671688701613, w0=70.70000000000013, w1=17.038681543924657\n",
      "SubSGD iter. 351/499: loss=89.62451547791929, w0=70.00000000000013, w1=17.222842322242975\n",
      "SubSGD iter. 352/499: loss=93.34959919996885, w0=70.70000000000013, w1=17.46715504595211\n",
      "SubSGD iter. 353/499: loss=92.28877375220996, w0=70.00000000000013, w1=17.73863522793469\n",
      "SubSGD iter. 354/499: loss=96.67431975947862, w0=69.30000000000013, w1=17.682325612889386\n",
      "SubSGD iter. 355/499: loss=99.39088420515547, w0=70.00000000000013, w1=17.93260672935144\n",
      "SubSGD iter. 356/499: loss=97.99346681427421, w0=69.30000000000013, w1=18.441107847091274\n",
      "SubSGD iter. 357/499: loss=104.72271179846098, w0=70.00000000000013, w1=18.045131603869443\n",
      "SubSGD iter. 358/499: loss=98.77596189985303, w0=69.30000000000013, w1=18.43041574419757\n",
      "SubSGD iter. 359/499: loss=104.6435809677465, w0=70.00000000000013, w1=17.990727339112386\n",
      "SubSGD iter. 360/499: loss=98.39605503636764, w0=70.70000000000013, w1=17.640390405743993\n",
      "SubSGD iter. 361/499: loss=93.41807390370195, w0=71.40000000000013, w1=17.80584319767662\n",
      "SubSGD iter. 362/499: loss=92.4121947975837, w0=72.10000000000014, w1=17.232833114661997\n",
      "SubSGD iter. 363/499: loss=87.07407931112557, w0=71.40000000000013, w1=17.504313296644575\n",
      "SubSGD iter. 364/499: loss=90.41601155784508, w0=72.10000000000014, w1=18.33765087920415\n",
      "SubSGD iter. 365/499: loss=94.53198273821862, w0=72.80000000000014, w1=18.001287921528075\n",
      "SubSGD iter. 366/499: loss=90.9997120476268, w0=73.50000000000014, w1=18.16709418109557\n",
      "SubSGD iter. 367/499: loss=91.52606538225021, w0=74.20000000000014, w1=18.60701862397975\n",
      "SubSGD iter. 368/499: loss=94.6079968517625, w0=73.50000000000014, w1=17.881998433898946\n",
      "SubSGD iter. 369/499: loss=89.53334548188948, w0=74.20000000000014, w1=17.482540661512367\n",
      "SubSGD iter. 370/499: loss=86.72553590086368, w0=74.90000000000015, w1=16.878774591545852\n",
      "SubSGD iter. 371/499: loss=83.35246877353754, w0=74.20000000000014, w1=17.61288891454684\n",
      "SubSGD iter. 372/499: loss=87.57447060002978, w0=73.50000000000014, w1=17.88436909652942\n",
      "SubSGD iter. 373/499: loss=89.54958046446556, w0=72.80000000000014, w1=17.718562836961922\n",
      "SubSGD iter. 374/499: loss=89.07010471859995, w0=73.50000000000014, w1=16.934330108922598\n",
      "SubSGD iter. 375/499: loss=83.49359993983052, w0=74.20000000000014, w1=16.361320025907975\n",
      "SubSGD iter. 376/499: loss=80.12487024070768, w0=73.50000000000014, w1=16.481211839396305\n",
      "SubSGD iter. 377/499: loss=80.92311613445932, w0=72.80000000000014, w1=15.534299577653876\n",
      "SubSGD iter. 378/499: loss=76.85671732517156, w0=72.10000000000014, w1=15.654191391142207\n",
      "SubSGD iter. 379/499: loss=78.53581023759013, w0=71.40000000000013, w1=15.455430525323683\n",
      "SubSGD iter. 380/499: loss=79.25989190011703, w0=70.70000000000013, w1=15.399120910278379\n",
      "SubSGD iter. 381/499: loss=81.1250227236675, w0=70.00000000000013, w1=15.951092342216057\n",
      "SubSGD iter. 382/499: loss=86.2887511519196, w0=69.30000000000013, w1=16.509216045965097\n",
      "SubSGD iter. 383/499: loss=92.28081262850472, w0=70.00000000000013, w1=15.805711127574234\n",
      "SubSGD iter. 384/499: loss=85.58459624019203, w0=69.30000000000013, w1=16.352873429808106\n",
      "SubSGD iter. 385/499: loss=91.43716444035704, w0=70.00000000000013, w1=16.9958031285719\n",
      "SubSGD iter. 386/499: loss=91.97046599675338, w0=70.70000000000013, w1=17.24540159125908\n",
      "SubSGD iter. 387/499: loss=90.88698513315289, w0=70.00000000000013, w1=17.429562369577397\n",
      "SubSGD iter. 388/499: loss=94.65013858001353, w0=70.70000000000013, w1=17.24540159125908\n",
      "SubSGD iter. 389/499: loss=90.88698513315289, w0=71.40000000000013, w1=16.72076866654734\n",
      "SubSGD iter. 390/499: loss=85.65390458032999, w0=70.70000000000013, w1=17.198816868973463\n",
      "SubSGD iter. 391/499: loss=90.59875547027843, w0=71.40000000000013, w1=18.379621569673358\n",
      "SubSGD iter. 392/499: loss=96.46182931829901, w0=72.10000000000014, w1=17.539593998213547\n",
      "SubSGD iter. 393/499: loss=89.02241544432506, w0=71.40000000000013, w1=18.273708321214535\n",
      "SubSGD iter. 394/499: loss=95.6895342621691, w0=70.70000000000013, w1=17.659793907836953\n",
      "SubSGD iter. 395/499: loss=93.54643189817936, w0=71.40000000000013, w1=18.18397445928867\n",
      "SubSGD iter. 396/499: loss=95.04399365805887, w0=72.10000000000014, w1=17.38414372522596\n",
      "SubSGD iter. 397/499: loss=88.02334058220461, w0=71.40000000000013, w1=16.854442751163845\n",
      "SubSGD iter. 398/499: loss=86.42289293631362, w0=72.10000000000014, w1=17.68778033372342\n",
      "SubSGD iter. 399/499: loss=89.99730256733957, w0=72.80000000000014, w1=17.256769648974807\n",
      "SubSGD iter. 400/499: loss=86.0902588530727, w0=73.50000000000014, w1=17.37058997578912\n",
      "SubSGD iter. 401/499: loss=86.16244810921954, w0=72.80000000000014, w1=17.364162094910775\n",
      "SubSGD iter. 402/499: loss=86.76420777304689, w0=73.50000000000014, w1=18.489292055041084\n",
      "SubSGD iter. 403/499: loss=93.87595057312069, w0=74.20000000000014, w1=18.700456449607287\n",
      "SubSGD iter. 404/499: loss=95.31988494472076, w0=73.50000000000014, w1=18.850913126058416\n",
      "SubSGD iter. 405/499: loss=96.63700256791608, w0=72.80000000000014, w1=18.116637382566616\n",
      "SubSGD iter. 406/499: loss=91.80993448104681, w0=73.50000000000014, w1=17.73451139801628\n",
      "SubSGD iter. 407/499: loss=88.53436268515357, w0=72.80000000000014, w1=16.89332122979164\n",
      "SubSGD iter. 408/499: loss=83.89497567554429, w0=73.50000000000014, w1=16.320311146777016\n",
      "SubSGD iter. 409/499: loss=80.05974447765658, w0=72.80000000000014, w1=16.12155028095849\n",
      "SubSGD iter. 410/499: loss=79.6714276008147, w0=72.10000000000014, w1=15.174638019216061\n",
      "SubSGD iter. 411/499: loss=76.43559673128908, w0=72.80000000000014, w1=15.698818570667779\n",
      "SubSGD iter. 412/499: loss=77.61048810702782, w0=73.50000000000014, w1=16.578066536563952\n",
      "SubSGD iter. 413/499: loss=81.4553079317794, w0=72.80000000000014, w1=17.018261791270923\n",
      "SubSGD iter. 414/499: loss=84.63473582677419, w0=73.50000000000014, w1=16.27892899182698\n",
      "SubSGD iter. 415/499: loss=79.84187880352376, w0=72.80000000000014, w1=16.014143882636212\n",
      "SubSGD iter. 416/499: loss=79.13085633038659, w0=72.10000000000014, w1=16.35932097036121\n",
      "SubSGD iter. 417/499: loss=82.04161671440393, w0=72.80000000000014, w1=16.009383038880895\n",
      "SubSGD iter. 418/499: loss=79.10716223661613, w0=73.50000000000014, w1=15.633797200594325\n",
      "SubSGD iter. 419/499: loss=76.66688318866632, w0=74.20000000000014, w1=16.257285322880502\n",
      "SubSGD iter. 420/499: loss=79.57614879071446, w0=73.50000000000014, w1=16.474542327060252\n",
      "SubSGD iter. 421/499: loss=80.88681409741653, w0=72.80000000000014, w1=17.363374151226747\n",
      "SubSGD iter. 422/499: loss=86.75922097733698, w0=72.10000000000014, w1=16.638353961145942\n",
      "SubSGD iter. 423/499: loss=83.5662369594923, w0=72.80000000000014, w1=16.98206658232933\n",
      "SubSGD iter. 424/499: loss=84.41882165731457, w0=72.10000000000014, w1=16.102894270098524\n",
      "SubSGD iter. 425/499: loss=80.70916902397107, w0=71.40000000000013, w1=16.534686848381945\n",
      "SubSGD iter. 426/499: loss=84.61318006718899, w0=72.10000000000014, w1=16.648507175196258\n",
      "SubSGD iter. 427/499: loss=83.62318162298538, w0=71.40000000000013, w1=17.273323275007204\n",
      "SubSGD iter. 428/499: loss=88.94831864961478, w0=72.10000000000014, w1=14.503023578118215\n",
      "SubSGD iter. 429/499: loss=73.88081842589152, w0=71.40000000000013, w1=14.242311356736352\n",
      "SubSGD iter. 430/499: loss=74.63308441495141, w0=72.10000000000014, w1=14.856972337866738\n",
      "SubSGD iter. 431/499: loss=75.1709981444616, w0=72.80000000000014, w1=14.67350655881632\n",
      "SubSGD iter. 432/499: loss=73.35414347108545, w0=72.10000000000014, w1=14.855370402391488\n",
      "SubSGD iter. 433/499: loss=75.1648767060689, w0=72.80000000000014, w1=14.907530747921198\n",
      "SubSGD iter. 434/499: loss=74.23305028250998, w0=72.10000000000014, w1=15.641645070922188\n",
      "SubSGD iter. 435/499: loss=78.47793376926985, w0=71.40000000000013, w1=15.921574063358651\n",
      "SubSGD iter. 436/499: loss=81.42914093139485, w0=72.10000000000014, w1=17.046704023488957\n",
      "SubSGD iter. 437/499: loss=85.9377847170439, w0=72.80000000000014, w1=18.059382364841724\n",
      "SubSGD iter. 438/499: loss=91.40610825626437, w0=73.50000000000014, w1=17.265013020350946\n",
      "SubSGD iter. 439/499: loss=85.49911795570215, w0=72.80000000000014, w1=16.607830317155962\n",
      "SubSGD iter. 440/499: loss=82.26320071693218, w0=73.50000000000014, w1=16.955797363546377\n",
      "SubSGD iter. 441/499: loss=83.62047503983761, w0=74.20000000000014, w1=16.352031293579863\n",
      "SubSGD iter. 442/499: loss=80.07543764340559, w0=73.50000000000014, w1=16.091319072197997\n",
      "SubSGD iter. 443/499: loss=78.87564472361524, w0=74.20000000000014, w1=16.97056703809417\n",
      "SubSGD iter. 444/499: loss=83.55556981595053, w0=73.50000000000014, w1=17.506086720920376\n",
      "SubSGD iter. 445/499: loss=87.03009385871205, w0=72.80000000000014, w1=16.901554615066217\n",
      "SubSGD iter. 446/499: loss=83.94324425121677, w0=73.50000000000014, w1=15.958048131529097\n",
      "SubSGD iter. 447/499: loss=78.21065088504618, w0=72.80000000000014, w1=16.63864730918518\n",
      "SubSGD iter. 448/499: loss=82.43541666780901, w0=73.50000000000014, w1=16.05433477163929\n",
      "SubSGD iter. 449/499: loss=78.68932033026442, w0=74.20000000000014, w1=16.59529182361043\n",
      "SubSGD iter. 450/499: loss=81.39847491176154, w0=73.50000000000014, w1=15.502667309284636\n",
      "SubSGD iter. 451/499: loss=76.07242714958309, w0=72.80000000000014, w1=15.93367799403325\n",
      "SubSGD iter. 452/499: loss=78.73343401330098, w0=73.50000000000014, w1=16.130022587550478\n",
      "SubSGD iter. 453/499: loss=79.07209509655924, w0=74.20000000000014, w1=16.106056747747388\n",
      "SubSGD iter. 454/499: loss=78.79780916409838, w0=74.90000000000015, w1=15.706598975360809\n",
      "SubSGD iter. 455/499: loss=77.18941329291117, w0=74.20000000000014, w1=16.063324493716646\n",
      "SubSGD iter. 456/499: loss=78.58202000910337, w0=73.50000000000014, w1=15.301967543097577\n",
      "SubSGD iter. 457/499: loss=75.19588646291048, w0=72.80000000000014, w1=16.11008128413951\n",
      "SubSGD iter. 458/499: loss=79.61315453498518, w0=73.50000000000014, w1=15.744160553409113\n",
      "SubSGD iter. 459/499: loss=77.18052350440335, w0=72.80000000000014, w1=16.175171238157727\n",
      "SubSGD iter. 460/499: loss=79.9456165366335, w0=73.50000000000014, w1=15.82523330667741\n",
      "SubSGD iter. 461/499: loss=77.56560303521569, w0=72.80000000000014, w1=14.945503176844932\n",
      "SubSGD iter. 462/499: loss=74.38082462596333, w0=72.10000000000014, w1=14.83520314338414\n",
      "SubSGD iter. 463/499: loss=75.08803154556297, w0=72.80000000000014, w1=14.453077158833805\n",
      "SubSGD iter. 464/499: loss=72.57638103905452, w0=73.50000000000014, w1=15.99313949722099\n",
      "SubSGD iter. 465/499: loss=78.38402675861755, w0=74.20000000000014, w1=16.263277947510662\n",
      "SubSGD iter. 466/499: loss=79.60746257216972, w0=74.90000000000015, w1=15.492454881442475\n",
      "SubSGD iter. 467/499: loss=76.21192429583527, w0=74.20000000000014, w1=15.511034340341352\n",
      "SubSGD iter. 468/499: loss=75.95738004552649, w0=73.50000000000014, w1=15.934273765113867\n",
      "SubSGD iter. 469/499: loss=78.09388864564212, w0=74.20000000000014, w1=16.790495817618403\n",
      "SubSGD iter. 470/499: loss=82.50293890579734, w0=73.50000000000014, w1=16.056220074126603\n",
      "SubSGD iter. 471/499: loss=78.69878526998617, w0=72.80000000000014, w1=16.561237805950924\n",
      "SubSGD iter. 472/499: loss=82.00462909052892, w0=73.50000000000014, w1=15.584440327009006\n",
      "SubSGD iter. 473/499: loss=76.44111379799104, w0=72.80000000000014, w1=15.694614443889746\n",
      "SubSGD iter. 474/499: loss=77.59088921783126, w0=73.50000000000014, w1=15.964752894179416\n",
      "SubSGD iter. 475/499: loss=78.24368193610387, w0=72.80000000000014, w1=15.621040272996025\n",
      "SubSGD iter. 476/499: loss=77.25076079260133, w0=72.10000000000014, w1=16.15655995582223\n",
      "SubSGD iter. 477/499: loss=80.98258669015206, w0=71.40000000000013, w1=15.551156804378246\n",
      "SubSGD iter. 478/499: loss=79.68763509046394, w0=72.10000000000014, w1=14.872809808403703\n",
      "SubSGD iter. 479/499: loss=75.23165559654898, w0=72.80000000000014, w1=14.522871876923386\n",
      "SubSGD iter. 480/499: loss=72.817387759899, w0=73.50000000000014, w1=14.538179543635522\n",
      "SubSGD iter. 481/499: loss=72.22843361336372, w0=72.80000000000014, w1=13.776822593016453\n",
      "SubSGD iter. 482/499: loss=70.49347977561666, w0=72.10000000000014, w1=13.512037483825685\n",
      "SubSGD iter. 483/499: loss=70.93497781849548, w0=71.40000000000013, w1=14.311868217888394\n",
      "SubSGD iter. 484/499: loss=74.85860131692175, w0=72.10000000000014, w1=14.944142646370084\n",
      "SubSGD iter. 485/499: loss=75.50796914815116, w0=72.80000000000014, w1=15.288610893240552\n",
      "SubSGD iter. 486/499: loss=75.78144596574391, w0=73.50000000000014, w1=15.738790639446846\n",
      "SubSGD iter. 487/499: loss=77.15524957082442, w0=72.80000000000014, w1=16.109058729975644\n",
      "SubSGD iter. 488/499: loss=79.6079654047894, w0=72.10000000000014, w1=16.388987722412107\n",
      "SubSGD iter. 489/499: loss=82.20001520154429, w0=71.40000000000013, w1=15.18297143264166\n",
      "SubSGD iter. 490/499: loss=78.09259374379874, w0=72.10000000000014, w1=15.610488517859078\n",
      "SubSGD iter. 491/499: loss=78.33488867746077, w0=71.40000000000013, w1=16.168612221608118\n",
      "SubSGD iter. 492/499: loss=82.66685108608465, w0=70.70000000000013, w1=15.907900000226254\n",
      "SubSGD iter. 493/499: loss=83.4748777599919, w0=71.40000000000013, w1=17.026210220837736\n",
      "SubSGD iter. 494/499: loss=87.43725398301521, w0=72.10000000000014, w1=17.567167272808877\n",
      "SubSGD iter. 495/499: loss=89.20215144239384, w0=72.80000000000014, w1=17.832650975033108\n",
      "SubSGD iter. 496/499: loss=89.83914000451853, w0=72.10000000000014, w1=17.22811886917895\n",
      "SubSGD iter. 497/499: loss=87.04487182078778, w0=72.80000000000014, w1=17.26540618079682\n",
      "SubSGD iter. 498/499: loss=86.14403156746947, w0=72.10000000000014, w1=16.540385990716015\n",
      "SubSGD iter. 499/499: loss=83.02207630616775, w0=71.40000000000013, w1=15.921252594771348\n",
      "SubSGD: execution time=0.112 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9105805b41cb4413a1fc0e4d6acf3b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses, subsgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "state": {
    "d2b2c3aea192430e81437f33ba0b0e69": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "e4a6a7a70ccd42ddb112989c04f2ed3f": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
